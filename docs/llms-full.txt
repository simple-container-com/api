# Simple Container - Complete Documentation

> This file contains the complete documentation for Simple Container, optimized for LLM consumption.

## Table of Contents

1. [Home](#home)
2. [Getting Started](#getting-started)
3. [Core Concepts](#core-concepts)
4. [Guides](#guides)
5. [Examples](#examples)
6. [Reference](#reference)
7. [Advanced](#advanced)
8. [AI Assistant](#ai-assistant)

---

# Home

## Home

# Simple Container

Unlike other products whose main focus usually is to provide fine-grained configuration options for either CI or CD 
aspect of software delivery, Simple Container offers high-level primitives for quick and easy set-up of integration 
and delivery pipelines for microservice applications. Simple Container reflects simplicity of use and 
container-native nature of the provided DevOps practices.

## Why do I need Simple-Container.com?

Simple Container allows companies to **scale their microservices infrastructure from startup to enterprise** with quantified advantages:

**üöÄ 500x Faster Scaling:**

- **Customer Onboarding**: 5 minutes vs 2-3 days traditional approaches
- **Developer Onboarding**: 1-2 hours vs 2-4 weeks Kubernetes/AWS training
- **Time to First Deployment**: 15 minutes vs 2-3 days infrastructure setup

**üí∞ 70% Cost Reduction:**

- **DevOps Efficiency**: 1 engineer per 100+ customers vs 1 per 10-20 traditional
- **Configuration Complexity**: 90% reduction (500 vs 5000+ lines for 100 customers)
- **Operational Staff**: 80% reduction in DevOps overhead

If you're already familiar with tools like [Terraform](https://www.terraform.io/) or [Pulumi](https://www.pulumi.com/), 
you know that **scaling Infrastructure as Code becomes exponentially complex** as you add customers. Traditional approaches require:

- **Linear DevOps scaling** (more customers = more DevOps engineers)
- **Manual configuration** for each customer deployment
- **Complex secret management** across hundreds of services
- **Infrastructure drift** and inconsistencies

**Simple Container transforms this challenge** by providing separation of concerns where:

- **DevOps manages infrastructure once** in parent stacks
- **Developers deploy self-service** with simple client configurations
- **Scaling complexity remains constant** regardless of customer count

Simple Container delivers **enterprise-grade scaling** with **startup-level simplicity** through:

**üéØ Quantified Scaling Advantages:**

* **500x faster customer onboarding** - 5 minutes vs 2-3 days
* **90% configuration reduction** - 500 vs 5000+ lines for 100 customers  
* **5x operational efficiency** - 1 DevOps per 100+ vs 10-20 customers
* **Zero downtime migrations** - one-line configuration changes
* **70% cost reduction** through intelligent resource sharing

**üîß Built for Scale:**

* **Multi-dimensional resource allocation** - shared and dedicated resource pools
* **Automatic namespace isolation** for multi-tenant security
* **Template-based deployments** eliminating infrastructure drift
* **Self-service developer operations** without DevOps bottlenecks
* **Cloud-agnostic flexibility** across Kubernetes, AWS, and GCP 

**Real-World Impact:**

- **SaaS companies** scale from 10 to 1000 customers with 80% staff reduction
- **Enterprise organizations** achieve instant multi-region expansion
- **Development teams** deploy independently without infrastructure expertise

Please read [Why Simple Container](concepts/motivation.md) to understand the quantified scaling advantages and [Scaling Advantages](advanced/scaling-advantages.md) for detailed comparison with traditional approaches.

## Getting Started

Ready to get started? Follow our structured learning path:

1. **[Getting Started Guide](getting-started/index.md)** - Overview and installation
2. **[Quick Start](getting-started/quick-start.md)** - Deploy your first app in 15 minutes
3. **[Core Concepts](concepts/main-concepts.md)** - Understand the fundamentals

## Documentation Sections

### üöÄ [Getting Started](getting-started/index.md)
Perfect for newcomers - installation, quick start, and first deployment.

### üß† [Core Concepts](concepts/main-concepts.md)
Understand templates, resources, environments, and the Simple Container philosophy.

### üìñ [Guides](guides/index.md)
Step-by-step tutorials for ECS Fargate, GKE Autopilot, Kubernetes, and operational tasks.

### üí° [Examples](examples/README.md)
Real-world configuration examples from production deployments across different use cases.

### üìö [Reference](reference/supported-resources.md)
Complete technical documentation, API references, and supported resources.

### ‚ö° [Advanced](advanced/scaling-advantages.md)
Scaling strategies, tool comparisons, and best practices for production environments.

## Questions/Issues?

If you have any issues or questions related to Simple-Container.com, please reach out at [support@simple-container.com](mailto:support@simple-container.com).

---

# Getting Started

## Overview

# Getting Started with Simple Container

Welcome to Simple Container! This section will help you get up and running quickly with your first deployment.

## Prerequisites

Before you begin, make sure you have:

- **Docker** installed and running on your machine
- **Cloud account** (AWS, GCP, or Azure) with appropriate permissions
- **Basic familiarity** with docker-compose for local development

## Overview

Simple Container follows a simple workflow:

1. **Create a parent stack** - Define your cloud infrastructure (databases, storage, etc.)
2. **Create service stacks** - Define your applications that use the infrastructure
3. **Deploy locally** - Test with `docker-compose up`
4. **Deploy to cloud** - Deploy with `sc deploy`

## Your First Deployment Journey

## Getting Started Steps

1. **[Installation](installation.md)** - Install Simple Container CLI and set up your environment
2. **[Quick Start](quick-start.md)** - Deploy your first application in minutes
3. **[Core Concepts](../concepts/main-concepts.md)** - Understand the fundamental concepts

## Next Steps

Once you've completed the getting started guide, explore:

- **[Deployment Guides](../guides/index.md)** - Step-by-step tutorials for different platforms
- **[Examples](../examples/README.md)** - Real-world configuration examples
- **[Reference Documentation](../reference/supported-resources.md)** - Complete API reference

## Need Help?

If you have any issues or questions, please reach out at [support@simple-container.com](mailto:support@simple-container.com).


---

## Installation

# **Guide: Installing and Using Simple Container**

This guide is for **both DevOps teams and developers** who want to install and start using **Simple Container** for **cloud-native microservices deployment**.

---

# **1Ô∏è‚É£ What is Simple Container?**
Simple Container is a **cloud-agnostic CI/CD tool** that simplifies the deployment of **microservices and static websites** across platforms like **Kubernetes, AWS ECS, and Google Cloud**.

‚úÖ **Supports multiple cloud providers** (AWS, GCP, Kubernetes).

‚úÖ **Easy configuration using `client.yaml` and `server.yaml`**.

‚úÖ **Secure secrets management** with SSH-based encryption.

‚úÖ **Automated infrastructure provisioning and deployments**.

---

# **2Ô∏è‚É£ Installing `sc`**
## **üîπ Install `sc` on Linux/macOS**
To install `sc`, run:
```sh
curl -s "https://dist.simple-container.com/sc.sh" | bash
```

‚úÖ This automatically downloads and installs `sc` in `/usr/local/bin`.

**Verify the installation:**
```sh
sc --version
```

---

# **3Ô∏è‚É£ Initializing `sc`**
After installing `sc`, you need to **set up SSH authentication** for secrets management.

## **üîπ Step 3: Set Up SSH Key for Secrets**
If you **already have an SSH key**, initialize `sc`:
```sh
sc secrets init
```
If you **need to generate a new SSH key**, use:
```sh
sc secrets init -g
```

‚úÖ This ensures that **secrets** can be securely encrypted and managed.

---

# **4Ô∏è‚É£ Setting Up the Parent Stack (For DevOps)**
The **DevOps team** must configure a **parent stack** (`server.yaml`) to define infrastructure and cloud resources.

## **üîπ Step 4: Create `secrets.yaml`**
```sh
mkdir -p .sc/stacks/devops
touch .sc/stacks/devops/secrets.yaml
```
Define **cloud authentication and secrets** in `secrets.yaml`:
```yaml
---
schemaVersion: 1.0

auth:
  aws:
    type: aws-token
    config:
      accessKey: "AKIAIOSFODNN7EXAMPLE"
      secretAccessKey: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
      region: "us-west-2"

values:
  CLOUDFLARE_API_TOKEN: "abcdefgh123456789"
  MONGODB_ATLAS_PUBLIC_KEY: "public-key-123"
  MONGODB_ATLAS_PRIVATE_KEY: "private-key-456"
```

‚úÖ This securely **stores AWS credentials and API tokens**.

---

## **üîπ Step 5: Define the Infrastructure in `server.yaml`**
Now, define **infrastructure resources** inside `.sc/stacks/devops/server.yaml`:
```yaml
---
schemaVersion: 1.0

provisioner:
  type: pulumi
  config:
    state-storage:
      type: s3-bucket
      config:
        credentials: "${auth:aws}"
        bucketName: myproject-sc-state
    secrets-provider:
      type: aws-kms
      config:
        credentials: "${auth:aws}"
        keyName: myproject-sc-kms-key

templates:
  stack-per-app:
    type: ecs-fargate
    config:
      credentials: "${auth:aws}"
      account: "${auth:aws.projectId}"

resources:
  staging:
    template: stack-per-app
    resources:
      mongodb:
        type: mongodb-atlas
        config:
          admins: [ "admin" ]
          developers: [ "developer1" ]
          instanceSize: "M10"
          region: "US_WEST_2"
          cloudProvider: AWS
          privateKey: "${secret:MONGODB_ATLAS_PRIVATE_KEY}"
          publicKey: "${secret:MONGODB_ATLAS_PUBLIC_KEY}"
```

### **üîπ Step 6: Provision the Parent Stack**
Once `server.yaml` is configured, **provision the infrastructure**:
```sh
sc provision -s devops
```

‚úÖ This **creates AWS infrastructure and configures MongoDB Atlas**.

---

# **5Ô∏è‚É£ Setting Up Services (For Developers)**
Once the **parent stack is ready**, developers can configure **`client.yaml`** to deploy services.

## **üîπ Step 7: Create `client.yaml` for a Microservice**
```sh
mkdir -p .sc/stacks/myservice
touch .sc/stacks/myservice/client.yaml
```
Define the **service configuration** inside `.sc/stacks/myservice/client.yaml`:
```yaml
---
schemaVersion: 1.0

stacks:
  staging:
    type: cloud-compose
    parent: myproject/devops
    config:
      domain: staging-myservice.myproject.com
      dockerComposeFile: ./docker-compose.yaml
      uses:
        - mongodb
      runs:
        - myservice
      env:
        DATABASE_HOST: "${resource:mongodb.host}"
        DATABASE_NAME: "${resource:mongodb.database}"
        DATABASE_USER: "${resource:mongodb.user}"
      secrets:
        DATABASE_PASSWORD: "${resource:mongodb.password}"
```

---

# **6Ô∏è‚É£ Deploying a Microservice**
Once **`client.yaml`** is defined, deploy the service.

### **üîπ Step 8: Deploy the Service to Staging**
```sh
sc deploy -s myservice -e staging
```
‚úÖ This **builds, pushes, and deploys the service to AWS ECS Fargate**.

---

# **7Ô∏è‚É£ Managing Secrets with `sc`**
### **üîπ Add a Secret File**
```sh
sc secrets add .env
```
‚úÖ Encrypts `.env` before committing to Git.

### **üîπ Hide Secrets Before Committing**
```sh
sc secrets hide
```
‚úÖ Ensures **no secrets are leaked in Git**.

### **üîπ Reveal Secrets After Pulling Changes**
```sh
sc secrets reveal
```
‚úÖ **Decrypts and restores** secret files locally.

---

# **8Ô∏è‚É£ Summary**
| Step                      | For                 | Command                                            | Purpose                              |
|---------------------------|---------------------|----------------------------------------------------|--------------------------------------|
| **Install `sc`**          | DevOps & Developers | `curl -s "https://dist.simple-container.com/sc.sh" | bash`                                | Installs Simple Container CLI |
| **Initialize Secrets**    | DevOps & Developers | `sc secrets init -g`                               | Generates SSH keys for secrets       |
| **Define Infrastructure** | DevOps              | `server.yaml`                                      | Configures cloud resources           |
| **Provision Infra**       | DevOps              | `sc provision -s devops`                           | Deploys AWS/GCP/Kubernetes resources |
| **Define a Service**      | Developers          | `client.yaml`                                      | Configures microservice deployment   |
| **Deploy a Service**      | Developers          | `sc deploy -s myservice -e staging`                | Deploys microservice to the cloud    |
| **Manage Secrets**        | DevOps              | `sc secrets add .env`                              | Encrypts a secret file               |

---

## Quick Start

# Quick Start Guide

This guide will help you deploy your first application with Simple Container in under 15 minutes.

## Prerequisites

- Simple Container CLI installed ([Installation Guide](installation.md))
- Access to a cloud provider (AWS, GCP, or Kubernetes cluster)
- Basic familiarity with YAML configuration

## Step 1: Initialize Your Project

Create a new directory for your project and initialize Simple Container:

```bash
mkdir my-first-app
cd my-first-app
sc init
```

This creates a `.sc/` directory with basic configuration files.

## Step 2: Choose Your Deployment Type

Simple Container supports several deployment patterns. For this quick start, we'll deploy a simple static website.

Create the parent stack configuration file `.sc/stacks/infrastructure/server.yaml`:

```yaml
# .sc/stacks/infrastructure/server.yaml
schemaVersion: 1.0

provisioner:
  type: pulumi
  config:
    state-storage:
      type: s3-bucket
      config:
        credentials: "${auth:aws}"
        bucketName: my-first-app-sc-state
    secrets-provider:
      type: aws-kms
      config:
        credentials: "${auth:aws}"
        keyName: my-first-app-sc-kms-key

templates:
  static-site:
    type: aws-static-website
    config:
      domain: "${secret:DOMAIN_NAME}"

resources:
  resources:
    prod:
      main-bucket:
        type: s3-bucket
        config:
          name: "my-app-${env:ENVIRONMENT}-bucket"
          allowOnlyHttps: true
```

Create the client stack configuration file `.sc/stacks/myapp/client.yaml`:

```yaml
# .sc/stacks/myapp/client.yaml
schemaVersion: 1.0
stacks:
  prod:
    type: static
    parent: infrastructure
    template: static-site
    config:
      bundleDir: "./dist"
      bucketName: "my-app-prod-bucket"
      location: "us-east-1"
      domain: "myapp.com"
      baseDnsZone: "myapp.com"
      indexDocument: "index.html"
      errorDocument: "index.html"
      provisionWwwDomain: true
```

## Step 3: Set Up Secrets

Create the secrets configuration file in the same directory as your server.yaml:

```bash
# Create the secrets file (same directory as server.yaml)
mkdir -p .sc/stacks/infrastructure
```

Create `.sc/stacks/infrastructure/secrets.yaml` with your actual AWS credentials:

```yaml
# .sc/stacks/infrastructure/secrets.yaml
schemaVersion: 1.0

# AWS authentication (used by provisioner for state storage and KMS)
auth:
  aws:
    type: aws-token
    config:
      account: "123456789012"                          # Your AWS Account ID
      accessKey: "AKIAIOSFODNN7EXAMPLE"               # Your AWS Access Key
      secretAccessKey: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"  # Your AWS Secret Key
      region: us-east-1                                # AWS region for resources

# Secret values (used in templates via ${secret:NAME} placeholders)
values:
  DOMAIN_NAME: "myapp.example.com"                     # Your actual domain name
```

**Important:** Replace the example values above with your actual AWS credentials and domain name.

Add the secrets file to Simple Container's managed secrets:

```bash
sc secrets add .sc/stacks/infrastructure/secrets.yaml
```

## Step 4: Deploy

Deploy your application:

```bash
sc deploy -s my-first-app -e prod
```

Simple Container will:

1. Create the S3 bucket
2. Set up CloudFront distribution
3. Configure DNS (if using Route53)
4. Deploy your static files

## Step 5: Verify Deployment

Your website should now be live at your configured domain!

You can verify the deployment by:
- Visiting your domain in a web browser
- Checking your AWS S3 bucket for the deployed files
- Verifying CloudFront distribution is active in AWS Console

## Next Steps

Now that you have your first deployment running, explore:

- **[Core Concepts](../concepts/main-concepts.md)** - Understand templates, resources, and environments
- **[Deployment Guides](../guides/index.md)** - Learn about ECS, GKE, and Kubernetes deployments
- **[Examples](../examples/README.md)** - See real-world configuration examples
- **[Secrets Management](../guides/secrets-management.md)** - Advanced secret handling

## Common Issues

**Domain not resolving?**

- DNS propagation can take up to 24 hours
- Check your DNS settings with your domain provider
- Verify the domain is correctly configured in your secrets.yaml file

**Deployment failed?**

- Verify your secrets.yaml file contains correct AWS credentials
- Check that `sc secrets add .sc/stacks/prod/secrets.yaml` was run successfully
- Ensure your domain is properly configured

**Need help?**

- Join our community or contact [support@simple-container.com](mailto:support@simple-container.com)


---

# Core Concepts

## Main Concepts

# **Separation of Parent Stack and Service Stack in Simple Container**

## **Architecture Overview**

```mermaid
graph TB
    subgraph PS ["üèóÔ∏è PARENT STACK (DevOps Owned)"]
        SY["üìÑ server.yaml<br/>resources:<br/>  production:<br/>    postgres-db: ...<br/>    redis-cache: ...<br/>    s3-storage: ...<br/>provisioner: ...<br/>templates: ..."]
        IR["Infrastructure Resources<br/>‚Ä¢ Databases (RDS, MongoDB)<br/>‚Ä¢ Storage (S3, GCS)<br/>‚Ä¢ Clusters (EKS, GKE)<br/>‚Ä¢ Networking, Security"]
        PSN["üë• Managed by: DevOps Team<br/>üîÑ Updated: Infrastructure changes"]
    end
    
    subgraph SS ["üöÄ SERVICE STACK (Developer Owned)"]
        CY["üìÑ client.yaml<br/>parent: org/infrastructure<br/>config:<br/>  uses: [postgres-db, redis]<br/>  runs: [web-app]<br/>  env:<br/>    DB_URL: \${resource:...}"]
        DC["üìÑ docker-compose.yaml<br/>services:<br/>  web-app: ...<br/>  worker: ..."]
        AS["Application Services<br/>‚Ä¢ Microservices<br/>‚Ä¢ Web Applications<br/>‚Ä¢ Background Jobs<br/>‚Ä¢ APIs & Services"]
        SSN["üë©‚Äçüíª Managed by: Developers<br/>üîÑ Updated: New services/features"]
    end
    
    subgraph DF ["üìã DEPLOYMENT FLOW"]
        S1["1Ô∏è‚É£ DevOps defines<br/>infrastructure<br/>(server.yaml)"]
        S2["2Ô∏è‚É£ Developers define<br/>services<br/>(client.yaml)"]
        S3["3Ô∏è‚É£ Simple Container<br/>orchestrates<br/>deployment"]
        
        S1 --> S2
        S2 --> S3
    end
    
    PS --> SS
    SS -.-> PS
    
    classDef parentStack fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef serviceStack fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    classDef deployFlow fill:#fff8e1,stroke:#f57f17,stroke-width:2px
    
    class PS,SY,IR,PSN parentStack
    class SS,CY,DC,AS,SSN serviceStack
    class DF,S1,S2,S3 deployFlow
```

**üìà SCALING BENEFITS:**

- üöÄ **500x faster** customer onboarding (5 min vs 2-3 days)
- üìä **90% configuration reduction** (500 vs 5000+ lines)
- üë• **5x operational efficiency** (1 DevOps per 100+ customers)
- üí∞ **70% cost reduction** through resource sharing
- ‚ö° **Zero-downtime** service deployments

## **Introduction**

One of the key principles of **Simple Container** is the **separation of concerns** between **infrastructure management** and **microservice deployment**.
This is achieved by **separating the "parent stack" (managed by DevOps) from the "service stack" (managed by developers)**.

This guide explains:

‚úÖ **What the parent stack is and how it works**

‚úÖ **What the service stack is and how it works**

‚úÖ **How this separation benefits both DevOps and developers**

---

# **1Ô∏è‚É£ What is the Parent Stack?**

The **parent stack** is the **core infrastructure** required for microservices to run. It is **managed by DevOps** and provides:

- **Cloud infrastructure** (Kubernetes clusters, AWS ECS clusters, databases, storage, networking)
- **Secrets management** (via Kubernetes Secrets, AWS Secrets Manager, or Google Secret Manager)
- **Centralized state management** (so infrastructure is consistent across environments)
- **Provisioning of shared resources** (databases, message queues, API gateways)

### **Who Manages the Parent Stack?**
‚û°Ô∏è **DevOps teams** define and maintain the parent stack.

### **When is the Parent Stack Modified?**
‚û°Ô∏è Only when **adding new infrastructure resources** (e.g., a new database, message queue, or cloud provider).

---

# **2Ô∏è‚É£ What is the Service Stack?**

The **service stack** represents an **individual microservice** that a **developer wants to deploy**. It consumes infrastructure from the **parent stack** but does not modify it.

- **Developers only configure their microservice's deployment settings**
- **Microservices automatically connect to infrastructure provisioned by the parent stack**
- **No need to request DevOps intervention for every new service**

### **Who Manages the Service Stack?**
‚û°Ô∏è **Developers** define and maintain their own service configurations.

### **When is the Service Stack Modified?**
‚û°Ô∏è Whenever a **new microservice is added** or an **existing service is updated**.

---

# **3Ô∏è‚É£ Key Differences Between Parent Stack and Service Stack**

| Feature                | Parent Stack (DevOps)                           | Service Stack (Developers)          |
|------------------------|-------------------------------------------------|-------------------------------------|
| **Purpose**            | Defines shared infrastructure                   | Defines microservice deployment     |
| **Managed By**         | DevOps                                          | Developers                          |
| **Configuration File** | `server.yaml`                                   | `client.yaml`                       |
| **Modified When**      | Infrastructure changes (new DB, queue, cluster) | New service added or updated        |
| **Includes**           | Databases, secrets, cloud resources             | Microservice dependencies & scaling |

---

# **4Ô∏è‚É£ Why This Separation Matters: Scaling Advantages**

## **Multi-Dimensional Resource Allocation**

**Traditional Approach - Manual Resource Management:**
```yaml
# Each customer needs separate infrastructure definition
resource "aws_ecs_cluster" "customer_a" {
  name = "customer-a-cluster"
}

resource "aws_rds_instance" "customer_a_db" {
  identifier = "customer-a-database"
  engine     = "postgres"
  # ... complex configuration
}
```

**Simple Container - Flexible Resource Sharing:**
```yaml
# server.yaml - Define resource pools once
resources:
  production:
    resources:
      # Shared resources for standard customers
      mongodb-shared-us:
        type: mongodb-atlas
        config:
          clusterName: shared-us
          instanceSize: M30
          
      # Dedicated resources for enterprise
      mongodb-enterprise-1:
        type: mongodb-atlas
        config:
          clusterName: enterprise-1
          instanceSize: M80

# client.yaml - Customers choose resources flexibly
stacks:
  standard-customer-1:
    uses: [mongodb-shared-us]  # Shared resource
    
  enterprise:
    uses: [mongodb-enterprise-1]  # Dedicated resource
```

## **Core Scaling Benefits:**

- **Developers focus on coding, not cloud infrastructure** - **15 minutes** to first deployment vs **2-3 days**
- **DevOps standardizes infrastructure without worrying about microservices** - **Template updates apply to all customers**
- **Adding a new microservice is self-service** - **5 minutes** vs **1-2 days** DevOps bottleneck
- **Security is maintained by isolating infrastructure from microservices** - **Automatic namespace isolation**
- **Resource Pool Management** - Define resources once, allocate flexibly
- **Cost Optimization** - Share resources among compatible customers
- **Easy Migration** - Move customers between resource pools by changing `uses` directive

This separation **scales exceptionally well** as organizations grow, preventing bottlenecks where **DevOps must manually configure every microservice**.

---

# **5Ô∏è‚É£ Quantified Scaling Benefits**

## **Operational Scalability Metrics**

| Metric                        | Traditional Approach          | Simple Container            | Improvement        |
|-------------------------------|-------------------------------|-----------------------------|--------------------|
| **DevOps to Customer Ratio**  | 1:10-20 customers             | 1:100+ customers            | **5x efficiency**  |
| **Customer Onboarding Time**  | 2-3 days                      | 5 minutes                   | **500x faster**    |
| **Configuration Lines**       | 5000+ lines for 100 customers | 500 lines for 100 customers | **90% reduction**  |
| **Infrastructure Drift Risk** | High (manual management)      | Low (template-based)        | **Reduced errors** |

## **Development Velocity Impact**

**Traditional Approach:**

- **Time to First Deployment**: 2-3 days (infrastructure setup)
- **Developer Onboarding**: 2-4 weeks (Kubernetes/AWS training)
- **Feature Development**: Blocked by infrastructure changes

**Simple Container:**

- **Time to First Deployment**: 15 minutes (configuration only)
- **Developer Onboarding**: 1-2 hours (simple YAML configuration)
- **Feature Development**: Independent of infrastructure

## **Cost Optimization Results**

**Simple Container achieves:**

- **70% cost reduction** through intelligent resource sharing
- **80% staff reduction** in operational overhead
- **1 DevOps engineer per 100+ customers** vs 1 per 10-20 traditional
- **Automatic right-sizing** and scaling optimization

---

# **6Ô∏è‚É£ Real-World Scaling Scenarios**

## **Scenario 1: Adding 100 New Customers**

**Traditional Kubernetes/ECS:**
```bash
# For each of 100 customers, DevOps must:
1. Create namespace/cluster
2. Define deployment YAML (50+ lines each)
3. Configure ingress and SSL certificates
4. Set up monitoring and logging
5. Create secrets manually

# Result: 5000+ lines of configuration
# Time: 2-3 days per customer = 200-300 days
```

**Simple Container:**
```yaml
# For each of 100 customers, developers add:
customer-001:
  parentEnv: production
  config:
    domain: customer001.myapp.com
    secrets:
      CUSTOMER_SETTINGS: ${env:CUSTOMER_001_SETTINGS}

# Result: 5 lines per customer = 500 lines total
# Time: 5 minutes per customer = 8.3 hours total
```

## **Scenario 2: Performance Tier Migration**

**Traditional Approach:**

- Manual infrastructure rebuild
- Data migration downtime
- Complex rollback procedures
- High risk of errors

**Simple Container:**
```yaml
# Before: Customer on shared resources
customer-enterprise:
  uses: [mongodb-shared-us]
  
# After: Customer on dedicated resources (one line change!)
customer-enterprise:
  uses: [mongodb-enterprise-dedicated]
  
# Automatic migration, zero downtime, easy rollback
```

---

# **Conclusion**

The **separation of parent stack and service stack** in Simple Container ensures:

- **500x faster customer onboarding** (5 minutes vs 2-3 days)
- **90% reduction in configuration complexity** (500 vs 5000+ lines)
- **5x operational efficiency** (1 DevOps per 100+ vs 10-20 customers)
- **70% cost reduction** through intelligent resource sharing
- **Zero downtime migrations** with one-line configuration changes
- **Developer self-service** without infrastructure expertise requirements

By adopting this separation, organizations can **scale from startup to enterprise without operational complexity growth**, transforming container orchestration from a complex infrastructure challenge into a simple configuration management task.

---

## Template Placeholders

# **Template Placeholders Guide**

Simple Container provides a powerful template engine that allows you to use dynamic placeholders in your configuration files. These placeholders are resolved at deployment time, enabling flexible and reusable configurations.

**Golang Implementation Reference:** `pkg/provisioner/placeholders/placeholders.go`

## **Template Syntax**

All template placeholders use the syntax: `${extension:path}`

Where:

- `extension` is one of the 9 supported template extensions
- `path` specifies what value to retrieve from that extension

**Important:** All template placeholders can be used in both `client.yaml` and `server.yaml` files, providing flexibility for dynamic configuration at both the parent stack (DevOps) and client stack (developer) levels.

## **Supported Template Extensions**

Simple Container supports 9 template extensions for different types of dynamic values:

### **1. Environment Variables** (`env`)

Access environment variables from the system where Simple Container is running.

**Syntax:** `${env:VARIABLE_NAME}`

**With Default Value:** `${env:VARIABLE_NAME:default_value}`

**Examples:**
```yaml
# Basic environment variable
credentials: "${env:AWS_ACCESS_KEY_ID}"

# Environment variable with default value
region: "${env:AWS_REGION:us-east-1}"

# Database password from environment
password: "${env:DB_PASSWORD}"
```

**Real-World Usage:**

**In server.yaml (Parent Stack):**
```yaml
# Production example example
provisioner:
  config:
    secrets-provider:
      type: aws-kms
      config:
        keyName: "${env:KMS_KEY_NAME}"

resources:
  resources:
    production:
      resources:
        mongodb:
          config:
            publicKey: "${env:MONGODB_ATLAS_PUBLIC_KEY}"
            privateKey: "${env:MONGODB_ATLAS_PRIVATE_KEY}"
```

**In client.yaml (Client Stack):**
```yaml
# Environment-specific configuration
stacks:
  staging:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      domain: "${env:STAGING_DOMAIN:staging.myapp.com}"
      env:
        NODE_ENV: "${env:NODE_ENV:development}"
        API_URL: "${env:API_URL}"
```

### **2. Git Repository Information** (`git`)

Access comprehensive information about the current Git repository, including paths, commit details, and branch information.

**Available Properties:**

- `root` - Git repository root directory path
- `commit.short` - Short commit hash (7 characters)
- `commit.full` - Full commit hash
- `branch` - Clean branch name (slashes replaced with dashes)
- `branch.raw` - Raw branch name as returned by git
- `branch.clean` - Clean branch name (same as `branch`)

**Syntax:** `${git:property}`

**Examples:**
```yaml
# Reference files relative to git root
dockerComposeFile: ${git:root}/docker-compose.yaml

# Build context from git root
image:
  context: ${git:root}/src
  dockerfile: ${git:root}/src/Dockerfile

# Dynamic container tagging with git info
image:
  name: myapp
  tag: "${git:branch}-${git:commit.short}"  # e.g., "main-a1b2c3d"

# Environment-specific configurations
environment: "${git:branch}"               # e.g., "staging"
deployment_id: "${git:branch}-${date:timestamp}"
```

**Real-World Usage:**

**In client.yaml (Client Stack):**
```yaml
# Advanced git integration for dynamic deployments
stacks:
  "${git:branch}":  # Dynamic stack name from git branch
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      dockerComposeFile: ${git:root}/docker-compose.yaml
      image:
        dockerfile: ${git:root}/src/Dockerfile
        context: ${git:root}/src
        name: myapp
        tag: "${git:branch}-${git:commit.short}"  # Dynamic tagging
      domain: "${git:branch}.myapp.com"  # Branch-specific domains
      env:
        GIT_COMMIT: "${git:commit.short}"
        GIT_BRANCH: "${git:branch.clean}"
        BUILD_VERSION: "${git:commit.short}-${date:timestamp}"
```

**In server.yaml (Parent Stack):**
```yaml
# Template configuration with git-relative paths and versioning
templates:
  my-template:
    type: ecs-fargate
    config:
      buildContext: ${git:root}/build
      configPath: ${git:root}/config
      version: "${git:commit.short}"
      metadata:
        git_branch: "${git:branch.raw}"
        git_commit: "${git:commit.full}"
```

### **3. Project Paths** (`project`)

Access project-specific paths and information. The project extension provides consistent path resolution across different environments.

**Available Properties:**

- `root` - Project root directory (uses git working directory when available, falls back to current directory)

**Syntax:** `${project:root}`

**Examples:**
```yaml
# Reference files relative to project root
dockerComposeFile: docker-compose.yaml  # References ${project:root}/docker-compose.yaml

# Build contexts and file paths
image:
  dockerfile: ${project:root}/Dockerfile
  context: ${project:root}

# Configuration file references
configFiles:
  - ${project:root}/config/app.yaml
  - ${project:root}/config/database.yaml
```

**Real-World Usage:**

**In client.yaml (Client Stack):**
```yaml
# Docker Compose and build configurations
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      dockerComposeFile: docker-compose.yaml  # Resolves to ${project:root}/docker-compose.yaml
      runs: [app, worker]
      image:
        dockerfile: ${project:root}/Dockerfile
        context: ${project:root}/src
```

**Environment Resolution:**

- **GitHub Actions**: `${project:root}` ‚Üí `/github/workspace`
- **Local development**: `${project:root}` ‚Üí Current project directory
- **Git context**: `${project:root}` ‚Üí Git working directory

### **4. Date and Time Information** (`date`)

Access current date and time information for timestamped configurations, versioning, and deployment tracking.

**Available Formats:**

- `time` - ISO-like format: 2024-10-24T20:46:41
- `dateOnly` - Date only: 2024-10-24
- `timestamp` - Unix timestamp: 1698167445
- `iso8601` - Full ISO 8601: 2024-10-24T20:46:41Z
- `rfc3339` - RFC 3339 format: 2024-10-24T20:46:41Z
- `year` - Year: 2024
- `month` - Month: 10 (zero-padded)
- `day` - Day: 24 (zero-padded)
- `hour` - Hour: 20 (24-hour format, zero-padded)
- `minute` - Minute: 46 (zero-padded)
- `second` - Second: 41 (zero-padded)

**Syntax:** `${date:format}`

**Examples:**
```yaml
# Deployment metadata with timestamps
metadata:
  deployed_at: "${date:iso8601}"    # 2024-10-24T20:46:41Z
  version: "${git:commit.short}"    # a1b2c3d
  build_date: "${date:dateOnly}"    # 2024-10-24

# Timestamped resource naming
backup:
  schedule: "0 2 * * *"  # 2 AM daily
  retention: "${date:dateOnly}-${git:commit.short}"  # 2024-10-24-a1b2c3d

# CI/CD integration
env:
  BUILD_DATE: "${date:dateOnly}"
  BUILD_TIME: "${date:time}"
  BUILD_TIMESTAMP: "${date:timestamp}"
  DEPLOYMENT_ID: "${git:branch}-${date:timestamp}"
```

**Real-World Usage:**

**In client.yaml (Client Stack):**
```yaml
# Timestamped deployments for tracking
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      image:
        name: myapp
        tag: "${git:branch}-${git:commit.short}-${date:timestamp}"
      env:
        DEPLOYMENT_TIME: "${date:iso8601}"
        BUILD_VERSION: "${date:dateOnly}.${git:commit.short}"
      metadata:
        deployed_at: "${date:rfc3339}"
        git_info: "${git:branch}@${git:commit.short}"
```

**In server.yaml (Parent Stack):**
```yaml
# Backup and retention policies with dates
resources:
  app-storage:
    type: s3-bucket
    config:
      name: "mycompany-${stack:name}-${date:year}${date:month}"
      lifecycle:
        backup_retention: "${date:year}-${date:month}-${date:day}"
```

### **5. Authentication Configurations** (`auth`)

Access authentication configurations defined in your secrets.yaml file.

**Syntax:**

- `${auth:auth_name}` - Get credentials value
- `${auth:auth_name.projectId}` - Get project ID (for cloud providers)

**Examples:**
```yaml
# AWS authentication
credentials: "${auth:aws-main}"
account: "${auth:aws-main.projectId}"

# GCP authentication  
projectId: "${auth:gcloud.projectId}"
credentials: "${auth:gcloud}"
```

**Real-World Usage:**

**In server.yaml (Parent Stack):**
```yaml
# Production example
templates:
  stack-per-app-gke:
    type: gcp-gke-autopilot
    config:
      projectId: "${auth:gcloud.projectId}"
      credentials: "${auth:gcloud}"

resources:
  resources:
    prod:
      resources:
        gke-autopilot-res:
          type: gcp-gke-autopilot-cluster
          config:
            projectId: "${auth:gcloud.projectId}"
            credentials: "${auth:gcloud}"
```

**In client.yaml (Client Stack):**
```yaml
# Client stack referencing parent authentication
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [gke-cluster]
      env:
        GCP_PROJECT: "${auth:gcloud.projectId}"
        CLUSTER_CREDENTIALS: "${auth:gcloud}"
```

### **6. Secrets Management** (`secret`)

Access secrets defined in your secrets.yaml file with support for inheritance.

**Syntax:** `${secret:SECRET_NAME}`

**Examples:**
```yaml
# API tokens
credentials: "${secret:CLOUDFLARE_API_TOKEN}"

# Database credentials
password: "${secret:MONGODB_ATLAS_PRIVATE_KEY}"

# Service keys
apiKey: "${secret:OPENAI_API_KEY}"
```

**Real-World Usage:**
```yaml
# Production example
resources:
  registrar:
    type: cloudflare
    config:
      credentials: "${secret:CLOUDFLARE_API_TOKEN}"
      accountId: 23c5ca78cfb4721d9a603ed695a2623e
      zoneName: amagenta.ai

# From billing system example
stacks:
  production:
    config:
      secrets:
        API_KEY: ${secret:billing-api-key}
        STRIPE_SECRET_KEY: ${secret:stripe-secret-key}
```

**Inheritance Support:**
Secrets support inheritance from parent stacks. If a secret is not found in the current stack, Simple Container will look in the parent stack.

### **7. Stack Variables** (`var`)

Access custom variables defined in your stack configuration.

**Syntax:** `${var:VARIABLE_NAME}`

**Examples:**
```yaml
# Define variables in server.yaml
variables:
  app_version: "1.2.3"
  environment: "production"

# Use variables in configuration
image: "myapp:${var:app_version}"
domain: "${var:environment}.myapp.com"
```

**Real-World Usage:**
```yaml
# Stack variables for environment-specific configuration
variables:
  cluster_size: "3"
  instance_type: "m5.large"

resources:
  my-cluster:
    config:
      nodeCount: "${var:cluster_size}"
      instanceType: "${var:instance_type}"
```

### **8. Stack Metadata** (`stack`)

Access metadata about the current stack being processed.

**Available Properties:**

- `name` - Current stack name

**Syntax:** `${stack:name}`

**Examples:**
```yaml
# Use stack name in resource naming
bucketName: "myapp-${stack:name}-storage"

# Environment-specific configuration
domain: "${stack:name}.myapp.com"

# Stack-specific tags
tags:
  Environment: "${stack:name}"
  Stack: "${stack:name}"
```

**Real-World Usage:**
```yaml
# Environment-specific resource naming
resources:
  app-storage:
    type: s3-bucket
    config:
      name: "mycompany-${stack:name}-storage"
      
stacks:
  staging:
    config:
      domain: "${stack:name}-api.mycompany.com"  # Results in: staging-api.mycompany.com
  production:
    config:
      domain: "api.mycompany.com"
```

### **9. Current User Information** (`user`)

Access information about the current system user running Simple Container.

**Available Properties:**

- `homeDir` or `home` - User's home directory
- `username` - Username
- `id` - User ID
- `name` - Full name

**Syntax:** `${user:property}`

**Examples:**
```yaml
# Local development paths
statePath: "${user:homeDir}/.sc/state"

# User-specific configuration
configPath: "${user:home}/.config/myapp"

# User identification
deployedBy: "${user:username}"
```

**Real-World Usage:**
```yaml
# From local development configuration
provisioner:
  config:
    state-storage:
      type: fs
      config:
        path: "file:///${user:homeDir}/.sc/pulumi/state"

# User-specific local paths
volumes:
  - "${user:home}/.aws:/root/.aws:ro"
  - "${user:home}/.kube:/root/.kube:ro"
```

## **Advanced Template Patterns**

### **Combining Multiple Placeholders**

You can combine multiple placeholders in a single value:

```yaml
# Combine stack name with environment variable
domain: "${stack:name}-${env:COMPANY_DOMAIN:example.com}"

# User and project-based paths
buildPath: "${user:home}/builds/${project:root}"

# Dynamic container tagging with git and date info
imageName: "${git:branch.clean}-${stack:name}:${git:commit.short}"
version: "${date:dateOnly}.${git:commit.short}"

# Timestamped deployment configurations
deployment_id: "${git:branch}-${date:timestamp}"
build_metadata: "${date:iso8601}-${git:commit.full}"
```

### **Environment-Specific Configuration**

```yaml
# Different configurations based on environment
instanceSize: "${env:INSTANCE_SIZE:M10}"
region: "${env:AWS_REGION:us-east-1}"

# Environment-specific secrets (use simple references)
apiKey: "${secret:api-key}"
dbPassword: "${secret:db-password}"
```

### **Cross-Stack References**

```yaml
# Reference parent stack and its resources
parent: myorg/infrastructure
parentEnv: production
config:
  uses: [postgres-db, s3-storage]
  runs: [web-app]
  env:
    DATABASE_URL: "${resource:postgres-db.url}"
    S3_BUCKET: "${resource:s3-storage.name}"
```

## **Template Resolution Order**

Simple Container resolves templates in the following order:

1. **Stack Inheritance Resolution** - Resolve parent stack relationships
2. **Template Processing** - Process all placeholders in each stack
3. **Extension Resolution** - Resolve each extension type:
   - `env` - Environment variables
   - `git` - Git repository information (enhanced with commit/branch data)
   - `project` - Project paths and information
   - `date` - Date and time information
   - `auth` - Authentication configurations
   - `secret` - Secrets (with inheritance support)
   - `var` - Stack variables
   - `stack` - Stack metadata
   - `user` - Current user information

## **Best Practices**

### **1. Use Environment Variables for Sensitive Data**
```yaml
# Good - sensitive data from environment
credentials: "${env:AWS_SECRET_ACCESS_KEY}"

# Avoid - hardcoded sensitive data
credentials: "AKIAIOSFODNN7EXAMPLE"
```

### **2. Provide Default Values for Optional Configuration**
```yaml
# Good - provides fallback
region: "${env:AWS_REGION:us-east-1}"
timeout: "${env:TIMEOUT:30}"

# Consider - may fail if not set
region: "${env:AWS_REGION}"
```

### **3. Use Project Root for Relative Paths**
```yaml
# Good - relative to project root (works in all environments)
dockerComposeFile: docker-compose.yaml  # References ${project:root}/docker-compose.yaml
image:
  dockerfile: ${project:root}/Dockerfile  # ‚úÖ Full path to file
  context: ${project:root}                # ‚úÖ Directory path

# Also good - explicit git root for git-specific operations
buildContext: ${git:root}/build

# ‚ùå COMMON MISTAKE - Missing filename
image:
  dockerfile: ${project:root}  # ‚ùå This is a directory, not a file!
  
# Avoid - absolute paths that may not exist
dockerComposeFile: /home/user/project/docker-compose.yaml
```

### **3a. Leverage Dynamic Git Information**
```yaml
# Good - dynamic tagging and deployment tracking
image:
  name: myapp
  tag: "${git:branch}-${git:commit.short}"  # e.g., "main-a1b2c3d"

metadata:
  version: "${date:dateOnly}.${git:commit.short}"  # e.g., "2024-10-24.a1b2c3d"
  deployed_at: "${date:iso8601}"

# Environment-specific configurations
domain: "${git:branch}.myapp.com"  # staging.myapp.com, production.myapp.com
```

### **4. Leverage Stack Names for Environment-Specific Resources**
```yaml
# Good - automatically environment-specific
bucketName: "myapp-${stack:name}-storage"
domain: "${stack:name}.myapp.com"

# Avoid - requires manual changes per environment
bucketName: "myapp-production-storage"
```

### **5. Use Secrets for API Keys and Tokens**
```yaml
# Good - managed through secrets system
apiToken: "${secret:CLOUDFLARE_API_TOKEN}"

# Avoid - environment variables for secrets (less secure)
apiToken: "${env:CLOUDFLARE_API_TOKEN}"
```

## **Error Handling**

When a template placeholder cannot be resolved, Simple Container will:

1. **Environment Variables** - Return empty string if not set (unless default provided)
2. **Git Information** - Return error if not in a git repository or if git command fails
3. **Project Paths** - Fall back to current working directory if git context unavailable
4. **Date Information** - Return error for unknown format (with helpful format suggestions)
5. **Auth Configurations** - Return error if auth not found
6. **Secrets** - Return error if secret not found (checks inheritance)
7. **Variables** - Return error if variable not defined in stack
8. **Stack Metadata** - Return error if property not available
9. **User Information** - Return error if user information cannot be determined

## **Real-World Examples from Production**

### **Multi-Cloud Parent Stack**
```yaml
# Production example
provisioner:
  config:
    state-storage:
      type: gcp-bucket
      config:
        credentials: "${auth:gcloud}"
        projectId: "${auth:gcloud.projectId}"
        bucketName: mycompany-sc-state
        location: europe-west3

templates:
  stack-per-app-gke:
    type: gcp-gke-autopilot
    config:
      projectId: "${auth:gcloud.projectId}"
      credentials: "${auth:gcloud}"
      gkeClusterResource: gke-autopilot-res
      artifactRegistryResource: artifact-registry-res
```

### **AWS Multi-Region Setup**
```yaml
# Production example
templates:
  stack-per-app-eu:
    type: ecs-fargate
    config:
      credentials: "${auth:aws-eu}"
      account: "${auth:aws-eu.projectId}"
  
  lambda-eu:
    type: aws-lambda
    config:
      credentials: "${auth:aws-eu}"
      account: "${auth:aws-eu.projectId}"
```

### **Environment-Specific MongoDB Configuration**
```yaml
# Production example
resources:
  staging:
    resources:
      mongodb:
        type: mongodb-atlas
        config:
          publicKey: "${secret:MONGODB_ATLAS_PUBLIC_KEY}"
          privateKey: "${secret:MONGODB_ATLAS_PRIVATE_KEY}"
          orgId: 67bc72f86e5ef36f7584d7d0
          instanceSize: "M10"
          region: "EU_CENTRAL_1"
```

### **Local Development Configuration**
```yaml
# Local development with user-specific paths
provisioner:
  config:
    state-storage:
      type: fs
      config:
        path: "file:///${user:homeDir}/.sc/pulumi/state"

# Project-relative paths for consistency across environments
stacks:
  local:
    config:
      dockerComposeFile: docker-compose.yaml  # Uses ${project:root}/docker-compose.yaml
      buildContext: ${project:root}/src
      env:
        BUILD_DATE: "${date:dateOnly}"
        GIT_COMMIT: "${git:commit.short}"
```

## **See Also**

- [Supported Resources Reference](supported-resources.md) - Complete resource configuration guide
- [Examples Directory](examples/) - Production-tested configuration examples
- [Parent Stack Examples](examples/parent-stacks/) - Multi-region and hybrid cloud configurations
- [Authentication Guide](authentication.md) - Setting up auth configurations for template placeholders

This template system enables Simple Container to provide flexible, reusable, and secure configuration management across different environments and deployment scenarios.

---

## Advanced Template Placeholders

# Advanced Template Placeholders & Compute Processors

Simple Container provides an advanced template placeholder system that automatically injects environment variables and secrets when resources are used in your deployments. This is powered by **compute processors** that understand how to connect your services to the resources they depend on.

## Overview

When you reference resources in your `client.yaml` using the `uses` or `dependencies` sections, Simple Container's compute processors automatically:

1. **Inject environment variables** with connection details, credentials, and configuration
2. **Inject secrets** for sensitive data like passwords and API keys
3. **Resolve template placeholders** like `${resource:name.property}` and `${dependency:name.property}`

This eliminates the need to manually configure connection strings, credentials, and other resource-specific details.

## Environment Variable Precedence

Simple Container follows a specific precedence order for environment variables:

1. **docker-compose.yaml** - Base values for local development
2. **client.yaml `env` section** - Overrides docker-compose values for deployment
3. **client.yaml `secrets` section** - Highest precedence for sensitive values

This design allows you to have docker-compose.yaml files that work locally for development while automatically getting production values when deployed via Simple Container.

## How It Works

### Resource Usage (`uses` section)

When you reference a resource from your parent stack:

```yaml
# client.yaml
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [postgres-db, s3-storage]
      runs: [web-app, worker]
```

The compute processors automatically inject the necessary environment variables and secrets into your containers.

### Cross-Service Dependencies (`dependencies` section)

When you need to connect to another service's shared resources:

```yaml
# client.yaml
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [redis-cache]
      runs: [api-service]
      dependencies:
        - name: billing-db
          owner: myproject/billing-service
          resource: postgres-cluster
```

## Template Placeholders

### Resource Placeholders

Access properties of resources defined in your parent stack in client.yaml:

```yaml
# client.yaml - Using resource placeholders in env/secrets sections
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [postgres-db, s3-storage, redis-cache]
      runs: [web-app]
      env:
        DATABASE_URL: "${resource:postgres-db.url}"
        S3_BUCKET: "${resource:s3-storage.bucket}"
        REDIS_HOST: "${resource:redis-cache.host}"
```

### Dependency Placeholders

Access properties of resources from other services in client.yaml:

```yaml
# client.yaml - Using dependency placeholders in env/secrets sections
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [redis-cache]
      runs: [api-service]
      dependencies:
        - name: billing-db
          owner: myproject/billing-service
          resource: postgres-cluster
        - name: shared-files
          owner: myproject/file-service
          resource: s3-storage
      env:
        BILLING_DB_URL: "${dependency:billing-db.url}"
        SHARED_STORAGE: "${dependency:shared-files.bucket}"
```

## Compute Processors by Resource Type

### AWS Resources

#### S3 Bucket
AWS S3 bucket connection details and credentials.

**Auto-injected Environment Variables:**

- `S3_<BUCKET_NAME>_REGION` - Bucket region (specific bucket)
- `S3_<BUCKET_NAME>_BUCKET` - Bucket name (specific bucket)
- `S3_<BUCKET_NAME>_ACCESS_KEY` - Access key ID (specific bucket, secret)
- `S3_<BUCKET_NAME>_SECRET_KEY` - Secret access key (specific bucket, secret)
- `S3_REGION` - Generic bucket region
- `S3_BUCKET` - Generic bucket name
- `S3_ACCESS_KEY` - Generic access key ID (secret)
- `S3_SECRET_KEY` - Generic secret access key (secret)

**Template Placeholders:**

- `${resource:bucket-name.bucket}` - Bucket name
- `${resource:bucket-name.region}` - Bucket region
- `${resource:bucket-name.access-key}` - Access key ID
- `${resource:bucket-name.secret-key}` - Secret access key

#### RDS PostgreSQL
**Auto-injected Environment Variables:**

- `PGHOST_<NAME>` - Database host
- `PGUSER_<NAME>` - Database username
- `PGPORT_<NAME>` - Database port
- `PGDATABASE_<NAME>` - Database name
- `PGPASSWORD_<NAME>` - Database password (secret)
- `PGHOST` - Generic database host
- `PGUSER` - Generic database username
- `PGPORT` - Generic database port
- `PGDATABASE` - Generic database name
- `PGPASSWORD` - Generic database password (secret)

**Template Placeholders:**

- `${resource:postgres-name.url}` - Full connection URL
- `${resource:postgres-name.host}` - Database host
- `${resource:postgres-name.port}` - Database port
- `${resource:postgres-name.user}` - Database username
- `${resource:postgres-name.database}` - Database name
- `${resource:postgres-name.password}` - Database password

#### RDS MySQL
AWS RDS MySQL database connection details and credentials.

**Auto-injected Environment Variables:**

- `MYSQL_HOST_<NAME>` - Database host (specific database)
- `MYSQL_USER_<NAME>` - Database username (specific database)
- `MYSQL_PORT_<NAME>` - Database port (specific database)
- `MYSQL_DB_<NAME>` - Database name (specific database)
- `MYSQL_PASSWORD_<NAME>` - Database password (specific database, secret)
- `MYSQL_HOST` - Generic database host
- `MYSQL_USER` - Generic database username
- `MYSQL_PORT` - Generic database port
- `MYSQL_DB` - Generic database name
- `MYSQL_PASSWORD` - Generic database password (secret)

**Template Placeholders:**

- `${resource:mysql-name.host}` - Database host
- `${resource:mysql-name.port}` - Database port
- `${resource:mysql-name.user}` - Database username
- `${resource:mysql-name.database}` - Database name
- `${resource:mysql-name.password}` - Database password
- `${resource:mysql-name.url}` - Database endpoint URL

### GCP Resources

#### GCP Bucket
Google Cloud Storage bucket with S3-compatible access via HMAC keys.

**Auto-injected Environment Variables:**

**Bucket-Specific Variables** (where `BUCKET_NAME` is the sanitized bucket name):
- `GCS_<BUCKET_NAME>_BUCKET` - Bucket name
- `GCS_<BUCKET_NAME>_LOCATION` - Bucket location/region
- `GCS_<BUCKET_NAME>_ACCESS_KEY` - HMAC access key ID (secret)
- `GCS_<BUCKET_NAME>_SECRET_KEY` - HMAC secret key (secret)
- `GCS_<BUCKET_NAME>_ENDPOINT` - GCS S3-compatible endpoint

**S3-Compatible Variables** (for applications expecting AWS S3):
- `S3_<BUCKET_NAME>_BUCKET` - Bucket name
- `S3_<BUCKET_NAME>_REGION` - Bucket location/region
- `S3_<BUCKET_NAME>_ACCESS_KEY` - HMAC access key ID (secret)
- `S3_<BUCKET_NAME>_SECRET_KEY` - HMAC secret key (secret)
- `S3_<BUCKET_NAME>_ENDPOINT` - GCS S3-compatible endpoint

**Generic Variables** (when only one bucket is used):
- `GCS_BUCKET` - Bucket name
- `GCS_LOCATION` - Bucket location
- `GCS_ACCESS_KEY` - HMAC access key ID (secret)
- `GCS_SECRET_KEY` - HMAC secret key (secret)
- `GCS_ENDPOINT` - GCS S3-compatible endpoint

**AWS SDK Compatible Variables** (for seamless AWS SDK integration):
- `AWS_ACCESS_KEY_ID` - HMAC access key ID (secret)
- `AWS_SECRET_ACCESS_KEY` - HMAC secret key (secret)
- `S3_ENDPOINT` - GCS S3-compatible endpoint
- `S3_BUCKET` - Bucket name
- `S3_REGION` - Bucket location

**Template Placeholders:**
- `${resource:bucket-name.bucket}` - Bucket name
- `${resource:bucket-name.location}` - Bucket location
- `${resource:bucket-name.access-key}` - HMAC access key ID
- `${resource:bucket-name.secret-key}` - HMAC secret key
- `${resource:bucket-name.endpoint}` - S3-compatible endpoint

#### GKE Autopilot
**Note:** GKE Autopilot compute processor is currently not implemented. No environment variables are automatically injected for GKE Autopilot resources at this time.

#### PostgreSQL Cloud SQL
PostgreSQL database connection details similar to AWS RDS.

**Auto-injected Environment Variables:**

- `POSTGRES_HOST` - Database host (localhost via Cloud SQL Proxy)
- `POSTGRES_PORT` - Database port (5432)
- `POSTGRES_USERNAME` - Database username (stack name)
- `POSTGRES_DATABASE` - Database name (stack name)
- `POSTGRES_PASSWORD` - Database password (auto-generated)
- `PGHOST` - PostgreSQL host (localhost via Cloud SQL Proxy)
- `PGPORT` - PostgreSQL port (5432)
- `PGUSER` - PostgreSQL username
- `PGDATABASE` - PostgreSQL database name
- `PGPASSWORD` - PostgreSQL password

**Template Placeholders:**

- `${resource:postgres-name.host}` - Database host
- `${resource:postgres-name.port}` - Database port
- `${resource:postgres-name.user}` - Database username
- `${resource:postgres-name.database}` - Database name
- `${resource:postgres-name.password}` - Database password

#### Redis Memorystore
Redis cache connection details for Google Cloud Memorystore.

**Auto-injected Environment Variables:**

- `REDIS_HOST` - Redis instance host
- `REDIS_PORT` - Redis instance port (defaults to 6379 if not available)

**Template Placeholders:**

- `${resource:redis-name.host}` - Redis host
- `${resource:redis-name.port}` - Redis port

### Kubernetes Resources

#### Helm Postgres Operator
PostgreSQL database connections managed by Kubernetes operators.

**Auto-injected Environment Variables:**

- `POSTGRES_HOST` - PostgreSQL service host
- `POSTGRES_PORT` - PostgreSQL service port (5432)
- `POSTGRES_USERNAME` - Database username (stack name)
- `POSTGRES_DATABASE` - Database name (stack name)
- `POSTGRES_PASSWORD` - Database password (auto-generated)
- `PGHOST` - PostgreSQL host
- `PGPORT` - PostgreSQL port
- `PGUSER` - PostgreSQL username
- `PGDATABASE` - PostgreSQL database name
- `PGPASSWORD` - PostgreSQL password

**Template Placeholders:**

- `${resource:postgres-name.host}` - Database host
- `${resource:postgres-name.port}` - Database port
- `${resource:postgres-name.user}` - Database username
- `${resource:postgres-name.database}` - Database name
- `${resource:postgres-name.password}` - Database password
- `${resource:postgres-name.url}` - Full connection URL

#### Helm RabbitMQ Operator
Message queue connection details and configuration.

**Auto-injected Environment Variables:**

- `RABBITMQ_HOST` - RabbitMQ service host
- `RABBITMQ_PORT` - RabbitMQ service port
- `RABBITMQ_USERNAME` - RabbitMQ username
- `RABBITMQ_PASSWORD` - RabbitMQ password (auto-generated)
- `RABBITMQ_URI` - Full AMQP connection string

**Template Placeholders:**

- `${resource:rabbitmq-name.host}` - RabbitMQ host
- `${resource:rabbitmq-name.port}` - RabbitMQ port
- `${resource:rabbitmq-name.user}` - Username
- `${resource:rabbitmq-name.password}` - Password
- `${resource:rabbitmq-name.uri}` - Full AMQP connection string

#### Helm Redis Operator
Redis cache connection details and configuration.

**Auto-injected Environment Variables:**

- `REDIS_HOST` - Redis service host
- `REDIS_PORT` - Redis service port

**Template Placeholders:**

- `${resource:redis-name.host}` - Redis host
- `${resource:redis-name.port}` - Redis port

### MongoDB Atlas

#### MongoDB Atlas Cluster
MongoDB Atlas database cluster connection details and credentials.

**Auto-injected Environment Variables:**

- `MONGO_USER` - Database username (stack name)
- `MONGO_DATABASE` - Database name (stack name)
- `MONGO_PASSWORD` - Database password (auto-generated, secret)
- `MONGO_URI` - Full MongoDB connection string with authentication (secret)
- `MONGO_DEP_<OWNER>_USER` - Dependency username (for dependency relationships)
- `MONGO_DEP_<OWNER>_PASSWORD` - Dependency password (for dependency relationships, secret)
- `MONGO_DEP_<OWNER>_URI` - Dependency connection string (for dependency relationships, secret)

**Template Placeholders:**

- `${resource:mongodb-name.uri}` - Full MongoDB connection string
- `${resource:mongodb-name.user}` - Database username
- `${resource:mongodb-name.password}` - Database password
- `${resource:mongodb-name.dbName}` - Database name
- `${resource:mongodb-name.oplogUri}` - MongoDB oplog connection string

**For Dependencies** (when used via `dependencies` section):
- `${dependency:dep-name.uri}` - Full MongoDB connection string for dependency
- `${dependency:dep-name.user}` - Database username for dependency
- `${dependency:dep-name.password}` - Database password for dependency
- `${dependency:dep-name.dbName}` - Database name for dependency
- `${dependency:dep-name.oplogUri}` - MongoDB oplog connection string for dependency

## Practical Examples

### Using S3 Storage

```yaml
# parent stack (server.yaml)
resources:
  resources:
    production:
      file-storage:
        type: s3-bucket
        config:
          name: myapp-files
          allowOnlyHttps: true
```

```yaml
# service stack (client.yaml)
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [file-storage]
      runs: [web-app]
```

```yaml
# docker-compose.yaml (for local development)
services:
  web-app:
    build: .
    environment:
      S3_BUCKET: "local-dev-bucket"
      S3_REGION: "us-east-1"
      S3_ACCESS_KEY: "dev-access-key"
      S3_SECRET_KEY: "dev-secret-key"
    # When deployed via Simple Container, these values are automatically
    # overridden by compute processor environment variables
```

### Using Database Connection

```yaml
# parent stack (server.yaml)
resources:
  resources:
    production:
      main-db:
        type: aws-rds-postgres
        config:
          name: myapp-db
          instanceClass: db.t3.micro
          username: postgres
          password: "${secret:DB_ROOT_PASSWORD}"
```

```yaml
# service stack (client.yaml)
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [main-db]
      runs: [api-service]
```

```yaml
# docker-compose.yaml (for local development)
services:
  api-service:
    build: .
    environment:
      PGHOST: "localhost"
      PGUSER: "postgres"
      PGPORT: "5432"
      PGDATABASE: "myapp_dev"
      PGPASSWORD: "dev-password"
    # When deployed via Simple Container, these values are automatically
    # overridden by compute processor environment variables
```

### Using Template Placeholders in Client Configuration

```yaml
# client.yaml - Custom environment variables using template placeholders
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [main-db, redis-cache, file-storage]
      runs: [api-service]
      env:
        # Custom environment variables using template placeholders
        DATABASE_URL: "postgresql://${resource:main-db.user}:${resource:main-db.password}@${resource:main-db.host}:${resource:main-db.port}/${resource:main-db.database}"
        DB_HOST: "${resource:main-db.host}"
        REDIS_URL: "redis://${resource:redis-cache.host}:${resource:redis-cache.port}"
      secrets:
        # Custom secrets using template placeholders
        API_SECRET_KEY: "${resource:main-db.password}"
        S3_UPLOAD_KEY: "${resource:file-storage.access-key}"
```

## Benefits

1. **Automatic Configuration** - No manual connection string management
2. **Security** - Sensitive data automatically handled as secrets
3. **Consistency** - Standardized environment variable naming
4. **Flexibility** - Use both auto-injected variables and explicit placeholders
5. **Cross-Service Integration** - Easy sharing of resources between services

## Best Practices

1. **Prefer Auto-injected Variables** - Let compute processors handle standard environment variables
2. **Use Template Placeholders for Custom Config** - When you need specific formatting or custom configuration files
3. **Leverage Dependencies** - Share resources between services using the dependencies section
4. **Follow Naming Conventions** - Use descriptive resource names that map to clear environment variables

This system makes Simple Container deployments truly declarative - you declare what resources you need, and Simple Container handles all the connection details automatically.


---

## Why Simple Container

# **Why Simple Container?**

In today's fast-paced software development environment, organizations must deploy and manage **dozens or even hundreds of microservices** efficiently. Traditional **CI/CD pipelines** often require **significant DevOps effort**, complex infrastructure provisioning, and manual management of microservices. **Simple Container** offers a **game-changing approach** that simplifies deployment, automates infrastructure management, and accelerates software delivery.

This page explores why organizations should adopt **`sc`** for their **CI/CD pipelines**, highlighting its **ease of use, automation capabilities, cloud-agnostic nature, and security features**.

As organizations embrace **microservices architectures**, they face a fundamental challenge: **how to efficiently manage, deploy, and scale multiple services without creating complexity for developers and DevOps teams**. Traditional methods of provisioning and deploying microservices often involve **manual infrastructure management, complex CI/CD pipelines, and extensive collaboration between developers and DevOps**.

Simple Container is a **powerful yet lightweight tool** designed to streamline **microservices deployment and delivery**. It provides a **developer-friendly approach** while keeping **infrastructure management centralized and isolated from service deployment**. By bridging the gap between **DevOps and development teams**, `sc` enables organizations to **move faster, reduce bottlenecks, and simplify their microservices workflow**.

For the detailed use-cases of what you can do with Simple Container, please read [Howto](/howto/use-cases/) section.

---

## **1Ô∏è‚É£ The Challenges of Microservices Deployment**

Microservices offer **flexibility, scalability, and modularity**, but **deploying and maintaining them is complex**. In a traditional setup:

- **Developers must coordinate with DevOps** teams to configure cloud resources (databases, networking, secrets, storage).
- **Infrastructure provisioning requires expertise in Infrastructure as Code (IaC) tools** like Terraform or Pulumi.
- **Deployments are tightly coupled with CI/CD pipelines**, requiring manual configuration for every new service.
- **Secrets management and environment-specific configurations require additional setup**, often leading to security risks.

As organizations scale, these challenges become **major bottlenecks**, slowing down **innovation and delivery cycles**.

---

## **2Ô∏è‚É£ What is Simple Container?**

Simple Container is a **cloud-agnostic deployment tool** designed specifically for **microservices**. It provides:

‚úÖ **A unified way to deploy services without DevOps intervention.**

‚úÖ **A declarative configuration approach using simple YAML files.**

‚úÖ **Built-in secrets management and cloud resource integration.**

‚úÖ **Seamless support for Kubernetes, AWS ECS Fargate, and Google Cloud Run.**

At its core, `sc` allows **developers to focus on coding**, while **DevOps teams manage infrastructure separately**. This **clear separation of concerns** eliminates the need for developers to understand cloud provisioning and allows **DevOps to standardize infrastructure across all services**.

---

## **3Ô∏è‚É£ Where Simple Container Fits in Microservices Development**

### **üîπ Empowering Developers with Hassle-Free Deployments**
With `sc`, developers no longer need to:

- **Request infrastructure changes from DevOps.**
- **Manually configure networking, storage, or database connections.**
- **Write complex deployment scripts.**

Instead, they can **deploy new microservices seamlessly**, using simple configurations that **automatically integrate with the existing infrastructure**.

This **reduces friction between development and operations**, enabling teams to launch new features **faster and more reliably**.

### **üîπ A Single Source of Truth for Infrastructure**
While developers focus on building services, **DevOps teams retain full control over infrastructure**. In `sc`:

- Cloud resources such as **databases, message queues, storage, and networking** are centrally defined.
- **Secrets and environment-specific configurations are managed securely.**
- **All infrastructure is provisioned via a single, structured configuration, reducing inconsistencies.**

This **eliminates duplication, reduces security risks, and ensures a consistent setup across multiple services**.

### **üîπ Automating CI/CD Without Extra Complexity**
Traditional CI/CD pipelines require:

- **Setting up deployment scripts for every service.**
- **Managing multiple cloud provider integrations.**
- **Manually configuring rollbacks and service scaling.**

With `sc`, these processes are **built-in**. Deployments happen **without manually modifying CI/CD pipelines**, and rollback mechanisms ensure **safer deployments with minimal downtime**.

---

## **4Ô∏è‚É£ Where Simple Container Fits in Microservices Delivery**

### **üîπ Cloud-Agnostic Deployment for Enterprise Flexibility**
Organizations today operate in **multi-cloud and hybrid environments**. Some workloads run **on Kubernetes clusters**, while others leverage **managed services like AWS ECS Fargate or Google Cloud Run**.

Simple Container enables **seamless deployment across all these platforms** without requiring developers to modify service configurations. This **future-proofs deployments**, allowing teams to switch cloud providers **without rewriting infrastructure code**.

### **üîπ Security & Compliance Without Extra Tools**
Security is a **critical concern** in microservices delivery. Managing **secrets and environment-specific credentials** across multiple services is challenging.

`sc` offers **built-in secrets management**, ensuring that credentials are:

- **Stored securely in cloud-native secret managers (AWS Secrets Manager, GCP Secret Manager, Kubernetes Secrets).**
- **Automatically injected into services at runtime.**
- **Never exposed in plaintext or CI/CD logs.**

This **reduces security risks while keeping deployments efficient**.

### **üîπ Scalability & Reliability for Growing Organizations**
As organizations scale, microservices need to:

- **Spin up new instances dynamically based on demand.**
- **Auto-scale without manual intervention.**
- **Ensure high availability and fault tolerance.**

Simple Container integrates **seamlessly with cloud-native scaling solutions**, ensuring that services remain **resilient, highly available, and cost-efficient**.

---

## **5Ô∏è‚É£ Why Organizations Should Adopt `sc`: Quantified Scaling Benefits**

### **‚úÖ Faster Microservice Deployment - 500x Speed Improvement**

- **Customer Onboarding**: **5 minutes** vs **2-3 days** traditional approaches
- **Time to First Deployment**: **15 minutes** vs **2-3 days** infrastructure setup
- **Developer Onboarding**: **1-2 hours** vs **2-4 weeks** Kubernetes/AWS training
- Developers can **self-deploy services** without waiting for DevOps approval

### **‚úÖ Reduced DevOps Overhead - 5x Operational Efficiency**

- **DevOps to Customer Ratio**: **1:100+ customers** vs **1:10-20** traditional
- **Configuration Complexity**: **90% reduction** (500 vs 5000+ lines for 100 customers)
- **Infrastructure Drift Risk**: **Low (template-based)** vs **High (manual management)**
- DevOps focuses **only on infrastructure**, reducing operational burden by **80%**

### **‚úÖ Built-in Security & Secrets Management - Zero External Tools**

- **Automatic namespace isolation** for multi-tenant security
- **Built-in secrets management** with `${secret:}` and `${env:}` support
- **External secret manager integration** ready (AWS Secrets Manager, Vault, Azure Key Vault)
- Cloud-native integrations ensure **compliance with security policies**

### **‚úÖ Cloud-Agnostic & Future-Proof - Zero Downtime Migrations**

- **Performance tier migration**: **One-line change** vs **manual infrastructure rebuild**
- **Multi-region expansion**: **Single parent stack change** vs **duplicate infrastructure code**
- Works with **Kubernetes, AWS, and Google Cloud** seamlessly
- Migrating workloads **requires no changes to service configurations**

### **‚úÖ CI/CD Without Complexity - 70% Cost Reduction**

- **Infrastructure cost savings**: **70% reduction** through intelligent resource sharing
- **Operational staff reduction**: **80% fewer** DevOps engineers needed
- **Resource utilization**: **Right-sized sharing** vs **often over-provisioned**
- **No need for external deployment tools**‚ÄîCI/CD is built into `sc`

---

## **6Ô∏è‚É£ Real-World Scaling Impact**

### **Adding 100 New Customers**

**Traditional Approach:**
- **Time**: 2-3 days per customer = **200-300 days total**
- **Configuration**: 5000+ lines of infrastructure code
- **Team**: Requires DevOps expertise for each deployment

**Simple Container:**
- **Time**: 5 minutes per customer = **8.3 hours total**
- **Configuration**: 500 lines total (5 lines per customer)
- **Team**: Developers can self-serve, no DevOps bottleneck

### **Multi-Region Expansion**

**Traditional**: Duplicate entire infrastructure for each region
**Simple Container**: Single parent stack change, customers choose regions easily

### **Performance Tier Migration**

**Traditional**: Manual infrastructure rebuild, downtime required
**Simple Container**: One-line configuration change, zero downtime

---

## **Conclusion**

Simple Container is a **revolutionary tool** for organizations adopting **microservices architectures**. By delivering **quantified scaling advantages**, `sc` transforms container orchestration from a complex infrastructure challenge into a simple configuration management task.

**Proven Results:**
- **500x faster customer onboarding** (5 minutes vs 2-3 days)
- **90% reduction in configuration complexity** (500 vs 5000+ lines)
- **5x operational efficiency** (1 DevOps per 100+ vs 10-20 customers)
- **70% cost reduction** through intelligent resource sharing
- **Zero downtime migrations** with one-line configuration changes

For any organization scaling microservices, `sc` presents a **compelling alternative** to traditional CI/CD pipelines, offering **automation, security, and ease of use**‚Äîall while enabling **scale from startup to enterprise without operational complexity growth**.

---

# Guides

## Deployment Guides

# Deployment Guides

This section provides step-by-step guides for deploying applications using Simple Container across different platforms and scenarios.

## Platform-Specific Guides

### Container Orchestration

- **[ECS Fargate](parent-ecs-fargate.md)** - Deploy containerized applications on AWS ECS Fargate
- **[GKE Autopilot](parent-gcp-gke-autopilot.md)** - Deploy on Google Kubernetes Engine Autopilot
- **[Pure Kubernetes](parent-pure-kubernetes.md)** - Deploy on any Kubernetes cluster

### Operational Guides

- **[Resource Adoption](resource-adoption.md)** - Import existing cloud infrastructure without downtime or modifications
- **[CI/CD with GitHub Actions](cicd-github-actions.md)** - Complete guide to automated deployment pipelines with GitHub Actions
- **[DNS Management](dns-management.md)** - Complete guide to domain and DNS configuration with Cloudflare
- **[Secrets Management](secrets-management.md)** - Comprehensive guide to handling secrets and credentials
- **[Migration Guide](migration.md)** - Migrate existing applications to Simple Container
- **[Service Deployment Steps](service-steps-to-deploy.md)** - General deployment workflow

## Choosing the Right Platform

### AWS ECS Fargate
**Best for:** Teams already using AWS, serverless container deployments, auto-scaling workloads

- Fully managed container orchestration
- Pay-per-use pricing model
- Integrated with AWS services (RDS, S3, etc.)

### GKE Autopilot
**Best for:** Google Cloud users, teams wanting managed Kubernetes without complexity

- Fully managed Kubernetes experience
- Automatic node provisioning and scaling
- Built-in security and compliance features

### Pure Kubernetes
**Best for:** Multi-cloud deployments, existing Kubernetes expertise, maximum control

- Works with any Kubernetes cluster (EKS, AKS, on-premises)
- Full control over cluster configuration
- Portable across cloud providers

## Common Deployment Patterns

### Microservices Architecture

1. **Parent Stack Setup** - Configure shared infrastructure (databases, networking)
2. **Service Templates** - Define reusable service configurations
3. **Environment Management** - Deploy across dev/staging/prod environments
4. **Secret Management** - Secure handling of credentials and configuration

### Multi-Tenant Applications

1. **Resource Isolation** - Namespace-based tenant separation
2. **Shared Resources** - Common databases and services
3. **Scaling Strategies** - Per-tenant and shared resource scaling
4. **Monitoring & Logging** - Tenant-aware observability

## Next Steps

After completing a deployment guide:

1. Explore **[Examples](../examples/README.md)** for real-world configurations
2. Review **[Advanced Topics](../advanced/scaling-advantages.md)** for optimization strategies
3. Check **[Reference Documentation](../reference/supported-resources.md)** for complete API details

## Need Help?

- Review **[Core Concepts](../concepts/main-concepts.md)** for fundamental understanding
- Check **[Template Placeholders](../concepts/template-placeholders.md)** for configuration syntax
- Contact [support@simple-container.com](mailto:support@simple-container.com) for assistance


---

## ECS Fargate

# **Guide: Configuring a Parent Stack for AWS ECS Fargate & MongoDB Atlas with Simple Container**

This guide is for **DevOps teams** who want to configure a **parent stack (`server.yaml`)** for deploying infrastructure **on AWS ECS Fargate** and **MongoDB Atlas** using **Simple Container**.

With this setup, developers can **deploy microservices to ECS Fargate** while using **MongoDB Atlas as a managed database**.

---

# **1Ô∏è‚É£ Prerequisites**
Before configuring the parent stack, ensure that:

‚úÖ You have an **AWS account** & IAM credentials with permissions to create ECS Fargate clusters.

‚úÖ You have a **MongoDB Atlas account** with a valid API key.

‚úÖ **Simple Container is installed**:

   ```sh
   curl -s "https://dist.simple-container.com/sc.sh" | bash
   ```

---

# **2Ô∏è‚É£ Setting Up AWS & MongoDB Atlas Secrets**

AWS and MongoDB Atlas credentials must be stored in **`secrets.yaml`**.

## **Step 1: Define `secrets.yaml`**
Create the **`.sc/stacks/devops/secrets.yaml`** file to store AWS credentials & MongoDB Atlas API keys:
```yaml
---
# File: "myproject/.sc/stacks/devops/secrets.yaml"

schemaVersion: 1.0

auth:
  aws:
    type: aws-token
    config:
      accessKey: "AKIAIOSFODNN7EXAMPLE"
      secretAccessKey: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
      region: "us-west-2"

values:
  CLOUDFLARE_API_TOKEN: "abcdefgh123456789"
  MONGODB_ATLAS_PUBLIC_KEY: "public-key-123"
  MONGODB_ATLAS_PRIVATE_KEY: "private-key-456"
```

### **üîπ What This Does**

‚úÖ Stores **AWS credentials** for programmatic access.

‚úÖ Saves **MongoDB Atlas API keys** for provisioning databases.

---

# **3Ô∏è‚É£ Configuring Infrastructure Provisioning (`server.yaml`)**

Now, define `.sc/stacks/devops/server.yaml` to provision **ECS Fargate & MongoDB Atlas**.

## **Step 2: Define `server.yaml`**
```yaml
---
# File: "myproject/.sc/stacks/devops/server.yaml"
schemaVersion: 1.0

# Provisioning state management
provisioner:
  type: pulumi
  config:
    state-storage:
      type: s3-bucket
      config:
        credentials: "${auth:aws}"
        bucketName: myproject-sc-state
    secrets-provider:
      type: aws-kms
      config:
        credentials: "${auth:aws}"
        keyName: myproject-sc-kms-key

# Deployment templates for ECS Fargate-based workloads
templates:
  stack-per-app:
    type: ecs-fargate
    config:
      credentials: "${auth:aws}"
      account: "${auth:aws.projectId}"

# Infrastructure resources provisioned inside AWS & MongoDB Atlas
resources:
  registrar:
    type: cloudflare  # Optional DNS management
    config:
      credentials: "${secret:CLOUDFLARE_API_TOKEN}"
      accountId: "89cc23bd273c76d6767f6566c54621c2"
      zoneName: "myproject.com"

  resources:
    staging:
      template: stack-per-app
      resources:
        mongodb:
          type: mongodb-atlas
          config:
            admins: [ "admin" ]
            developers: [ "developer1", "developer2" ]
            instanceSize: "M10"
            orgId: "878cd82332ff12c2332d2234"
            region: "US_WEST_2"
            cloudProvider: AWS
            privateKey: "${secret:MONGODB_ATLAS_PRIVATE_KEY}"
            publicKey: "${secret:MONGODB_ATLAS_PUBLIC_KEY}"
            backup:
              every: 4h
              retention: 24h
```

### **üîπ What This Does**

‚úÖ **Configures Pulumi** for managing **state in an S3 bucket**.

‚úÖ **Uses AWS KMS to encrypt secrets**.

‚úÖ **Defines an ECS Fargate template (`stack-per-app`)** for developers to deploy microservices.

‚úÖ **Provisions MongoDB Atlas**, making it available for microservices.

---

# **4Ô∏è‚É£ Provisioning the AWS & MongoDB Atlas Parent Stack**
Once `server.yaml` is configured, **provision** the infrastructure:

```sh
sc provision -s devops
```

### **What This Does**

‚úÖ Creates an **S3 bucket** for state storage.

‚úÖ Deploys **MongoDB Atlas** with provisioned users.

‚úÖ Configures **Cloudflare DNS (optional)**.

‚úÖ Prepares **ECS Fargate infrastructure for microservices**.

---

# **5Ô∏è‚É£ Deploying Microservices to ECS Fargate**
Once the infrastructure is provisioned, developers can deploy their microservices.

## **Step 1: Define `client.yaml` for a Microservice**
```yaml
---
# File: "myproject/.sc/stacks/myservice/client.yaml"

schemaVersion: 1.0

stacks:
  staging:
    type: cloud-compose
    parent: myproject/devops
    config:
      domain: ${env:MY_SERVICE_DOMAIN}
      dockerComposeFile: ./docker-compose.yaml
      uses:
        - mongodb
      runs:
        - myservice
      size:
        cpu: 512
        memory: 1024
      scale:
        min: 1
        max: 5
        policy:
          cpu:
            max: 70
      env:
        DATABASE_HOST: "${resource:mongodb.host}"
        DATABASE_NAME: "${resource:mongodb.database}"
        DATABASE_USER: "${resource:mongodb.user}"
      secrets:
        DATABASE_PASSWORD: "${resource:mongodb.password}"
```

### **Advanced Configuration: AWS CloudExtras**

For advanced AWS ECS Fargate deployments, you can use `cloudExtras` to configure AWS-specific features:

```yaml
stacks:
  staging:
    type: cloud-compose
    parent: myproject/devops
    config:
      # ... basic configuration above
      cloudExtras:
        # AWS IAM roles for the ECS task
        awsRoles:
          - "arn:aws:iam::123456789012:role/MyServiceRole"
          - "arn:aws:iam::123456789012:role/S3AccessRole"
        
        # Lambda scheduling for background tasks
        lambdaSchedule:
          expression: "rate(5 minutes)"
          timezone: "America/New_York"
        
        # Multiple Lambda schedules with request payloads
        lambdaSchedules:
          - name: "daily-report"
            expression: "cron(0 9 * * ? *)"  # Daily at 9 AM
            timezone: "UTC"
            request: |-
              {
                "requestId": "daily-report",
                "requestTime": "23/Jun/2024:09:00:00 +0000",
                "httpMethod": "POST",
                "path": "/api/reports/daily",
                "requestContext": {
                  "http": {
                    "path": "/api/reports/daily",
                    "method": "POST",
                    "protocol": "HTTP/1.1"
                  }
                },
                "body": "{\"type\":\"daily\",\"format\":\"pdf\"}",
                "headers": {
                  "Authorization": "Bearer ${secret:api-key}",
                  "Content-Type": "application/json"
                }
              }
          - name: "hourly-cleanup"
            expression: "rate(1 hour)"       # Every hour
            timezone: "America/Los_Angeles"
            request: |-
              {
                "requestId": "hourly-cleanup",
                "httpMethod": "POST",
                "path": "/api/cleanup",
                "body": "{\"action\":\"cleanup\",\"maxAge\":3600}",
                "headers": {
                  "Authorization": "Bearer ${secret:cleanup-api-key}"
                }
              }
        
        # Lambda routing and invoke configuration
        lambdaRoutingType: "weighted"  # or "simple"
        lambdaInvokeMode: "async"      # or "sync"
        
        # Security group configuration
        securityGroup:
          ingress:
            allowOnlyCloudflare: true    # Restrict to Cloudflare IPs
            cidrBlocks:                  # Additional CIDR blocks
              - "10.0.0.0/8"
              - "192.168.0.0/16"
            ipv6CidrBlocks:              # IPv6 CIDR blocks
              - "2001:db8::/32"
        
        # Load balancer type
        loadBalancerType: "alb"  # Application Load Balancer (default)
        # loadBalancerType: "nlb"  # Network Load Balancer
```

#### **CloudExtras Field Reference**

| Field               | Type       | Description                                     | Example                                                              |
|---------------------|------------|-------------------------------------------------|----------------------------------------------------------------------|
| `awsRoles`          | `[]string` | IAM roles to attach to ECS tasks                | `["arn:aws:iam::123:role/MyRole"]`                                   |
| `lambdaSchedule`    | `object`   | Single Lambda schedule configuration            | `{expression: "rate(5 minutes)"}`                                    |
| `lambdaSchedules`   | `[]object` | Multiple Lambda schedules with request payloads | `[{name: "daily", expression: "cron(0 9 * * ? *)", request: "..."}]` |
| `lambdaRoutingType` | `string`   | Lambda routing strategy                         | `"weighted"` or `"simple"`                                           |
| `lambdaInvokeMode`  | `string`   | Lambda invocation mode                          | `"async"` or `"sync"`                                                |
| `securityGroup`     | `object`   | Security group ingress rules                    | See security group configuration                                     |
| `loadBalancerType`  | `string`   | Load balancer type                              | `"alb"` or `"nlb"`                                                   |

**LambdaSchedules Object Fields:**
| Field        | Type     | Description                          | Required |
|--------------|----------|--------------------------------------|----------|
| `name`       | `string` | Unique identifier for the schedule   | ‚úÖ Yes   |
| `expression` | `string` | Cron or rate expression              | ‚úÖ Yes   |
| `timezone`   | `string` | Timezone for execution               | ‚ùå Optional |
| `request`    | `string` | Complete HTTP request payload (JSON) | ‚úÖ Yes   |

#### **Security Group Configuration**

```yaml
securityGroup:
  ingress:
    allowOnlyCloudflare: true      # Automatically allow Cloudflare IP ranges
    cidrBlocks:                    # Custom IPv4 CIDR blocks
      - "10.0.0.0/8"              # Private network
      - "203.0.113.0/24"          # Specific public range
    ipv6CidrBlocks:                # Custom IPv6 CIDR blocks
      - "2001:db8::/32"           # IPv6 range
```

#### **Lambda Schedule Examples**

```yaml
# Single schedule (simple)
lambdaSchedule:
  expression: "rate(5 minutes)"
  timezone: "America/New_York"

# Multiple schedules with full request payloads
lambdaSchedules:
  - name: "every-minute-health-check"
    expression: "cron(* * * * ? *)"    # Every minute
    timezone: "UTC"
    request: |-
      {
        "requestId": "health-check",
        "requestTime": "23/Jun/2024:15:48:12 +0000",
        "httpMethod": "POST",
        "path": "/api/health",
        "requestContext": {
          "http": {
            "path": "/api/health",
            "method": "POST",
            "protocol": "HTTP/1.1"
          }
        },
        "body": "{\"check\":\"all\"}",
        "headers": {
          "Authorization": "Bearer ${secret:health-check-api-key}",
          "Content-Type": "application/json"
        }
      }
  - name: "monthly-billing"
    expression: "cron(0 0 1 * ? *)"    # First day of every month
    timezone: "America/New_York"
    request: |-
      {
        "requestId": "monthly-billing",
        "httpMethod": "POST",
        "path": "/api/billing/process",
        "body": "{\"period\":\"monthly\",\"notify\":true}",
        "headers": {
          "Authorization": "Bearer ${secret:billing-api-key}"
        }
      }
```

**Lambda Schedule Request Structure:**
- `name`: Unique identifier for the schedule
- `expression`: Cron or rate expression for timing
- `timezone`: Timezone for schedule execution (optional)
- `request`: Complete HTTP request payload sent to your Lambda function
  - `requestId`: Unique request identifier
  - `requestTime`: Timestamp of the request
  - `httpMethod`: HTTP method (GET, POST, etc.)
  - `path`: API endpoint path
  - `requestContext`: Additional request context
  - `body`: JSON request body (as string)
  - `headers`: HTTP headers including authentication

## **Step 2: Deploy the Service**
```sh
sc deploy -s myservice -e staging
```

‚úÖ The service is **automatically deployed to ECS Fargate** using the defined settings.

---

# **6Ô∏è‚É£ Summary**
| Step                | Command                             | Purpose                                       |
|---------------------|-------------------------------------|-----------------------------------------------|
| **Define Secrets**  | `secrets.yaml`                      | Stores AWS & MongoDB Atlas credentials        |
| **Configure Infra** | `server.yaml`                       | Defines ECS Fargate & MongoDB Atlas resources |
| **Provision Infra** | `sc provision -s devops`            | Deploys AWS infrastructure                    |
| **Define Service**  | `client.yaml`                       | Describes a microservice deployment           |
| **Deploy Service**  | `sc deploy -s myservice -e staging` | Deploys a microservice to ECS Fargate         |

---

## GKE Autopilot

# **Guide: Configuring a Parent Stack for GCP & GKE Autopilot with Simple Container**

This guide is for **DevOps teams** who want to configure a **parent stack (`server.yaml`)** for deploying infrastructure **on Google Cloud Platform (GCP) using GKE Autopilot** with **Simple Container**.

With this setup, developers can **deploy microservices to GKE Autopilot** while leveraging **GCP-native services like CloudSQL, Redis, and Pub/Sub**.

---

# **1Ô∏è‚É£ Prerequisites**
Before configuring the parent stack, ensure that:

‚úÖ You have a **GCP account** and a **GCP project**.

‚úÖ You have a **GCP service account with proper IAM permissions** to create GKE clusters and other resources.

‚úÖ **Simple Container** is installed:

   ```sh
   curl -s "https://dist.simple-container.com/sc.sh" | bash
   ```

---

# **2Ô∏è‚É£ Setting Up GCP Authentication & Secrets**

## **Step 1: Define `secrets.yaml`**
Create the **`.sc/stacks/devops/secrets.yaml`** file to store GCP credentials:
```yaml
---
# File: "myproject/.sc/stacks/devops/secrets.yaml"
schemaVersion: 1.0

auth:
  gcloud:
    type: gcp-service-account
    config:
      projectId: "my-gcp-project-id"
      credentials: |-
        {
          "type": "service_account",
          "project_id": "my-gcp-project-id",
          "private_key_id": "60bb42f229bc21f6d303b5967b6cd59265cb316d",
          "private_key": "-----BEGIN PRIVATE KEY-----\nBLABLABLA\n-----END PRIVATE KEY-----\n",
          "client_email": "deploy-bot@my-gcp-project-id.iam.gserviceaccount.com",
          "client_id": "2387492479284792742398427",
          "auth_uri": "https://accounts.google.com/o/oauth2/auth",
          "token_uri": "https://oauth2.googleapis.com/token",
          "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
          "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/deploy-bot%40my-gcp-project-id.iam.gserviceaccount.com"
        }

values:
  CLOUDFLARE_API_TOKEN: "abcdefgh123456789"
  MONGODB_ATLAS_PUBLIC_KEY: "public-key-123"
  MONGODB_ATLAS_PRIVATE_KEY: "private-key-456"
```

### **üîπ What This Does**

‚úÖ **Stores GCP service account credentials** (`gcloud`).

‚úÖ **Saves API tokens for DNS management**.

---

# **3Ô∏è‚É£ Configuring Infrastructure Provisioning (`server.yaml`)**

Now, define **`.sc/stacks/devops/server.yaml`** to provision **GKE Autopilot, CloudSQL, Redis, and Pub/Sub**.

## **Step 2: Define `server.yaml`**
```yaml
---
# File: "myproject/.sc/stacks/devops/server.yaml"
schemaVersion: 1.0

# Provisioning state management
provisioner:
  type: pulumi
  config:
    state-storage:
      type: gcp-bucket
      config:
        credentials: "${auth:gcloud}"
        projectId: "${auth:gcloud.projectId}"
        bucketName: myproject-sc-state
        location: europe-west3
    secrets-provider:
      type: gcp-kms
      config:
        projectId: "${auth:gcloud.projectId}"
        keyName: myproject-sc-kms-key
        keyLocation: global
        credentials: "${auth:gcloud}"

# Deployment templates for GKE Autopilot workloads
templates:
  stack-per-app-gke:
    type: gcp-gke-autopilot
    config:
      projectId: "${auth:gcloud.projectId}"
      credentials: "${auth:gcloud}"
      gkeClusterResource: gke-autopilot-res
      artifactRegistryResource: artifact-registry-res

# Infrastructure resources provisioned inside GCP
resources:
  registrar:
    type: cloudflare
    config:
      credentials: "${secret:CLOUDFLARE_API_TOKEN}"
      accountId: "89cc23bd273c76d6767f6566c54621c2"
      zoneName: "myproject.com"

  resources:
    staging:
      template: stack-per-app-gke
      resources:
        mongodb:
          type: mongodb-atlas
          config:
            admins: [ "admin" ]
            developers: [ "developer1" ]
            instanceSize: "M10"
            orgId: "878cd82332ff12c2332d2234"
            region: "EU_CENTRAL_1"
            cloudProvider: GCP
            privateKey: "${secret:MONGODB_ATLAS_PRIVATE_KEY}"
            publicKey: "${secret:MONGODB_ATLAS_PUBLIC_KEY}"
        redis:
          type: gcp-redis
          config:
            projectId: "${auth:gcloud.projectId}"
            credentials: "${auth:gcloud}"
            memorySizeGb: 2
            region: europe-west3
        gke-autopilot-res:
          type: gcp-gke-autopilot-cluster
          config:
            gkeMinVersion: "1.33.4-gke.1245000"  # Check: gcloud container get-server-config --location=europe-west3
            projectId: "${auth:gcloud.projectId}"
            credentials: "${auth:gcloud}"
            location: europe-west3
        artifact-registry-res:
          type: gcp-artifact-registry
          config:
            projectId: "${auth:gcloud.projectId}"
            credentials: "${auth:gcloud}"
            location: europe-west3
        pubsub:
          type: gcp-pubsub
          config:
            projectId: "${auth:gcloud.projectId}"
            credentials: "${auth:gcloud}"
            subscriptions:
              - name: workers.image-generator.sub
                topic: workers.image-generator
```

### **üîπ What This Does**

‚úÖ **Configures Pulumi** for managing **state in a Google Cloud Storage bucket**.

‚úÖ **Uses GCP KMS to encrypt secrets**.

‚úÖ **Defines a GKE Autopilot template** (`stack-per-app-gke`) for deploying workloads.

‚úÖ **Provisions MongoDB Atlas, Redis, Pub/Sub, and Artifact Registry** to support microservices.

---

# **4Ô∏è‚É£ Provisioning the GCP & GKE Autopilot Parent Stack**
Once `server.yaml` is configured, **provision** the infrastructure:

```sh
sc provision -s devops
```

### **What This Does**

‚úÖ Creates a **Google Cloud Storage bucket** for state storage.

‚úÖ Deploys **MongoDB Atlas, Redis, and Pub/Sub** in GCP.

‚úÖ Configures **GKE Autopilot for running microservices**.

---

# **5Ô∏è‚É£ Deploying Microservices to GKE Autopilot**
Once the infrastructure is provisioned, developers can deploy their microservices.

## **Step 1: Define `client.yaml` for a Microservice**
```yaml
---
# File: "myproject/.sc/stacks/myservice/client.yaml"

schemaVersion: 1.0

stacks:
  staging:
    type: cloud-compose
    parent: myproject/devops
    config:
      domain: ${env:MY_SERVICE_DOMAIN}
      dockerComposeFile: ./docker-compose.yaml
      uses:
        - mongodb
      runs:
        - myservice
      env:
        DATABASE_HOST: "${resource:mongodb.host}"
        DATABASE_NAME: "${resource:mongodb.database}"
        DATABASE_USER: "${resource:mongodb.user}"
      secrets:
        DATABASE_PASSWORD: "${resource:mongodb.password}"
```

## **Step 2: Deploy the Service**
```sh
sc deploy -s myservice -e staging
```

‚úÖ The service is **automatically deployed to GKE Autopilot** using the defined settings.

---

# **6Ô∏è‚É£ Advanced Configuration: Vertical Pod Autoscaler (VPA)**

GKE Autopilot supports **Vertical Pod Autoscaler (VPA)** for automatic resource optimization. Simple Container provides built-in VPA support for both **application deployments** and **Caddy ingress controllers**.

## **VPA for Application Deployments**

Add VPA configuration to your `client.yaml` using `cloudExtras`:

```yaml
# File: "myproject/.sc/stacks/myservice/client.yaml"
stacks:
  staging:
    type: cloud-compose
    parent: myproject/devops
    config:
      dockerComposeFile: ./docker-compose.yaml
      uses: [mongodb]
      runs: [myservice]
      # VPA Configuration for automatic resource optimization
      cloudExtras:
        vpa:
          enabled: true
          updateMode: "Auto"  # Off, Initial, Auto, InPlaceOrRecreate
          minAllowed:
            cpu: "100m"
            memory: "128Mi"
          maxAllowed:
            cpu: "2"
            memory: "4Gi"
          controlledResources: ["cpu", "memory"]
```

## **VPA for Caddy Ingress Controller**

Configure VPA for the Caddy ingress controller in your `server.yaml`:

```yaml
# File: "myproject/.sc/stacks/devops/server.yaml"
resources:
  staging:
    resources:
      gke-cluster:
        type: gcp-gke-autopilot-cluster
        config:
          projectId: "${auth:gcloud.projectId}"
          credentials: "${auth:gcloud}"
          location: "us-central1"
          gkeMinVersion: "1.33.4-gke.1245000"
          # Caddy configuration as part of GKE Autopilot cluster
          caddy:
            enable: true
            namespace: caddy
            replicas: 2
            # VPA Configuration for Caddy ingress controller
            vpa:
              enabled: true
              updateMode: "Auto"  # Recommended for ingress controllers (recreates pods)
              minAllowed:
                cpu: "50m"
                memory: "64Mi"
              maxAllowed:
                cpu: "1"
                memory: "1Gi"
            # Optional: Manual resource limits alongside VPA
            resources:
              limits:
                cpu: "500m"
                memory: "512Mi"
              requests:
                cpu: "100m"
                memory: "128Mi"
```

## **VPA Update Modes**

| Mode                  | Description                             | Use Case                       |
|-----------------------|-----------------------------------------|--------------------------------|
| **Off**               | Only provides recommendations           | Testing and analysis           |
| **Initial**           | Sets resources only at pod creation     | Conservative approach          |
| **Auto**              | Updates by recreating pods              | Recommended for stateless apps |
| **InPlaceOrRecreate** | Updates resources in-place or recreates | Advanced use (preview feature) |

## **VPA Best Practices for GKE Autopilot**

‚úÖ **Use `Auto` mode** for ingress controllers like Caddy to ensure proper resource scaling

‚úÖ **Set appropriate `minAllowed`** to prevent resource starvation

‚úÖ **Set reasonable `maxAllowed`** to control costs

‚úÖ **Monitor VPA recommendations** before enabling automatic updates

‚úÖ **Combine with manual resource limits** for fine-grained control

---

# **6Ô∏è‚É£ Advanced Configuration: Kubernetes CloudExtras**

Beyond VPA, Simple Container supports comprehensive Kubernetes configuration through `cloudExtras`. This section covers all available options for fine-tuning your GKE Autopilot deployments.

## **Complete CloudExtras Reference**

```yaml
# File: "myproject/.sc/stacks/myservice/client.yaml"
stacks:
  staging:
    type: cloud-compose
    parent: myproject/devops
    config:
      dockerComposeFile: ./docker-compose.yaml
      uses: [mongodb]
      runs: [myservice]
      
      # Comprehensive Kubernetes configuration
      cloudExtras:
        # Node selection and placement
        nodeSelector:
          workload-group: "high-memory"
          environment: "staging"
          
        # Pod disruption budget for high availability
        disruptionBudget:
          minAvailable: 2          # Keep at least 2 pods running
          # maxUnavailable: 1      # Alternative: max 1 pod down
          
        # Rolling update strategy
        rollingUpdate:
          maxSurge: 2              # Add up to 2 extra pods during update
          maxUnavailable: 1        # Max 1 pod unavailable during update
          
        # Pod affinity and anti-affinity rules
        affinity:
          nodePool: "high-memory-pool"     # Target specific node pool
          exclusiveNodePool: true          # Only run on this node pool
          computeClass: "Performance"      # GKE compute class
          
        # Pod tolerations for tainted nodes
        tolerations:
          - key: "workload-group"
            operator: "Equal"
            value: "high-memory"
            effect: "NoSchedule"
          - key: "environment"
            operator: "Equal"
            value: "staging"
            effect: "NoExecute"
            
        # Vertical Pod Autoscaler (covered in detail above)
        vpa:
          enabled: true
          updateMode: "Auto"
          minAllowed:
            cpu: "100m"
            memory: "128Mi"
          maxAllowed:
            cpu: "4"
            memory: "8Gi"
          controlledResources: ["cpu", "memory"]
          
        # Global readiness probe configuration
        readinessProbe:
          httpGet:
            path: "/health"
            port: 8080
          initialDelaySeconds: 10
          timeoutSeconds: 5
          periodSeconds: 15
          failureThreshold: 5
          successThreshold: 1
          
        # Global liveness probe configuration
        livenessProbe:
          httpGet:
            path: "/health"
            port: 8080
          initialDelaySeconds: 30
          timeoutSeconds: 10
          periodSeconds: 30
          failureThreshold: 3
```

## **CloudExtras Field Reference**

| Field              | Type                | Description                    | GKE Autopilot Support      |
|--------------------|---------------------|--------------------------------|----------------------------|
| `nodeSelector`     | `map[string]string` | Node selection labels          | ‚úÖ Custom labels supported  |
| `disruptionBudget` | `object`            | Pod disruption budget for HA   | ‚úÖ Full support             |
| `rollingUpdate`    | `object`            | Rolling update strategy        | ‚úÖ Full support             |
| `affinity`         | `object`            | Pod affinity and anti-affinity | ‚úÖ With workload separation |
| `tolerations`      | `[]object`          | Pod tolerations for taints     | ‚úÖ Custom tolerations       |
| `vpa`              | `object`            | Vertical Pod Autoscaler        | ‚úÖ Native GKE support       |
| `readinessProbe`   | `object`            | Global readiness probe         | ‚úÖ Full support             |
| `livenessProbe`    | `object`            | Global liveness probe          | ‚úÖ Full support             |

## **Node Selection and Workload Separation**

GKE Autopilot supports custom node selection for workload separation:

```yaml
cloudExtras:
  # Custom node selector labels
  nodeSelector:
    workload-group: "compute-intensive"
    cost-optimization: "spot-instances"
    
  # Affinity rules for advanced placement
  affinity:
    nodePool: "compute-pool"           # Target specific node pool
    exclusiveNodePool: true            # Exclusive placement
    computeClass: "Performance"        # GKE compute class
    
  # Tolerations for custom taints
  tolerations:
    - key: "workload-group"
      operator: "Equal"
      value: "compute-intensive"
      effect: "NoSchedule"
```

**How it works:**
- GKE Autopilot automatically creates nodes with your custom labels
- Pods are scheduled only on nodes matching the `nodeSelector`
- Tolerations allow pods to run on tainted nodes

## **High Availability Configuration**

Configure pod disruption budgets and rolling updates for production workloads:

```yaml
cloudExtras:
  # Ensure minimum availability during disruptions
  disruptionBudget:
    minAvailable: 3              # Keep at least 3 pods running
    # maxUnavailable: 1          # Alternative: max 1 pod down
    
  # Control rolling update behavior
  rollingUpdate:
    maxSurge: "50%"              # Add 50% more pods during update
    maxUnavailable: "25%"        # Max 25% pods unavailable
```

## **Health Probe Configuration**

Configure global health probes for all containers:

```yaml
cloudExtras:
  # Readiness probe - when pod is ready to receive traffic
  readinessProbe:
    httpGet:
      path: "/api/ready"
      port: 8080
    initialDelaySeconds: 15      # Wait 15s before first check
    timeoutSeconds: 5            # 5s timeout per check
    periodSeconds: 10            # Check every 10s
    failureThreshold: 3          # 3 failures = not ready
    successThreshold: 1          # 1 success = ready
    
  # Liveness probe - when to restart pod
  livenessProbe:
    httpGet:
      path: "/api/health"
      port: 8080
    initialDelaySeconds: 60      # Wait 60s before first check
    timeoutSeconds: 10           # 10s timeout per check
    periodSeconds: 30            # Check every 30s
    failureThreshold: 3          # 3 failures = restart pod
```

### **Probe Types**

```yaml
# HTTP probe (most common)
readinessProbe:
  httpGet:
    path: "/health"
    port: 8080
    
# TCP probe (for non-HTTP services)
livenessProbe:
  tcpSocket:
    port: 5432
    
# Command probe (custom health check)
readinessProbe:
  exec:
    command:
      - "/bin/sh"
      - "-c"
      - "pg_isready -U postgres"
```

## **Environment-Specific Configuration**

Different environments can have different CloudExtras configurations:

```yaml
# Production - High availability focus
stacks:
  production:
    config:
      cloudExtras:
        disruptionBudget:
          minAvailable: 3
        vpa:
          updateMode: "Auto"
        affinity:
          exclusiveNodePool: true
          
# Staging - Cost optimization focus  
  staging:
    config:
      cloudExtras:
        disruptionBudget:
          minAvailable: 1
        vpa:
          updateMode: "Initial"
        nodeSelector:
          cost-optimization: "spot"
```

---

# **7Ô∏è‚É£ Summary**
| Step                | Command                             | Purpose                                 |
|---------------------|-------------------------------------|-----------------------------------------|
| **Define Secrets**  | `secrets.yaml`                      | Stores GCP credentials                  |
| **Configure Infra** | `server.yaml`                       | Defines GKE Autopilot & GCP resources   |
| **Provision Infra** | `sc provision -s devops`            | Deploys GCP infrastructure              |
| **Define Service**  | `client.yaml`                       | Describes a microservice deployment     |
| **Deploy Service**  | `sc deploy -s myservice -e staging` | Deploys a microservice to GKE Autopilot |

---

## Pure Kubernetes

# **Guide: Configuring a Parent Stack for Deploying Infrastructure to a Kubernetes Cluster with Simple Container**

This guide is for **DevOps teams** who want to configure a **parent stack (`server.yaml`)** for deploying infrastructure
**to a self-managed Kubernetes cluster** using **Simple Container**.

With this setup, developers can deploy microservices without needing to manage the Kubernetes infrastructure themselves.

---

# **1Ô∏è‚É£ Prerequisites**

Before configuring the parent stack, ensure that:

‚úÖ A **Kubernetes cluster** is running and accessible.

‚úÖ You have a **`kubeconfig` file** for authentication.

‚úÖ Simple Container is installed:

   ```sh
   curl -s "https://dist.simple-container.com/sc.sh" | bash
   ```

---

# **2Ô∏è‚É£ Setting Up Secrets for Kubernetes Cluster Authentication**

In **self-managed Kubernetes clusters**, `sc` needs a `kubeconfig` file for authentication.

## **Step 1: Define `secrets.yaml`**

Create the **`.sc/stacks/devops/secrets.yaml`** file to store Kubernetes credentials:

```yaml
---
# File: "myproject/.sc/stacks/devops/secrets.yaml"
schemaVersion: 1.0

auth:
  kubernetes:
    type: kubernetes  # Authentication provider type
    config:
      kubeconfig: |-
        ---
        apiVersion: v1
        clusters:
        - cluster:
            insecure-skip-tls-verify: true
            server: https://1.2.3.4:6443  # Kubernetes API server endpoint
          name: my-k8s-cluster
        contexts:
        - context:
            cluster: my-k8s-cluster
            user: admin@my-k8s-cluster
          name: my-k8s-cluster
        current-context: my-k8s-cluster
        kind: Config
        preferences: {}
        users:
        - name: admin@my-k8s-cluster
          user:
            client-certificate-data: LS0tLS1CRUdtLS0tRU5EIENFUlRJRklDQVRFLS0tLS0KAASJD...
            client-key-data: LS0zJlYTlhaEZ3PT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=

values:
  docker-registry-username: user
  docker-registry-password: password
  pass-phrase: some-secret-passphrase
```

### **üîπ What This Does**

‚úÖ Stores **Kubernetes authentication (`kubeconfig`)**.

‚úÖ Saves **Docker registry credentials** for pulling images.

---

# **3Ô∏è‚É£ Configuring Infra Provisioning (`server.yaml`)**

Now, define **`.sc/stacks/devops/server.yaml`** to provision infrastructure inside Kubernetes.

## **Step 2: Define `server.yaml`**

```yaml
---
# File: "myproject/.sc/stacks/devops/server.yaml"
schemaVersion: 1.0

# Provisioning state management
provisioner:
  type: pulumi
  config:
    state-storage:
      type: fs  # Store state locally (alternative: S3, GCS)
      config:
        path: file:///${user:home}/.sc/pulumi/state
    secrets-provider:
      type: passphrase
      config:
        passPhrase: "${secret:pass-phrase}"

# Deployment templates for Kubernetes workloads
templates:
  stack-per-app-k8s:
    type: kubernetes-cloudrun  # Deployment type for services
    config:
      kubeconfig: "${auth:kubernetes}"
      dockerRegistryURL: docker.myproject.com
      dockerRegistryUsername: "${secret:docker-registry-username}"
      dockerRegistryPassword: "${secret:docker-registry-password}"
      caddyResource: caddy  # Routing for services
      useSSL: false  # Disable SSL (can be enabled later)

# Infrastructure resources provisioned inside Kubernetes
resources:
  registrar:
    inherit: common  # No DNS management required

  resources:
    production:
      template: stack-per-app-k8s
      resources:
        caddy:
          type: kubernetes-caddy
          config:
            kubeconfig: "${auth:kubernetes}"
            enable: true
            namespace: caddy
            image: docker.io/simplecontainer/caddy:latest
            replicas: 2
            usePrefixes: true  # Routes services using `/service1`, `/service2`
            serviceType: ClusterIP  # Internal routing
            provisionIngress: true
            useSSL: false
        postgres:
          type: kubernetes-helm-postgres-operator
          config:
            kubeconfig: "${auth:kubernetes}"
        rabbitmq:
          type: kubernetes-helm-rabbitmq-operator
          config:
            kubeconfig: "${auth:kubernetes}"
        redis:
          type: kubernetes-helm-redis-operator
          config:
            kubeconfig: "${auth:kubernetes}"
        mongodb:
          type: kubernetes-helm-mongodb-operator
          config:
            kubeconfig: "${auth:kubernetes}"
```

### **üîπ What This Does**

‚úÖ **Configures Pulumi for state management** (`fs` for local storage).

‚úÖ **Defines deployment templates** (`kubernetes-cloudrun`).

‚úÖ **Provisions Kubernetes resources**:

- **Caddy** ‚Üí Handles ingress and routing.
- **PostgreSQL, RabbitMQ, Redis, MongoDB** ‚Üí Deployed using **Helm operators**.

---

# **4Ô∏è‚É£ Provisioning the Kubernetes Parent Stack**

Once `server.yaml` is configured, **provision** the Kubernetes infrastructure:

```sh
sc provision -s devops
```

### **What This Does**

‚úÖ Connects to **Kubernetes using `kubeconfig`**.

‚úÖ Deploys **Caddy, PostgreSQL, RabbitMQ, Redis, MongoDB** inside Kubernetes.

‚úÖ Configures **persistent storage and networking**.

---

# **5Ô∏è‚É£ Deploying Microservices to Kubernetes**

Once the infrastructure is provisioned, developers can deploy their microservices.

## **Step 1: Define `client.yaml` for a Microservice**

```yaml
---
# File: "myproject/.sc/stacks/myservice/client.yaml"

schemaVersion: 1.0

stacks:
  production:
    type: cloud-compose
    parent: myproject/devops
    config:
      domain: ${env:MY_SERVICE_DOMAIN}
      prefix: ${env:MY_SERVICE_PREFIX}
      dockerComposeFile: ./docker-compose.yaml
      uses:
        - postgres
      runs:
        - myservice
      env:
        DATABASE_HOST: "${resource:postgres.host}"
        DATABASE_NAME: "${resource:postgres.database}"
        DATABASE_USER: "${resource:postgres.user}"
      secrets:
        DATABASE_PASSWORD: "${resource:postgres.password}"
```

## **Step 2: Deploy the Service**

```sh
sc deploy -s myservice -e production
```

‚úÖ The service is **automatically deployed to Kubernetes** using the defined settings.

---

# **6Ô∏è‚É£ Summary**

| Step                | Command                                | Purpose                                             |
|---------------------|----------------------------------------|-----------------------------------------------------|
| **Define Secrets**  | `secrets.yaml`                         | Stores Kubernetes credentials (`kubeconfig`)        |
| **Configure Infra** | `server.yaml`                          | Defines Kubernetes resources (DBs, queues, ingress) |
| **Provision Infra** | `sc provision -s devops`               | Deploys Kubernetes resources                        |
| **Define Service**  | `client.yaml`                          | Describes a microservice deployment                 |
| **Deploy Service**  | `sc deploy -s myservice -e production` | Deploys a microservice to Kubernetes                |

---

## Secrets Management

# Secrets Management with Simple Container

This comprehensive guide covers how to manage secrets and confidential files using Simple Container's built-in secrets management system. Simple Container uses SSH-RSA encryption to securely store and share secrets within your team while maintaining them in your Git repository.

## Overview

Simple Container's secrets management provides:

- **SSH-RSA encryption** for secure secret storage
- **Team-based access control** with public key management
- **Git-native workflow** for secret versioning and collaboration
- **Built-in commands** for easy secret lifecycle management
- **Automatic encryption/decryption** during deployment
- **Multi-key encryption** - each secret is encrypted with all authorized public keys

## How Encryption Works

Simple Container uses a multi-key encryption approach:

1. **Public keys are registered** in `.sc/secrets.yaml` file
2. **Every secret file is encrypted** with ALL registered public keys
3. **Anyone with an authorized private key** can decrypt secrets using `sc secrets reveal`
4. **Adding/removing team members** requires re-encrypting all secrets

This means that when you run `sc secrets reveal`, the system uses your private key to decrypt secrets that were encrypted with your corresponding public key (along with all other team members' public keys).

## Prerequisites

Before working with secrets, ensure you have:

- Simple Container CLI installed
- SSH RSA key pair (2048-bit supported)
- Access to the project repository

## Configuration Override

Simple Container supports configuration override via the `SIMPLE_CONTAINER_CONFIG` environment variable. This is particularly useful for CI/CD environments where you want to provide configuration without creating files:

```shell
# Override configuration via environment variable (using explicit keys)
export SIMPLE_CONTAINER_CONFIG="
projectName: your-project-name
privateKey: |
  -----BEGIN RSA PRIVATE KEY-----
  MIIEpAIBAAKCAQEA...
  -----END RSA PRIVATE KEY-----
publicKey: 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQ... user@host'
"

# Or using key paths
export SIMPLE_CONTAINER_CONFIG="
privateKeyPath: /path/to/ci/private/key
publicKeyPath: /path/to/ci/public/key
projectName: your-project-name
"

# Now sc commands will use the environment configuration
sc secrets reveal
```

**Configuration Content Options:**
Configuration files (`.sc/cfg.default.yaml` or `.sc/cfg.<profile>.yaml`) and the `SIMPLE_CONTAINER_CONFIG` environment variable can contain:

1. **Key Paths** (typical for local development):
   ```yaml
   privateKeyPath: ~/.ssh/id_rsa
   publicKeyPath: ~/.ssh/id_rsa.pub
   projectName: your-project-name
   ```

2. **Explicit Key Content** (typical for CI/CD environments):
   ```yaml
   projectName: your-project-name
   privateKey: |
     -----BEGIN RSA PRIVATE KEY-----
     MIIEpAIBAAKCAQEA...
     -----END RSA PRIVATE KEY-----
   publicKey: "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQ..."
   ```

This approach is recommended for CI/CD pipelines as it avoids creating configuration files and allows for secure key management through environment variables.

### Install Simple Container CLI

```shell
bash <(curl -Ls "https://dist.simple-container.com/sc.sh") --version
```

## SSH Key Requirements

Simple Container currently supports SSH RSA keys of size 2048 bits. Your SSH key pair should be:

- **Private key**: `~/.ssh/id_rsa` (never commit or share)
- **Public key**: `~/.ssh/id_rsa.pub` (safe to share)
- **No passphrase**: Keys should not have a passphrase for automation

### Generate SSH Keys

If you don't have SSH keys, generate them:

```shell
# Using ssh-keygen
ssh-keygen -t rsa -b 2048 -f ~/.ssh/id_rsa -N ""

# Or using sc (generates and configures automatically)
sc secrets init -g
```

## Core Secrets Commands

### `sc secrets init`

Initialize secrets management in your project or configure your local environment.

```shell
# Initialize with existing SSH key
sc secrets init

# Generate new SSH key and initialize
sc secrets init -g

# Generate separate key pair for CI/CD environment
sc secrets init --profile github --generate

# Initialize with verbose output
sc secrets init -g --verbose

# Initialize and make initial commit
sc secrets init -g --commit
```

**What it does:**

- Creates `.sc/cfg.default.yaml` configuration file (or `.sc/cfg.<profile>.yaml` when using `--profile`)
- Configures your SSH key paths or embeds key contents
- Sets up local secrets environment
- When using `--profile`, creates separate configuration for different environments (e.g., CI/CD)
- Configuration can be overridden using `SIMPLE_CONTAINER_CONFIG` environment variable

**Example output:**
```yaml
# .sc/cfg.default.yaml (default profile - using key paths)
privateKeyPath: ~/.ssh/id_rsa
publicKeyPath: ~/.ssh/id_rsa.pub
projectName: simple-container-api

# .sc/cfg.github.yaml (when using --profile github - using key paths)
privateKeyPath: .sc/profiles/github/id_rsa
publicKeyPath: .sc/profiles/github/id_rsa.pub
projectName: simple-container-api

# Example with explicit key content (typical for CI/CD)
projectName: simple-container-api
privateKey: |
  -----BEGIN RSA PRIVATE KEY-----
  MIIEpAIBAAKCAQEA1234567890abcdef...
  -----END RSA PRIVATE KEY-----
publicKey: "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQ1234567890abcdef... user@host"
```

### `sc secrets list`

Display all available secrets in the current project.

```shell
# List all secrets
sc secrets list

# List secrets with verbose output
sc secrets list --verbose

# List secrets using specific profile
sc secrets list --profile github
```

**Example output:**
```
pkg/secrets/database-password.txt
pkg/secrets/api-keys.json
pkg/secrets/ssl-cert.pem
.sc/stacks/mystack/secrets.yaml
config/production.env
```

### `sc secrets add`

Add new secrets to the encrypted store.

```shell
# Add a single secret file
sc secrets add database-password.txt

# Add multiple files
sc secrets add api-keys.json ssl-cert.pem

# Add multiple files at once
sc secrets add config-dev.env config-prod.env

# Add with verbose output
sc secrets add --verbose database-password.txt

# Add using specific profile
sc secrets add --profile github database-password.txt
```

**What it does:**

- Encrypts the specified files/directories
- Adds encrypted versions to Git tracking
- Removes plaintext versions from Git (adds to .gitignore)
- Updates secrets index

### `sc secrets reveal`

Decrypt and reveal encrypted secrets for local use.

```shell
# Reveal all secrets
sc secrets reveal

# Reveal all secrets (no support for specific files)
sc secrets reveal

# Force reveal all secrets
sc secrets reveal --force

# Reveal with verbose output
sc secrets reveal --verbose
```

**What it does:**

- Decrypts encrypted secret files using your private key
- Creates plaintext versions for local development
- Updates .gitignore to prevent accidental commits

### `sc secrets allow`

Grant access to secrets by adding team members' public keys.

```shell
# Allow access for a team member (using public key content)
sc secrets allow "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQ... username@host"

# Allow multiple users (using public key content)
sc secrets allow "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQ... user1@host" "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQ... user2@host"

# Allow with verbose output
sc secrets allow --verbose "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQ... username@host"

# Allow using specific profile
sc secrets allow --profile github "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQ... username@host"

# Allow by reading from file (practical example)
sc secrets allow "$(cat ~/.ssh/id_rsa.pub)"
sc secrets allow "$(cat /path/to/teammate.pub)"
```

**What it does:**

- Adds public key to authorized keys list in `.sc/secrets.yaml`
- Re-encrypts all secrets with the new key included
- Updates team access permissions
- Requires the actual public key content as argument, not a file path

### `sc secrets disallow`

Revoke access to secrets by removing team members' public keys.

```shell
# Revoke access for a team member (using public key content)
sc secrets disallow "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQ... username@host"

# Revoke multiple users (using public key content)
sc secrets disallow "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQ... user1@host" "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQ... user2@host"

# Revoke with verbose output
sc secrets disallow --verbose "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQ... username@host"

# Revoke using specific profile
sc secrets disallow --profile github "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQ... username@host"

# Revoke by reading from file (practical example)
sc secrets disallow "$(cat /path/to/former-teammate.pub)"
```

**What it does:**

- Removes public key from authorized keys list in `.sc/secrets.yaml`
- Re-encrypts all secrets without the removed key
- Revokes access permissions
- Requires the actual public key content as argument, not a file path

### `sc secrets hide`

Hide (encrypt) repository secrets.

```shell
# Hide all secrets in the repository
sc secrets hide

# Force re-encrypt all secrets (useful after key changes)
sc secrets hide --force

# Hide with verbose output
sc secrets hide --verbose

# Hide using specific profile
sc secrets hide --profile github
```

**What it does:**

- Encrypts all secret files in the repository
- Updates encrypted versions with current authorized keys
- Removes plaintext versions and updates .gitignore
- Use `--force` to re-encrypt even if secrets appear up-to-date

### `sc secrets reveal`

Reveal (decrypt) repository secrets for local use.

```shell
# Reveal all secrets
sc secrets reveal

# Force decrypt all secrets
sc secrets reveal --force

# Reveal with verbose output
sc secrets reveal --verbose

# Reveal using specific profile
sc secrets reveal --profile github
```

**What it does:**

- Decrypts all encrypted secret files using your private key
- Creates plaintext versions for local development
- Updates .gitignore to prevent accidental commits of plaintext secrets
- Use `--force` to decrypt even if plaintext versions already exist

### `sc secrets delete`

Delete repository secrets.

```shell
# Delete specific secret files
sc secrets delete database-password.txt

# Delete multiple secrets
sc secrets delete api-keys.json ssl-cert.pem

# Delete with verbose output
sc secrets delete --verbose config-file.env

# Delete using specific profile
sc secrets delete --profile github secret-file.txt
```

**What it does:**

- Removes specified secret files from the encrypted store
- Cleans up both encrypted and plaintext versions
- Updates the secrets registry

### `sc secrets allowed-keys`

List public keys allowed to decrypt secrets.

```shell
# List all allowed public keys
sc secrets allowed-keys

# List with verbose output
sc secrets allowed-keys --verbose

# List using specific profile
sc secrets allowed-keys --profile github
```

**What it does:**

- Displays all public keys registered in `.sc/secrets.yaml`
- Shows which keys can decrypt the repository secrets
- Useful for auditing team access


## Team Collaboration Workflow

### Setting Up Team Access

1. **Project Administrator** initializes secrets:
```shell
sc secrets init
sc secrets add initial-secrets.env
git add . && git commit -m "Initialize secrets management"
git push
```

2. **Team Members** join:
```shell
# Generate SSH key if needed
sc secrets init -g

# Share public key with administrator
cp ~/.ssh/id_rsa.pub team-member-name.pub
# Send team-member-name.pub to administrator
```

3. **Administrator** grants access:
```shell
# Add team member's public key (using public key content)
sc secrets allow "$(cat team-member-name.pub)"
# Or directly paste the public key content:
# sc secrets allow "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQ... team-member@host"

git add . && git commit -m "Add team member access"
git push
```

4. **Team Members** access secrets:
```shell
git pull
sc secrets reveal
```

### Adding New Secrets

```shell
# Add new secret file
echo "secret-value" > new-secret.txt
sc secrets add new-secret.txt

# Commit changes
git add . && git commit -m "Add new secret"
git push
```

### Rotating Secrets

```shell
# Update secret content
echo "new-secret-value" > database-password.txt

# Re-encrypt with updated content
sc secrets add database-password.txt

# Commit changes
git add . && git commit -m "Rotate database password"
git push
```

### Removing Team Member Access

```shell
# Revoke access and re-encrypt all secrets (using public key content)
sc secrets disallow "$(cat former-team-member.pub)"
# Or directly use the public key content:
# sc secrets disallow "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQ... former-team-member@host"

# Commit security changes
git add . && git commit -m "Revoke access for former team member"
git push
```

## Best Practices

### Security Best Practices

1. **Never commit private keys**:
   ```shell
   # Ensure .gitignore includes
   echo "*.pem" >> .gitignore
   echo ".sc/cfg.*.yaml" >> .gitignore
   echo "id_rsa*" >> .gitignore
   ```

2. **Regular key rotation**:
   ```shell
   # Periodically rotate SSH keys
   sc secrets init -g
   sc secrets hide --force
   ```

3. **Audit access regularly**:
   ```shell
   sc secrets allowed-keys --verbose
   # Review authorized keys list
   ```

### Development Workflow

1. **Start development session**:
   ```shell
   git pull
   sc secrets reveal
   # Now use decrypted secrets for development
   ```

2. **End development session**:
   ```shell
   # Remove plaintext secrets manually for security
   rm -f database-password.txt api-keys.json
   # Or use sc secrets hide to encrypt them
   sc secrets hide
   ```

3. **Adding new secrets during development**:
   ```shell
   # Create secret file
   echo "new-api-key" > api-key.txt
   
   # Add to encrypted store
   sc secrets add api-key.txt
   
   # Commit
   git add . && git commit -m "Add new API key"
   ```

### CI/CD Integration

#### Setting Up CI/CD Secrets Access

For automated deployments, you can create separate key pairs for CI/CD environments:

```shell
# Generate CI/CD specific key pair
sc secrets init --profile github --generate

# This creates:
# .sc/cfg.github.yaml - CI/CD configuration
# .sc/profiles/github/id_rsa - CI/CD private key
# .sc/profiles/github/id_rsa.pub - CI/CD public key
```

**Add CI/CD public key to team access:**
```shell
# Administrator adds CI/CD public key to the chain
# Note: Use the actual public key content, not the file path
sc secrets allow "$(cat .sc/profiles/github/id_rsa.pub)"
git add . && git commit -m "Add CI/CD access"
git push
```

**Important:** When using `sc secrets allow`, you must provide the actual SSH public key content, not the file path. The generated public key needs to be added to the encryption chain so that secrets can be decrypted using the corresponding private key.

**In your CI/CD pipeline (e.g., GitHub Actions):**

**Method 1: Using SIMPLE_CONTAINER_CONFIG environment variable (Recommended)**
```yaml
# .github/workflows/deploy.yml
- name: Setup and deploy with secrets
  env:
    # Reference the entire .sc/cfg.github.yaml content from GitHub Actions secret
    SIMPLE_CONTAINER_CONFIG: ${{ secrets.SC_CONFIG }}
  run: |
    # Reveal secrets (uses SIMPLE_CONTAINER_CONFIG)
    sc secrets reveal
    
    # Deploy with secrets available
    sc deploy -s myservice -e staging
```

**Setup Instructions:**
1. Create your CI/CD configuration locally:
   ```shell
   # Generate CI/CD specific key pair
   sc secrets init --profile github --generate
   ```

2. Copy the entire content of `.sc/cfg.github.yaml` to a GitHub Actions secret named `SC_CONFIG`:
   ```yaml
   # Content to store in GitHub Actions secret SC_CONFIG:
   projectName: simple-container-api
   privateKey: |
     -----BEGIN RSA PRIVATE KEY-----
     MIIEpAIBAAKCAQEA...
     -----END RSA PRIVATE KEY-----
   publicKey: "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQ... user@host"
   ```

3. Add the CI/CD public key to your project:
   ```shell
   # Administrator adds CI/CD public key to the chain
   sc secrets allow "$(cat .sc/profiles/github/id_rsa.pub)"
   git add . && git commit -m "Add CI/CD access"
   git push
   ```

**Alternative approaches:**

**Method 2: Using profile-based configuration**
```yaml
# .github/workflows/deploy.yml
- name: Setup CI/CD secrets
  run: |
    # Copy CI/CD private key from secrets
    mkdir -p .sc/profiles/github
    echo "${{ secrets.SC_GITHUB_PRIVATE_KEY }}" > .sc/profiles/github/id_rsa
    chmod 600 .sc/profiles/github/id_rsa
    
    # Reveal secrets using CI/CD profile
    sc secrets reveal --profile github

- name: Deploy with secrets
  run: |
    # Now secrets are available for deployment
    sc deploy -s myservice -e staging --profile github
```

**For deployment configuration, secrets can be injected via environment variables:**
```yaml
# In your deployment configuration
env:
  DATABASE_URL: ${secret:database-url}
  API_KEY: ${secret:api-key}
```

**Important:** For this to work, secret values must be stored in `.sc/stacks/<parent-stack>/secrets.yaml` under the `values` section:
```yaml
# .sc/stacks/your-parent-stack/secrets.yaml
values:
  database-url: "postgresql://user:pass@host:5432/db"
  api-key: "your-secret-api-key-here"
```

Simple Container automatically decrypts and injects secrets during deployment by reading from the parent stack's secrets.yaml file.

## Troubleshooting

### Common Issues

**"Permission denied" errors**:
```shell
# Check SSH key permissions
chmod 600 ~/.ssh/id_rsa
chmod 644 ~/.ssh/id_rsa.pub

# Check allowed keys
sc secrets allowed-keys
```

**"Cannot decrypt secrets"**:
```shell
# Check if your key is allowed
sc secrets allowed-keys

# Re-initialize if needed
sc secrets init

# Contact administrator to grant access
```

**"Secrets out of sync"**:
```shell
# Pull latest changes
git pull

# Re-reveal secrets
sc secrets reveal
```

### Getting Help

```shell
# Get help for any command
sc secrets --help
sc secrets init --help
sc secrets add --help
sc secrets allow --help

# Check version and configuration
sc --version
sc secrets allowed-keys --verbose

# Check allowed keys for specific profile
sc secrets allowed-keys --profile github --verbose
```

## Summary

Simple Container's secrets management provides a secure, Git-native way to handle sensitive data in your projects. Key benefits:

- **Secure**: SSH-RSA encryption with team-based access control
- **Simple**: Easy-to-use commands for all secret operations
- **Collaborative**: Git-based workflow for team secret sharing
- **Integrated**: Seamless integration with Simple Container deployments

Use the commands outlined in this guide to implement robust secrets management in your Simple Container projects.



---

## Migration

# **Tutorial: Migrating from Terraform or Pulumi to Simple Container**

This tutorial guides **DevOps teams and developers** on how to migrate from **Terraform or Pulumi** to **Simple Container** for microservice deployment and infrastructure management.

‚úÖ **Why Migrate to `sc`?**

- **Simplifies microservice deployment** (CI/CD, scaling, rollbacks, and secrets included).
- **Reduces complexity** (high-level YAML configuration instead of Terraform HCL or Pulumi code).
- **Cloud-agnostic** (migrate across AWS, GCP, and Kubernetes **without modifying service configurations**).

---

# **1Ô∏è‚É£ Understanding the Migration Approach**

| Feature                       | Terraform / Pulumi                      | Simple Container                                |
|-------------------------------|-----------------------------------------|-------------------------------------------------|
| **Infrastructure Management** | Declarative IaC (HCL, Python, Go, etc.) | **High-level YAML (`server.yaml`)**             |
| **Microservice Deployment**   | Requires external CI/CD                 | **Built-in (`client.yaml`)**                    |
| **Secrets Management**        | Requires external tools (Vault, SSM)    | **Built-in (`sc secrets`)**                     |
| **Networking & Routing**      | Requires manual configuration           | **Simplified with `caddy` and ingress support** |

‚úÖ **SC abstracts infrastructure complexity while keeping cloud provider flexibility**.

---

# **2Ô∏è‚É£ Step-by-Step Migration Guide**
This guide covers **migrating an AWS ECS Fargate and MongoDB Atlas setup** from Terraform/Pulumi to **Simple Container**.

---

## **üîπ Step 1: Extract Infrastructure Configuration**
Identify Terraform or Pulumi resources you need to migrate.

### **Example Terraform Setup**
```hcl
resource "aws_s3_bucket" "state" {
  bucket = "myproject-sc-state"
}

resource "aws_ecs_cluster" "cluster" {
  name = "my-ecs-cluster"
}

resource "aws_rds_instance" "database" {
  engine         = "postgres"
  instance_class = "db.t3.micro"
  allocated_storage = 20
}
```

### **Example Pulumi Setup (TypeScript)**
```typescript
import * as aws from "@pulumi/aws";

const stateBucket = new aws.s3.Bucket("state");

const cluster = new aws.ecs.Cluster("ecs-cluster");

const db = new aws.rds.Instance("database", {
  engine: "postgres",
  instanceClass: "db.t3.micro",
  allocatedStorage: 20,
});
```

‚úÖ **We will migrate these resources to `server.yaml`.**

---

## **üîπ Step 2: Create `secrets.yaml`**
Define **cloud authentication and credentials** in **SC‚Äôs secrets file**.

```sh
mkdir -p .sc/stacks/devops
touch .sc/stacks/devops/secrets.yaml
```

### **`secrets.yaml` for AWS & MongoDB Atlas**
```yaml
---
# File: ".sc/stacks/devops/secrets.yaml"

schemaVersion: 1.0

auth:
  aws:
    type: aws-token
    config:
      accessKey: "AKIAIOSFODNN7EXAMPLE"
      secretAccessKey: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
      region: "us-west-2"

values:
  CLOUDFLARE_API_TOKEN: "abcdefgh123456789"
  MONGODB_ATLAS_PUBLIC_KEY: "public-key-123"
  MONGODB_ATLAS_PRIVATE_KEY: "private-key-456"
```

‚úÖ **SC securely manages AWS and MongoDB credentials**.

---

## **üîπ Step 3: Define `server.yaml` for Infrastructure**
Instead of using **Terraform** or **Pulumi**, SC defines **infrastructure in `server.yaml`**.

```sh
touch .sc/stacks/devops/server.yaml
```

### **`server.yaml` for AWS ECS & MongoDB Atlas**
```yaml
---
# File: ".sc/stacks/devops/server.yaml"

schemaVersion: 1.0

# Provisioning state management
provisioner:
  type: pulumi
  config:
    state-storage:
      type: s3-bucket
      config:
        credentials: "${auth:aws}"
        bucketName: myproject-sc-state
    secrets-provider:
      type: aws-kms
      config:
        credentials: "${auth:aws}"
        keyName: myproject-sc-kms-key

# Deployment templates for ECS Fargate workloads
templates:
  stack-per-app:
    type: ecs-fargate
    config:
      credentials: "${auth:aws}"
      account: "${auth:aws.projectId}"

# Infrastructure resources provisioned inside AWS & MongoDB Atlas
resources:
  registrar:
    type: cloudflare
    config:
      credentials: "${secret:CLOUDFLARE_API_TOKEN}"
      accountId: "89cc23bd273c76d6767f6566c54621c2"
      zoneName: "myproject.com"

  resources:
    staging:
      template: stack-per-app
      resources:
        mongodb:
          type: mongodb-atlas
          config:
            admins: [ "admin" ]
            developers: [ "developer1" ]
            instanceSize: "M10"
            region: "US_WEST_2"
            cloudProvider: AWS
            privateKey: "${secret:MONGODB_ATLAS_PRIVATE_KEY}"
            publicKey: "${secret:MONGODB_ATLAS_PUBLIC_KEY}"
```

‚úÖ **SC simplifies infrastructure by grouping resources logically in `server.yaml`**.

---

## **üîπ Step 4: Provision the Infrastructure**
Instead of running **Pulumi or Terraform**, use:
```sh
sc provision -s devops
```
‚úÖ **This provisions AWS ECS, MongoDB Atlas, and networking automatically**.

---

## **üîπ Step 5: Define `client.yaml` for Microservices Deployment**
Configure **SC‚Äôs microservice deployment** instead of Terraform‚Äôs ECS task definitions.

```sh
mkdir -p .sc/stacks/myservice
touch .sc/stacks/myservice/client.yaml
```

### **`client.yaml` for Deploying a Microservice**
```yaml
---
# File: ".sc/stacks/myservice/client.yaml"

schemaVersion: 1.0

stacks:
  staging:
    type: cloud-compose
    parent: myproject/devops
    config:
      domain: staging-myservice.myproject.com
      dockerComposeFile: ./docker-compose.yaml
      uses:
        - mongodb
      runs:
        - myservice
      env:
        DATABASE_HOST: "${resource:mongodb.host}"
        DATABASE_NAME: "${resource:mongodb.database}"
        DATABASE_USER: "${resource:mongodb.user}"
      secrets:
        DATABASE_PASSWORD: "${resource:mongodb.password}"
```

‚úÖ **SC automatically maps microservices to infrastructure resources**.

---

## **üîπ Step 6: Deploy the Microservice**
Instead of manually defining ECS tasks in Terraform or Pulumi, use:
```sh
sc deploy -s myservice -e staging
```
‚úÖ **SC automates the CI/CD process**.

---

# **3Ô∏è‚É£ Summary of Migration**
| Task                               | Terraform / Pulumi                | Simple Container             |
|------------------------------------|-----------------------------------|-------------------------------------|
| **Define Secrets**                 | AWS Secrets Manager, Vault        | `secrets.yaml`                      |
| **Define Infrastructure**          | Terraform / Pulumi files          | `server.yaml`                       |
| **Provision Infra**                | `terraform apply` or `pulumi up`  | `sc provision -s devops`            |
| **Define Microservice Deployment** | ECS Task Definitions, Helm Charts | `client.yaml`                       |
| **Deploy Microservice**            | CI/CD + Terraform                 | `sc deploy -s myservice -e staging` |

‚úÖ **SC simplifies infrastructure and deployment** while keeping cloud flexibility.

---

# Examples

## Overview

# Real-World Examples

This directory contains production-tested Simple Container configurations based on real-world deployments. All company names, domains, and sensitive information have been anonymized while preserving the actual configuration patterns and best practices.

## Directory Structure

### Static Websites (`static-websites/`)
- **documentation-site**: MkDocs documentation deployment (based on Simple Container docs)
- **landing-page**: Main website with SPA configuration
- **admin-dashboard**: Admin UI deployment patterns
- **customer-portal**: Customer-facing UI deployment
- **media-store**: Media-specific static hosting

### ECS Deployments (`ecs-deployments/`)
- **backend-service**: Node.js backend with MongoDB integration
- **vector-database**: High-performance vector database with NLB
- **blockchain-service**: Blockchain integration with cross-service dependencies
- **blog-platform**: Multi-service deployment with reverse proxy
- **meteor-app**: Meteor.js application deployment

### Lambda Functions (`lambda-functions/`)
- **ai-gateway**: AWS Bedrock integration with specific IAM roles
- **storage-service**: Scheduled cleanup with cron expressions
- **billing-system**: Multi-environment Lambda with long timeouts
- **scheduler**: High-frequency scheduling (every minute)
- **cost-analytics**: AWS cost analysis with comprehensive IAM permissions

### GKE Autopilot (`gke-autopilot/`)
- **comprehensive-setup**: Complete GCP setup with all resources
- **template-based**: GKE template with resource references

### Resource Adoption (`resource-adoption/`)
- **multi-environment**: Complete example of adopting existing MongoDB, PostgreSQL, Redis, and GKE resources across production and staging environments

### Kubernetes Native (`kubernetes-native/`)
- **streaming-platform**: Hardcoded infrastructure IPs with N8N integration
- **ai-development**: High-resource code execution environment
- **zero-downtime**: Advanced deployment configurations

### Advanced Configurations (`advanced-configs/`)
- **mixed-environments**: Different deployment types per environment
- **high-resource**: 32GB memory, 16 CPU configurations
- **ai-integration**: AI-powered development tools
- **blockchain-testnet**: Testnet integration patterns

### Parent Stacks (`parent-stacks/`)
- **aws-multi-region**: Multi-region AWS setup with comprehensive resources
- **gcp-comprehensive**: Complete GCP setup with all service types
- **hybrid-cloud**: Mixed cloud provider configurations

### CI/CD with GitHub Actions (`cicd-github-actions/`)
- **basic-setup**: Simple staging/production pipeline with automatic deployment
- **multi-stack**: Complex deployment managing multiple related stacks
- **preview-deployments**: PR-based preview environments with cleanup automation
- **advanced-notifications**: Multi-channel notifications with custom templates

### Kubernetes Affinity (`kubernetes-affinity/`)
- **multi-tier-node-isolation**: Real-world node pool isolation for multi-tier architecture
- **high-availability**: Zone anti-affinity and pod distribution patterns
- **performance-optimization**: Resource-specific scheduling and optimization

### Resource Management (`resource-management/`)
- **basic-resource-config**: Explicit CPU and memory limits and requests configuration
- **legacy-compatibility**: How existing configurations continue to work unchanged
- **mixed-configuration**: Combining new explicit and legacy configuration approaches
- **production-optimized**: Production-ready configurations with optimal resource allocation

## Usage

Each example directory contains:
- `client.yaml` - Service deployment configuration
- `server.yaml` - Parent stack with resource definitions (when applicable)
- `README.md` - Specific documentation for the example

All examples use anonymized domains like `example.com`, `mycompany.com`, etc., and generic resource names that can be easily adapted to your use case.

## Key Patterns Demonstrated

- **Resource References**: `${resource:database.uri}`, `${secret:api-key}` patterns
- **Multi-Environment**: Staging/production configurations with YAML anchors
- **Security**: Cloudflare-only ingress, proper IAM roles
- **Scaling**: Auto-scaling configurations with various CPU thresholds
- **Advanced Features**: Response streaming, scheduled jobs, cross-service dependencies
- **Database Integration**: MongoDB, PostgreSQL, MySQL resource patterns
- **Email Services**: SMTP integration patterns
- **AI/ML Integration**: Bedrock, LLM proxy configurations
- **Blockchain**: Smart contract integration patterns
- **Kubernetes Affinity**: Node pool isolation, pod scheduling, performance optimization
- **Enterprise Scheduling**: Multi-tier architectures with workload separation
- **Resource Management**: CPU and memory requests/limits optimization, backward compatibility


---

## Static Websites

# Static Website Examples

This section contains examples of deploying static websites using Simple Container.

## Available Examples

### Documentation Site
Deploy a MkDocs documentation site with Simple Container.

**Use Case:** Technical documentation, API docs, project documentation

**Configuration:**
```yaml
# .sc/stacks/docs-site/client.yaml
schemaVersion: 1.0
stacks:
  production:
    type: static
    parent: myorg/infrastructure
    config:
      bundleDir: ${git:root}/site
      domain: "docs.mycompany.com"
      indexDocument: index.html
      errorDocument: index.html
```

**Enhanced Example with Dynamic Placeholders:**
```yaml
# .sc/stacks/docs-site/client.yaml - Using new placeholders safely
schemaVersion: 1.0
stacks:
  # Dynamic stack name based on git branch for branch-specific deployments
  "${git:branch}":
    type: static
    parent: myorg/infrastructure
    config:
      bundleDir: ${git:root}/site
      # Dynamic domain based on branch: main.docs.mycompany.com, staging.docs.mycompany.com
      domain: "${git:branch}.docs.mycompany.com"
      indexDocument: index.html
      errorDocument: index.html
      # Note: Avoid version field here as it causes CloudFront recreation
      # Version tracking can be done in build process or deployment metadata
```

**Features:**

- Automated MkDocs build and deployment
- CloudFront CDN distribution
- Custom domain with SSL/TLS
- Automatic cache invalidation

### Landing Page
Deploy a company landing page or marketing site.

**Use Case:** Company websites, product landing pages, marketing campaigns

**Configuration:**
```yaml
# .sc/stacks/landing/client.yaml
schemaVersion: 1.0
stacks:
  production:
    type: static
    parent: myorg/infrastructure
    config:
      bundleDir: ${git:root}/dist
      domain: "www.mycompany.com"
      indexDocument: index.html
      errorDocument: index.html  # SPA routing support
```

**Features:**

- SPA (Single Page Application) routing
- Custom domain with SSL/TLS
- Global CDN distribution
- SEO optimization support

### Admin Dashboard
Deploy an admin dashboard or internal tool.

**Use Case:** Internal admin interfaces, dashboards, management tools

**Configuration:**
```yaml
# .sc/stacks/admin/client.yaml
schemaVersion: 1.0
stacks:
  staging:
    type: static
    parent: myorg/infrastructure
    config:
      bundleDir: ${git:root}/build
      domain: "admin-staging.mycompany.com"
      indexDocument: index.html
      errorDocument: index.html
  production:
    type: static
    parent: myorg/infrastructure
    config:
      bundleDir: ${git:root}/build
      domain: "admin.mycompany.com"
      indexDocument: index.html
      errorDocument: index.html
```

**Features:**

- Multi-environment deployment (staging/production)
- Basic authentication for security
- Internal tool deployment
- Secure access controls

### Customer Portal
Deploy a customer-facing portal or self-service interface.

**Use Case:** Customer portals, self-service interfaces, user dashboards

**Configuration:**
```yaml
# .sc/stacks/portal/client.yaml
schemaVersion: 1.0
stacks:
  production:
    type: static
    parent: myorg/infrastructure
    config:
      bundleDir: ${git:root}/build
      domain: "portal.mycompany.com"
      indexDocument: index.html
      errorDocument: index.html
```

**Features:**

- Customer-facing interface
- Security headers configuration
- SPA routing support
- Custom domain and SSL

## Common Patterns

### Multi-Environment Setup
```yaml
stacks:
  staging:
    type: aws-static-website
    parent: myorg/infrastructure
    parentEnv: staging
    config:
      domain: "staging.mysite.com"
  production:
    type: aws-static-website
    parent: myorg/infrastructure
    parentEnv: production
    config:
      domain: "mysite.com"
```

### Custom Build Process
```yaml
# docker-compose.yaml
version: '3.8'
services:
  build:
    build: .
    volumes:
      - ./dist:/app/dist
    command: npm run build
```

## Deployment Commands

**Deploy to staging:**
```bash
sc deploy -s mysite -e staging
```

**Deploy to production:**
```bash
sc deploy -s mysite -e production
```

## Best Practices

- **Use environment-specific domains** for staging vs production
- **Enable SPA routing** for single-page applications
- **Configure security headers** for enhanced security
- **Set up proper caching** through CloudFront configuration
- **Use basic auth** for internal tools and admin interfaces


---

## ECS Deployments

# ECS Deployment Examples

This section contains examples of deploying containerized applications to AWS ECS Fargate using Simple Container.

## Available Examples

### Backend Service
Deploy a Node.js backend service with MongoDB integration.

**Use Case:** REST APIs, GraphQL services, microservices backends

**Configuration:**
```yaml
# .sc/stacks/backend/client.yaml
schemaVersion: 1.0
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      domain: api.mycompany.com
      size:
        cpu: 2048
        memory: 4096
      dockerComposeFile: ${git:root}/docker-compose.yaml
      uses: [mongodb-shared, redis-cache]
      runs: [backend-service]
      # Enhanced with new placeholder extensions in environment variables
      env:
        MONGODB_URL: "${resource:mongodb-shared.uri}"
        REDIS_HOST: "${resource:redis-cache.host}"
        REDIS_PORT: "${resource:redis-cache.port}"
        NODE_ENV: production
        # Deployment tracking with new date and git placeholders
        DEPLOYMENT_TIME: "${date:iso8601}"           # 2024-10-24T20:46:41Z
        BUILD_VERSION: "${date:dateOnly}.${git:commit.short}"  # 2024-10-24.a1b2c3d
        GIT_BRANCH: "${git:branch.clean}"            # Clean branch name
        GIT_COMMIT: "${git:commit.short}"            # Short commit hash
        BUILD_ID: "${git:branch}-${date:timestamp}"  # Unique build identifier
      alerts:
        slack:
          webhookUrl: ${secret:alerts-slack-webhook}
        # ECS Service Monitoring
        maxMemory:
          threshold: 80
          alertName: backend-max-memory
          description: "Backend memory usage exceeds 80%"
        maxCPU:
          threshold: 70
          alertName: backend-max-cpu
          description: "Backend CPU usage exceeds 70%"
        # ALB Monitoring
        serverErrors:
          threshold: 10
          alertName: backend-server-errors
          description: "High 5XX error rate detected"
          periodSec: 300
        unhealthyHosts:
          threshold: 1
          alertName: backend-unhealthy-hosts
          description: "Unhealthy targets behind load balancer"
          periodSec: 300
        responseTime:
          threshold: 2.0
          alertName: backend-response-time
          description: "Response time exceeds 2 seconds"
          periodSec: 300
```

**Docker Compose:**
```yaml
# docker-compose.yaml (for local development)
version: '3.8'
services:
  backend-service:
    build: .
    ports:
      - "3000:3000"
    environment:
      MONGODB_URL: "mongodb://localhost:27017/backend_dev"
      REDIS_URL: "redis://localhost:6379"
      NODE_ENV: development
```

**Features:**

- MongoDB Atlas integration
- Redis caching layer
- Auto-scaling configuration
- Health checks and monitoring
- Comprehensive ALB monitoring (5XX errors, unhealthy hosts, response time)
- ECS service monitoring (CPU, memory)
- Multi-channel alerting (Slack, Discord, Telegram, Email)
- Secure environment variable injection

### Vector Database
Deploy a high-performance vector database service.

**Use Case:** AI/ML applications, similarity search, recommendation engines

**Configuration:**
```yaml
# .sc/stacks/vectordb/client.yaml
schemaVersion: 1.0
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      domain: vectordb.mycompany.com
      size:
        cpu: 4096
        memory: 8192
      dockerComposeFile: ${git:root}/docker-compose.yaml
      uses: [vector-storage, nlb-loadbalancer]
      runs: [vector-service]
      alerts:
        slack:
          webhookUrl: ${secret:alerts-slack-webhook}
        maxMemory:
          threshold: 85
          alertName: vectordb-max-memory
          description: "Vector DB memory usage exceeds 85%"
        maxCPU:
          threshold: 75
          alertName: vectordb-max-cpu
          description: "Vector DB CPU usage exceeds 75%"
```

**Features:**

- Network Load Balancer for high performance
- Auto-scaling based on CPU utilization
- Persistent vector storage
- High-throughput configuration
- GPU support for ML workloads

### Blockchain Service
Deploy blockchain integration services.

**Use Case:** Web3 applications, cryptocurrency services, smart contract interaction

**Configuration:**
```yaml
# .sc/stacks/blockchain/client.yaml
schemaVersion: 1.0
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [ethereum-node, postgres-db]
      domain: blockchain.mycompany.com
      size:
        cpu: 2048
        memory: 4096
      dockerComposeFile: ${git:root}/docker-compose.yaml
      runs: [blockchain-service]
      dependencies:
        - name: ethereum-shared
          owner: myproject/blockchain-infrastructure  
          resource: ethereum-node-cluster
```

**Features:**

- Ethereum node integration
- Cross-service dependencies
- PostgreSQL for transaction storage
- Secure API endpoints
- Real-time blockchain monitoring

### Blog Platform
Deploy a multi-service blog platform.

**Use Case:** Content management, publishing platforms, media sites

**Configuration:**
```yaml
# .sc/stacks/blog/client.yaml
schemaVersion: 1.0
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [postgres-db, redis-cache, s3-media]
      domain: blog.mycompany.com
      size:
        cpu: 1024
        memory: 2048
      dockerComposeFile: ${git:root}/docker-compose.yaml
      runs: [blog-api, blog-admin]
```

**Docker Compose:**
```yaml
version: '3.8'
services:
  blog-api:
    build: ./api
    ports:
      - "3000:3000"
    environment:
      DATABASE_URL: ${DATABASE_URL}
      REDIS_URL: ${REDIS_URL}
      S3_BUCKET: ${S3_BUCKET}
  
  blog-admin:
    build: ./admin
    ports:
      - "3001:3001"
    environment:
      API_URL: "https://blog.mycompany.com/api"
```

**Features:**

- Multi-service deployment
- Reverse proxy configuration
- Media storage with S3
- Admin interface separation
- Content delivery optimization

### Meteor.js Application
Deploy a Meteor.js full-stack application.

**Use Case:** Real-time applications, collaborative tools, full-stack JavaScript apps

**Configuration:**
```yaml
# .sc/stacks/meteor-app/client.yaml
schemaVersion: 1.0
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [mongodb-shared]
      domain: app.mycompany.com
      size:
        cpu: 1024
        memory: 2048
      dockerComposeFile: ${git:root}/docker-compose.yaml
      runs: [meteor-app]
```

**Features:**

- MongoDB integration
- Real-time data synchronization
- WebSocket support
- Meteor-specific optimizations
- Session affinity configuration

## Common Patterns

### Multi-Service Architecture
```yaml
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [postgres-db, redis-cache, s3-storage]
      runs: [api-service, worker-service, scheduler]
      dependencies:
        - name: billing
          owner: myproject/billing
          resource: mongo-cluster2
```


### Health Checks
```yaml
# docker-compose.yaml
services:
  api-service:
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
```

## Deployment Commands

**Deploy to staging:**
```bash
sc deploy -s myservice -e staging
```

**Deploy to production:**
```bash
sc deploy -s myservice -e production
```

## Best Practices

- **Use health checks** for all services to ensure proper deployment
- **Configure auto-scaling** based on actual usage patterns
- **Implement proper logging** with structured log formats
- **Use environment-specific configurations** for different environments
- **Set up comprehensive monitoring and alerting** for production services:
  - **ALB Monitoring**: Monitor server errors (5XX), unhealthy hosts, and response times
  - **ECS Monitoring**: Track CPU and memory utilization
  - **Multi-channel alerts**: Configure Slack, Discord, Telegram, or email notifications
- **Implement graceful shutdown** handling in your applications
- **Use secrets management** for sensitive configuration values

### ALB Monitoring Configuration

For production deployments with Application Load Balancers, configure monitoring for:

```yaml
alerts:
  # Notification channels
  slack:
    webhookUrl: ${secret:alerts-slack-webhook}
  email:
    addresses:
      - "ops@company.com"
      
  # ALB Health Monitoring
  serverErrors:
    threshold: 5.0        # Alert on 5+ server errors per period
    periodSec: 300        # 5-minute evaluation period
    alertName: "prod-server-errors"
    description: "High 5XX error rate detected"
    
  unhealthyHosts:
    threshold: 1          # Alert when any target becomes unhealthy
    periodSec: 300
    alertName: "prod-unhealthy-hosts"
    description: "Unhealthy targets detected"
    
  responseTime:
    threshold: 1.5        # Alert when response time > 1.5 seconds
    periodSec: 300
    alertName: "prod-response-time"
    description: "Response time degradation"
```

**Technical Details:**
- Server errors use `HTTPCode_Target_5XX_Count` metric with LoadBalancer dimension
- Unhealthy hosts use `UnHealthyHostCount` metric with LoadBalancer + TargetGroup dimensions
- Response time uses `TargetResponseTime` metric with LoadBalancer dimension
- All metrics use full AWS load balancer identifiers for reliable monitoring


---

## Lambda Functions

# Lambda Functions Examples

This section contains examples of deploying AWS Lambda functions using Simple Container.

## Available Examples

### AI Gateway
Deploy an AWS Bedrock integration service with specific IAM roles.

**Use Case:** AI/ML API gateway, content generation, natural language processing

**Configuration:**
```yaml
# .sc/stacks/ai-gateway/client.yaml
schemaVersion: 1.0
stacks:
  production:
    type: single-image
    template: lambda-us
    parent: myorg/infrastructure
    config:
      image:
        dockerfile: ${git:root}/Dockerfile
      timeout: 300
      maxMemory: 2048
      env:
        BEDROCK_REGION: us-east-1
        MODEL_ID: anthropic.claude-v2
```

**Dockerfile:**
```dockerfile
FROM public.ecr.aws/lambda/python:3.11
COPY requirements.txt ${LAMBDA_TASK_ROOT}
RUN pip install -r requirements.txt
COPY lambda_function.py ${LAMBDA_TASK_ROOT}
CMD ["lambda_function.lambda_handler"]
```

**Features:**

- AWS Bedrock integration
- Specific IAM roles for AI services
- Custom model configuration
- Secure API endpoints
- Cost-optimized serverless execution

### Storage Service
Deploy scheduled cleanup with cron expressions.

**Use Case:** Data cleanup, file management, automated maintenance tasks

**Configuration:**
```yaml
# .sc/stacks/storage-cleanup/client.yaml
schemaVersion: 1.0
stacks:
  production:
    type: single-image
    template: lambda-us
    parent: myorg/infrastructure
    config:
      image:
        dockerfile: ${git:root}/Dockerfile
      timeout: 900
      maxMemory: 1024
      cloudExtras:
        lambdaInvokeMode: RESPONSE_STREAM
        lambdaRoutingType: function-url
        lambdaSchedules:
          - name: daily-cleanup
            expression: "cron(0 2 * * ? *)"  # Daily at 2 AM
            request: |-
              {
                "requestId": "daily-cleanup",
                "httpMethod": "POST",
                "path": "/api/cleanup",
                "body": "{\"action\":\"cleanup\"}",
                "headers": {
                  "Authorization": "Bearer ${secret:CLEANUP_API_KEY}"
                }
              }
      env:
        ENV: production
        HOME: /tmp
      secrets:
        S3_BUCKET: "${resource:storage-bucket.name}"
        RETENTION_DAYS: "30"
        CLEANUP_API_KEY: "${secret:CLEANUP_API_KEY}"
```

**Features:**

- Scheduled execution with cron expressions
- S3 bucket cleanup automation
- Configurable retention policies
- Error handling and logging
- Cost-effective maintenance automation

### Scheduler
Deploy high-frequency scheduling (every minute).

**Use Case:** Real-time monitoring, frequent data processing, system health checks

**Configuration:**
```yaml
# .sc/stacks/scheduler/client.yaml
schemaVersion: 1.0
stacks:
  production:
    type: single-image
    template: lambda-us
    parent: myorg/infrastructure
    config:
      image:
        dockerfile: ${git:root}/Dockerfile
      timeout: 60
      maxMemory: 512
      cloudExtras:
        lambdaInvokeMode: RESPONSE_STREAM
        lambdaRoutingType: function-url
        lambdaSchedules:
          - name: health-check
            expression: "rate(1 minute)"
            request: |-
              {
                "requestId": "health-check",
                "httpMethod": "POST",
                "path": "/api/health-check",
                "body": "{\"check\":\"all\"}",
                "headers": {
                  "Authorization": "Bearer ${secret:SCHEDULER_API_KEY}"
                }
              }
      env:
        ENV: production
        HOME: /tmp
        MONITORING_ENDPOINT: "https://api.mycompany.com/health"
      secrets:
        ALERT_WEBHOOK: "${secret:SLACK_WEBHOOK_URL}"
        SCHEDULER_API_KEY: "${secret:SCHEDULER_API_KEY}"
```

**Features:**

- High-frequency execution (every minute)
- Real-time monitoring capabilities
- Webhook integration for alerts
- Fast execution and response
- Minimal cold start optimization

### Cost Analytics
Deploy AWS cost analysis with comprehensive IAM permissions.

**Use Case:** Cost monitoring, billing analysis, resource optimization

**Configuration:**
```yaml
# .sc/stacks/cost-analytics/client.yaml
schemaVersion: 1.0
stacks:
  production:
    type: single-image
    template: lambda-us
    parent: myorg/infrastructure
    config:
      image:
        dockerfile: ${git:root}/Dockerfile
      timeout: 600
      maxMemory: 1024
      cloudExtras:
        lambdaInvokeMode: RESPONSE_STREAM
        lambdaRoutingType: function-url
        awsRoles:
          - ce:GetCostAndUsage
          - ce:GetUsageReport
          - ce:DescribeCostCategoryDefinition
          - budgets:ViewBudget
        lambdaSchedules:
          - name: daily-cost-report
            expression: "cron(0 8 * * ? *)"
            request: |-
              {
                "requestId": "daily-cost-report",
                "httpMethod": "POST",
                "path": "/api/generate-report",
                "body": "{\"reportType\":\"daily\"}",
                "headers": {
                  "Authorization": "Bearer ${secret:COST_ANALYTICS_API_KEY}"
                }
              }
      env:
        ENV: production
        HOME: /tmp
        COST_EXPLORER_REGION: us-east-1
        REPORT_S3_BUCKET: "${resource:reports-bucket.name}"
        NOTIFICATION_EMAIL: "${secret:ADMIN_EMAIL}"
```

**IAM Permissions Required:**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "ce:GetCostAndUsage",
        "ce:GetUsageReport",
        "ce:DescribeCostCategoryDefinition"
      ],
      "Resource": "*"
    }
  ]
}
```

**Features:**

- AWS Cost Explorer integration
- Comprehensive IAM permissions
- Automated cost reporting
- S3 report storage
- Email notifications

### Billing System
Deploy multi-environment billing with YAML anchors.

**Use Case:** Subscription billing, payment processing, invoice generation

**Configuration:**
```yaml
# .sc/stacks/billing/client.yaml
schemaVersion: 1.0

# YAML anchors for reusable configuration
x-billing-config: &billing-config
  image:
    dockerfile: ${git:root}/Dockerfile
  timeout: 300
  env:
    DATABASE_URL: "${resource:postgres-db.url}"
    STRIPE_WEBHOOK_SECRET: "${secret:STRIPE_WEBHOOK_SECRET}"

stacks:
  staging:
    type: single-image
    template: lambda-us
    parent: myorg/infrastructure
    parentEnv: staging
    config:
      <<: *billing-config
      env:
        <<: *billing-config.env
        STRIPE_API_KEY: "${secret:STRIPE_TEST_KEY}"
        ENVIRONMENT: staging
  
  production:
    type: single-image
    template: lambda-us
    parent: myorg/infrastructure
    parentEnv: production
    config:
      <<: *billing-config
      env:
        <<: *billing-config.env
        STRIPE_API_KEY: "${secret:STRIPE_LIVE_KEY}"
        ENVIRONMENT: production
```

**Features:**

- Multi-environment deployment
- YAML anchors for configuration reuse
- Stripe payment integration
- Database connectivity
- Webhook handling
- Environment-specific API keys


## Deployment Commands

**Deploy to staging:**
```bash
sc deploy -s mylambda -e staging
```

**Deploy to production:**
```bash
sc deploy -s mylambda -e production
```

## Best Practices

- **Use appropriate timeout values** based on function complexity
- **Implement proper error handling** and retry logic
- **Configure dead letter queues** for failed executions
- **Use environment variables** for configuration
- **Optimize cold start performance** with proper runtime selection
- **Monitor function metrics** and set up appropriate alarms
- **Use IAM roles with least privilege** principle
- **Implement structured logging** for better observability
- **Consider memory allocation** for optimal cost/performance ratio


---

## GKE Autopilot

# GKE Autopilot Examples

This section contains examples of deploying applications to Google Kubernetes Engine Autopilot using Simple Container.

## Available Examples

### Comprehensive Setup
Complete GCP setup with GKE, Artifact Registry, Pub/Sub, Redis, and MongoDB Atlas.

**Use Case:** Full-stack applications, microservices architecture, production-ready GCP deployment

**Parent Stack Configuration:**
```yaml
# .sc/stacks/devops/server.yaml
schemaVersion: 1.0
templates:
  gke-autopilot-template:
    type: gcp-gke-autopilot
    gkeClusterResource: "main-gke-cluster"
    artifactRegistryResource: "main-registry"

resources:
  resources:
    production:
      main-gke-cluster:
        type: gcp-gke-autopilot-cluster
        config:
          projectId: "${auth:gcloud.projectId}"
          credentials: "${auth:gcloud}"
          location: europe-west3
          gkeMinVersion: "1.33.4-gke.1245000"  # Check: gcloud container get-server-config --location=europe-west3
          
      main-registry:
        type: gcp-artifact-registry
        config:
          projectId: "${auth:gcloud.projectId}"
          credentials: "${auth:gcloud}"
          location: europe-west3
          docker:
            immutableTags: true
            
      redis-cache:
        type: gcp-redis
        config:
          projectId: "${auth:gcloud.projectId}"
          credentials: "${auth:gcloud}"
          location: europe-west3
          tier: standard
          memorySizeGb: 1
          
      pubsub-messaging:
        type: gcp-pubsub
        config:
          projectId: "${auth:gcloud.projectId}"
          credentials: "${auth:gcloud}"
          topics:
            - name: "user-events"
              labels:
                environment: "production"
          subscriptions:
            - name: "user-events-processor"
              topic: "user-events"
              ackDeadlineSec: 60
              exactlyOnceDelivery: true
              
      mongodb-atlas:
        type: mongodb-atlas
        config:
          publicKey: "${secret:MONGODB_ATLAS_PUBLIC_KEY}"
          privateKey: "${secret:MONGODB_ATLAS_PRIVATE_KEY}"
          orgId: "${secret:MONGODB_ATLAS_ORG_ID}"
          projectId: "${secret:MONGODB_ATLAS_PROJECT_ID}"
          instanceSize: M30
          region: EUROPE_WEST_3
          cloudProvider: GCP
          admins: ["admin@mycompany.com"]
          developers: ["dev@mycompany.com"]
          backup:
            every: 6h
            retention: 168h
          networkConfig:
            allowCidrs: ["10.0.0.0/8"]
          extraProviders:
            - name: GCP
              credentials: "${auth:gcloud}"
```

**Client Stack Configuration:**
```yaml
# .sc/stacks/myapp/client.yaml
schemaVersion: 1.0
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [redis-cache, pubsub-messaging, mongodb-atlas]
      runs: [web-app, worker-service]
```

**Docker Compose:**
```yaml
# docker-compose.yaml
version: '3.8'
services:
  web-app:
    build: ./web
    ports:
      - "8080:8080"
    environment:
      REDIS_URL: ${REDIS_URL}
      MONGODB_URL: ${MONGODB_URL}
      PUBSUB_TOPIC: ${PUBSUB_TOPIC}
      
  worker-service:
    build: ./worker
    environment:
      REDIS_URL: ${REDIS_URL}
      MONGODB_URL: ${MONGODB_URL}
      PUBSUB_SUBSCRIPTION: ${PUBSUB_SUBSCRIPTION}
```

**Features:**

- Complete GCP setup with all major services
- GKE Autopilot for managed Kubernetes
- Artifact Registry for container images
- Redis for caching and session storage
- Pub/Sub for event-driven architecture
- MongoDB Atlas integration with GCP networking
- Multi-service deployment with resource sharing

### Multi-Region Deployment
Deploy applications across multiple GCP regions for high availability.

**Use Case:** Global applications, disaster recovery, low-latency worldwide access

**Configuration:**
```yaml
# .sc/stacks/global-app/client.yaml
schemaVersion: 1.0
stacks:
  us-production:
    type: cloud-compose
    parent: myorg/infrastructure-us
    config:
      uses: [gke-cluster-us, redis-us]
      runs: [api-service]
      
  eu-production:
    type: cloud-compose
    parent: myorg/infrastructure-eu
    config:
      uses: [gke-cluster-eu, redis-eu]
      runs: [api-service]
```

**Features:**

- Multi-region deployment
- Regional resource isolation
- Global load balancing
- Disaster recovery capabilities
- Reduced latency for global users

## Common Patterns

### Microservices Architecture
```yaml
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [gke-cluster, artifact-registry, mongodb-atlas]
      runs: [web-app, worker-service]
```

### Auto-Scaling Configuration
```yaml
# In docker-compose.yaml
services:
  web-app:
    deploy:
      resources:
        requests:
          cpu: "100m"
          memory: "128Mi"
        limits:
          cpu: "500m"
          memory: "512Mi"
      replicas: 3
      update_config:
        parallelism: 1
        delay: 10s
```


## Deployment Commands

**Deploy to staging:**
```bash
sc deploy -s myapp -e staging
```

**Deploy to production:**
```bash
sc deploy -s myapp -e production
```

## Best Practices

- **Use GKE Autopilot** for managed Kubernetes experience
- **Configure proper resource requests and limits** for optimal scheduling
- **Implement health checks** for all services
- **Use Artifact Registry** for secure container image storage
- **Configure network policies** for security isolation
- **Set up monitoring and logging** with Google Cloud Operations
- **Use Workload Identity** for secure GCP service access
- **Implement proper secret management** with Google Secret Manager
- **Configure auto-scaling** based on actual usage patterns
- **Use regional persistent disks** for data durability


---

## Kubernetes Native

# Kubernetes Native Examples

This section contains examples of deploying applications to native Kubernetes clusters using Simple Container.

## Available Examples

### Streaming Platform
Deploy a streaming platform with hardcoded IPs, N8N integration, and zero-downtime configurations.

**Use Case:** Media streaming, real-time data processing, workflow automation

**Configuration:**
```yaml
# .sc/stacks/streaming/client.yaml
schemaVersion: 1.0
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [redis-cache]
      runs: [streaming-server, n8n-workflow]
```

**Docker Compose:**
```yaml
# docker-compose.yaml
version: '3.8'
services:
  streaming-server:
    build: ./streaming
    ports:
      - "8080:8080"
    environment:
      RTMP_PORT: "1935"
      HLS_PORT: "8080"
      REDIS_URL: ${REDIS_URL}
    volumes:
      - streaming-data:/var/streaming
      
  n8n-workflow:
    image: n8nio/n8n:latest
    ports:
      - "5678:5678"
    environment:
      N8N_BASIC_AUTH_ACTIVE: "true"
      N8N_BASIC_AUTH_USER: "${secret:N8N_USERNAME}"
      N8N_BASIC_AUTH_PASSWORD: "${secret:N8N_PASSWORD}"
      WEBHOOK_URL: "https://streaming.mycompany.com/webhook"
    volumes:
      - n8n-data:/home/node/.n8n

volumes:
  streaming-data:
  n8n-data:
```


**Features:**

- Hardcoded IP addresses for consistent access
- N8N workflow automation integration
- Zero-downtime deployment strategy
- RTMP and HLS streaming support
- Redis integration for session management
- Custom registry authentication
- SSL/TLS termination with Caddy

### High-Resource Environment
Deploy applications requiring significant compute resources.

**Use Case:** AI/ML workloads, data processing, compute-intensive applications

**Configuration:**
```yaml
# .sc/stacks/ai-workload/client.yaml
schemaVersion: 1.0
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [gpu-resources, storage-volumes]
      runs: [ai-processor]
```

**Docker Compose:**
```yaml
# docker-compose.yaml
version: '3.8'
services:
  ai-processor:
    build: ./ai-processor
    ports:
      - "8080:8080"
    deploy:
      resources:
        requests:
          cpus: '8.0'
          memory: 32G
        limits:
          cpus: '16.0'
          memory: 64G
    environment:
      CUDA_VISIBLE_DEVICES: "0,1"
      MODEL_PATH: "/models"
      BATCH_SIZE: "32"
    volumes:
      - model-storage:/models
      - gpu-cache:/tmp/gpu-cache

volumes:
  model-storage:
    driver: local
    driver_opts:
      type: nfs
      o: addr=nfs.mycompany.com,rw
      device: ":/models"
  gpu-cache:
```


**Features:**

- High CPU and memory allocation (32GB/16CPU)
- GPU support for AI/ML workloads
- NFS storage for large model files
- Node selector for specific hardware
- Resource quotas and limits
- Optimized for compute-intensive tasks

## Common Patterns

### Zero-Downtime Deployment
```yaml
# Deployment strategy for zero downtime
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 50%
      maxUnavailable: 0
  template:
    spec:
      containers:
      - name: app
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 15"]
```


## Deployment Commands

**Deploy to staging:**
```bash
sc deploy -s myapp -e staging
```

**Deploy to production:**
```bash
sc deploy -s myapp -e production
```

## Best Practices

- **Use rolling updates** for zero-downtime deployments
- **Configure proper resource requests and limits** for optimal scheduling
- **Implement comprehensive health checks** (readiness and liveness probes)
- **Use network policies** for security isolation
- **Configure persistent storage** for stateful applications
- **Set up horizontal pod autoscaling** based on metrics
- **Use node selectors** for hardware-specific workloads
- **Implement proper logging and monitoring** with Prometheus and Grafana
- **Use secrets and configmaps** for configuration management
- **Configure RBAC** for proper access control


---

## Advanced Configs

# Advanced Configuration Examples

This section contains examples of advanced Simple Container configurations for complex deployment scenarios.

## Available Examples

### High-Resource AI Development Environment
Deploy a 32GB/16CPU AI development environment with Kubernetes integration.

**Use Case:** Machine learning development, AI model training, high-performance computing

**Configuration:**
```yaml
# .sc/stacks/ai-dev/client.yaml
schemaVersion: 1.0
stacks:
  development:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [gpu-resources, storage-volumes]
      runs: [jupyter-lab, tensorboard, mlflow]
```

**Docker Compose:**
```yaml
# docker-compose.yaml
version: '3.8'
services:
  jupyter-lab:
    build: ./jupyter
    ports:
      - "8888:8888"
    deploy:
      resources:
        requests:
          cpus: '8.0'
          memory: 16G
        limits:
          cpus: '16.0'
          memory: 32G
    environment:
      JUPYTER_ENABLE_LAB: "yes"
      JUPYTER_TOKEN: "${secret:JUPYTER_TOKEN}"
      CUDA_VISIBLE_DEVICES: "all"
    volumes:
      - notebooks:/home/jovyan/work
      - datasets:/data
      - models:/models
      
  tensorboard:
    image: tensorflow/tensorflow:latest-gpu
    ports:
      - "6006:6006"
    command: tensorboard --logdir=/logs --host=0.0.0.0
    volumes:
      - training-logs:/logs
      
  mlflow:
    image: python:3.9
    ports:
      - "5000:5000"
    command: |
      bash -c "pip install mlflow && 
               mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///mlflow.db"
    volumes:
      - mlflow-data:/mlflow

volumes:
  notebooks:
  datasets:
    driver: local
    driver_opts:
      type: nfs
      o: addr=nfs.mycompany.com,rw
      device: ":/datasets"
  models:
  training-logs:
  mlflow-data:
```


**Features:**

- 32GB RAM and 16 CPU cores allocation
- GPU support for AI/ML workloads
- Jupyter Lab for interactive development
- TensorBoard for experiment visualization
- MLflow for experiment tracking
- NFS storage for large datasets
- High-performance node selection

### Multi-Environment Complex Deployment
Deploy applications across multiple environments with different configurations.

**Use Case:** Enterprise applications, complex staging/production setups, A/B testing

**Configuration:**
```yaml
# .sc/stacks/complex-app/client.yaml
schemaVersion: 1.0

# YAML anchors for reusable configurations
x-common-config: &common-config
  type: cloud-compose
  parent: myorg/infrastructure

x-app-env: &app-env
  REDIS_HOST: "${resource:redis-cache.host}"
  REDIS_PORT: "${resource:redis-cache.port}"
  DATABASE_URL: "${resource:postgres-db.url}"
  LOG_LEVEL: info

stacks:
  development:
    <<: *common-config
    parentEnv: development
    config:
      uses: [redis-dev, postgres-dev, s3-dev]
      runs: [api-service, worker-service, admin-ui]
      scale:
        max: 3
        min: 1
        policy:
          cpu:
            max: 70
      env:
        <<: *app-env
        LOG_LEVEL: debug
        FEATURE_FLAGS: "all_enabled"
        
  staging:
    <<: *common-config
    parentEnv: staging
    config:
      uses: [redis-staging, postgres-staging, s3-staging]
      runs: [api-service, worker-service, admin-ui]
      scale:
        max: 8
        min: 2
        policy:
          cpu:
            max: 70
      env:
        <<: *app-env
        FEATURE_FLAGS: "staging_features"
        
  production:
    <<: *common-config
    parentEnv: production
    config:
      uses: [redis-prod, postgres-prod, s3-prod, monitoring]
      runs: [api-service, worker-service, admin-ui]
      scale:
        max: 50
        min: 5
        policy:
          cpu:
            max: 70
      env:
        <<: *app-env
        FEATURE_FLAGS: "production_stable"
      alerts:
        slack:
          webhookUrl: "${secret:SLACK_WEBHOOK_URL}"
        maxMemory:
          threshold: 85
          alertName: production-max-memory
          description: "Production memory usage exceeds 85% ${secret:alerts-bot-slack-cc}"
        maxCPU:
          threshold: 80
          alertName: production-max-cpu
          description: "Production CPU usage exceeds 80% ${secret:alerts-bot-slack-cc}"
```

**Features:**

- Multi-environment deployment (dev/staging/prod)
- YAML anchors for configuration reuse
- Environment-specific scaling policies
- Feature flag management per environment
- Monitoring and alerting integration
- Resource isolation per environment

### Hybrid Cloud Configuration
Deploy applications across multiple cloud providers.

**Use Case:** Multi-cloud strategy, vendor lock-in avoidance, geographic distribution

**Configuration:**
```yaml
# .sc/stacks/hybrid-app/client.yaml
schemaVersion: 1.0
stacks:
  aws-primary:
    type: cloud-compose
    parent: myorg/aws-infrastructure
    config:
      uses: [aws-rds, aws-redis, aws-s3]
      runs: [primary-api, data-processor]
      
  gcp-secondary:
    type: cloud-compose
    parent: myorg/gcp-infrastructure
    config:
      uses: [gcp-sql, gcp-redis, gcp-storage]
      runs: [secondary-api, backup-processor]
      
  azure-backup:
    type: cloud-compose
    parent: myorg/azure-infrastructure
    config:
      uses: [azure-sql, azure-storage]
      runs: [backup-api]
```

**Features:**
- Multi-cloud deployment (AWS, GCP, Azure)
- Geographic distribution
- Cloud-specific resource utilization
- Disaster recovery across providers
- Vendor lock-in avoidance

## Common Advanced Patterns



## Deployment Commands

**Deploy development environment:**
```bash
sc deploy -s myapp -e development
```

**Deploy staging environment:**
```bash
sc deploy -s myapp -e staging
```

**Deploy production environment:**
```bash
sc deploy -s myapp -e production
```

## Best Practices

- **Use YAML anchors** to reduce configuration duplication
- **Implement environment-specific scaling** policies
- **Configure proper resource limits** for high-resource workloads
- **Use node selectors** for hardware-specific requirements
- **Implement comprehensive monitoring** and alerting
- **Plan for disaster recovery** across multiple environments
- **Use feature flags** for controlled feature rollouts
- **Implement proper secret management** across environments
- **Configure network policies** for security isolation
- **Use persistent storage** for stateful applications


---

## Parent Stacks

# Parent Stack Examples

This section contains examples of parent stack configurations that provide shared infrastructure for client applications.

## Available Examples

### AWS Multi-Region Parent Stack
Complete parent stack with Cloudflare DNS configuration and multi-region AWS setup.

**Use Case:** Global applications, disaster recovery, multi-region deployment

**Configuration:**
```yaml
# .sc/stacks/devops/server.yaml
schemaVersion: 1.0

templates:
  ecs-fargate-us:
    type: ecs-fargate
    config: &aws-us-config
      credentials: "${auth:aws-us}"
      account: "${auth:aws-us.projectId}"
      region: us-east-1
      
  ecs-fargate-eu:
    type: ecs-fargate
    config: &aws-eu-config
      credentials: "${auth:aws-eu}"
      account: "${auth:aws-eu.projectId}"
      region: eu-west-1

resources:
  resources:
    production:
      # US East resources
      s3-storage-us:
        type: aws-s3-bucket
        config:
          <<: *aws-us-config
          name: "myapp-prod-storage-us"
          allowOnlyHttps: true
          corsConfig:
            allowedOrigins: ["https://myapp.com"]
            allowedMethods: ["GET", "POST", "PUT"]
            
      rds-postgres-us:
        type: aws-rds-postgres
        config:
          <<: *aws-us-config
          name: "myapp-prod-db-us"
          instanceClass: "db.r5.xlarge"
          allocateStorage: 100
          engineVersion: "14.9"
          username: "appuser"
          password: "${secret:DB_PASSWORD_US}"
          databaseName: "myapp"
          
      # EU West resources
      s3-storage-eu:
        type: aws-s3-bucket
        config:
          <<: *aws-eu-config
          name: "myapp-prod-storage-eu"
          allowOnlyHttps: true
          corsConfig:
            allowedOrigins: ["https://myapp.com"]
            allowedMethods: ["GET", "POST", "PUT"]
            
      rds-postgres-eu:
        type: aws-rds-postgres
        config:
          <<: *aws-eu-config
          name: "myapp-prod-db-eu"
          instanceClass: "db.r5.xlarge"
          allocateStorage: 100
          engineVersion: "14.9"
          username: "appuser"
          password: "${secret:DB_PASSWORD_EU}"
          databaseName: "myapp"
          
      # MongoDB Atlas with multi-region
      mongodb-atlas:
        type: mongodb-atlas
        config:
          publicKey: "${secret:MONGODB_ATLAS_PUBLIC_KEY}"
          privateKey: "${secret:MONGODB_ATLAS_PRIVATE_KEY}"
          orgId: "${secret:MONGODB_ATLAS_ORG_ID}"
          projectId: "${secret:MONGODB_ATLAS_PROJECT_ID}"
          instanceSize: M30
          region: US_EAST_1
          cloudProvider: AWS
          admins: ["admin@mycompany.com"]
          developers: ["dev@mycompany.com"]
          backup:
            every: 6h
            retention: 168h
          networkConfig:
            allowCidrs: ["10.0.0.0/8"]
            privateLinkEndpoint: true
          extraProviders:
            - name: AWS
              credentials: "${auth:aws-us}"
              
registrar:
  cloudflare:
    type: cloudflare-registrar
    config:
      credentials: "${secret:CLOUDFLARE_API_TOKEN}"
      accountId: "${secret:CLOUDFLARE_ACCOUNT_ID}"
      zoneName: "myapp.com"
      dnsRecords:
        - name: "@"
          type: A
          value: "203.0.113.10"
          proxied: true
        - name: "api"
          type: CNAME
          value: "us-east-1.elb.amazonaws.com"
          proxied: true
        - name: "api-eu"
          type: CNAME
          value: "eu-west-1.elb.amazonaws.com"
          proxied: true
```

**Features:**
- Multi-region AWS deployment (US East, EU West)
- Cloudflare DNS management with proxying
- MongoDB Atlas with cross-region networking
- S3 buckets with CORS configuration
- RDS PostgreSQL in multiple regions
- Private link endpoints for security
- Comprehensive backup strategies

### GCP Comprehensive Parent Stack
Complete GCP setup with GKE, databases, and Cloudflare domain management.

**Use Case:** GCP-native applications, Kubernetes workloads, global CDN

**Configuration:**
```yaml
# .sc/stacks/devops/server.yaml
schemaVersion: 1.0

templates:
  gke-autopilot:
    type: gcp-gke-autopilot
    gkeClusterResource: "main-gke-cluster"
    artifactRegistryResource: "main-registry"

resources:
  resources:
    production:
      main-gke-cluster:
        type: gcp-gke-autopilot-cluster
        config:
          projectId: "${auth:gcloud.projectId}"
          credentials: "${auth:gcloud}"
          location: europe-west3
          gkeMinVersion: "1.33.4-gke.1245000"  # Check: gcloud container get-server-config --location=europe-west3
          
      main-registry:
        type: gcp-artifact-registry
        config:
          projectId: "${auth:gcloud.projectId}"
          credentials: "${auth:gcloud}"
          location: europe-west3
          docker:
            immutableTags: true
            
      postgres-db:
        type: gcp-cloudsql-postgres
        config:
          projectId: "${auth:gcloud.projectId}"
          credentials: "${auth:gcloud}"
          region: europe-west3
          tier: db-standard-2
          diskSize: 100
          databaseVersion: POSTGRES_14
          
      redis-cache:
        type: gcp-redis
        config:
          projectId: "${auth:gcloud.projectId}"
          credentials: "${auth:gcloud}"
          location: europe-west3
          tier: standard
          memorySizeGb: 4
          
      storage-bucket:
        type: gcp-bucket
        config:
          projectId: "${auth:gcloud.projectId}"
          credentials: "${auth:gcloud}"
          name: "myapp-prod-storage"
          location: europe-west3
          
      pubsub-messaging:
        type: gcp-pubsub
        config:
          projectId: "${auth:gcloud.projectId}"
          credentials: "${auth:gcloud}"
          topics:
            - name: "events"
              labels:
                environment: "production"
          subscriptions:
            - name: "event-processor"
              topic: "events"
              ackDeadlineSec: 60
              exactlyOnceDelivery: true

registrar:
  cloudflare:
    type: cloudflare-registrar
    config:
      credentials: "${secret:CLOUDFLARE_API_TOKEN}"
      accountId: "${secret:CLOUDFLARE_ACCOUNT_ID}"
      zoneName: "myapp.com"
      dnsRecords:
        - name: "@"
          type: A
          value: "203.0.113.20"
          proxied: true
        - name: "api"
          type: CNAME
          value: "gcp-lb.myapp.com"
          proxied: true
```

**Features:**
- GKE Autopilot for managed Kubernetes
- Artifact Registry for container images
- Cloud SQL PostgreSQL database
- Redis for caching and sessions
- Cloud Storage for file storage
- Pub/Sub for event-driven architecture
- Cloudflare integration for global CDN

### Hybrid Cloud Parent Stack
Multi-cloud parent stack configuration with AWS and GCP resources.

**Use Case:** Multi-cloud strategy, vendor diversification, best-of-breed services

**Configuration:**
```yaml
# .sc/stacks/devops/server.yaml
schemaVersion: 1.0

templates:
  aws-compute:
    type: ecs-fargate
    config:
      credentials: "${auth:aws}"
      account: "${auth:aws.projectId}"
      region: us-east-1
      
  gcp-data:
    type: gcp-gke-autopilot
    gkeClusterResource: "data-cluster"
    artifactRegistryResource: "data-registry"

resources:
  resources:
    production:
      # AWS for compute workloads (using S3 for storage instead of ECS cluster)
      s3-compute-storage:
        type: s3-bucket
        config:
          credentials: "${auth:aws}"
          account: "${auth:aws.projectId}"
          region: us-east-1
          name: "myapp-compute-storage"
          allowOnlyHttps: true
          
      s3-storage:
        type: s3-bucket
        config:
          credentials: "${auth:aws}"
          account: "${auth:aws.projectId}"
          region: us-east-1
          name: "myapp-hybrid-storage"
          allowOnlyHttps: true
          
      # GCP for data services
      data-cluster:
        type: gcp-gke-autopilot-cluster
        config:
          projectId: "${auth:gcloud.projectId}"
          credentials: "${auth:gcloud}"
          location: us-central1
          gkeMinVersion: "1.33.4-gke.1245000"  # Check: gcloud container get-server-config --location=us-central1
          
      data-registry:
        type: gcp-artifact-registry
        config:
          projectId: "${auth:gcloud.projectId}"
          credentials: "${auth:gcloud}"
          location: us-central1
          
      analytics-bucket:
        type: gcp-bucket
        config:
          projectId: "${auth:gcloud.projectId}"
          credentials: "${auth:gcloud}"
          name: "myapp-analytics-data"
          location: US
```

**Features:**
- AWS for compute-intensive workloads
- GCP for data analytics and processing
- Cross-cloud resource sharing
- Unified DNS management
- Cost optimization through best-of-breed services

## Common Parent Stack Patterns

### Environment Separation

**Client Configuration (client.yaml) - Uses `stacks:` section:**
```yaml
# client.yaml
schemaVersion: 1.0
stacks:
  development:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      min: 1
      max: 3
      uses: [shared-database, shared-storage]
      # Other stack configuration properties...
  staging:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      min: 2
      max: 10
      uses: [shared-database, shared-storage]
      # Other stack configuration properties...
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      min: 5
      max: 100
      uses: [shared-database, shared-storage]
      # Other stack configuration properties...
```

**Server Configuration (server.yaml) - Uses `resources:` section:**
```yaml
# server.yaml
schemaVersion: 1.0
resources:
  resources:
    production:
      shared-database:
        type: aws-rds-postgres
        config:
          credentials: "${auth:aws}"
          account: "${auth:aws.projectId}"
          region: us-east-1
          name: "shared-prod-db"
          instanceClass: "db.r5.xlarge"
          allocateStorage: 100
          engineVersion: "14.9"
          username: "appuser"
          password: "${secret:DB_PASSWORD}"
          databaseName: "shared"
      shared-storage:
        type: s3-bucket
        config:
          credentials: "${auth:aws}"
          account: "${auth:aws.projectId}"
          region: us-east-1
          name: "shared-prod-storage"
          allowOnlyHttps: true
```

### Shared Resource Configuration
```yaml
resources:
  resources:
    production:
      shared-database:
        type: aws-rds-postgres
        config:
          credentials: "${auth:aws}"
          account: "${auth:aws.projectId}"
          region: us-east-1
          name: "shared-prod-db"
          instanceClass: "db.r5.2xlarge"
          allocateStorage: 100
          engineVersion: "14.9"
          username: "appuser"
          password: "${secret:DB_PASSWORD}"
          databaseName: "shared"
      shared-cache:
        type: gcp-redis
        config:
          projectId: "${auth:gcloud.projectId}"
          credentials: "${auth:gcloud}"
          location: us-central1
          tier: standard
          memorySizeGb: 4
```

## Deployment Commands

**Provision parent stack:**
```bash
sc provision -s devops
```

**Update parent stack:**
```bash
sc provision -s devops --update
```

## Best Practices

- **Use environment separation** for development, staging, and production
- **Implement proper backup strategies** for all data stores
- **Configure monitoring and alerting** for all shared resources
- **Use least-privilege IAM policies** for all service accounts
- **Implement network security** with proper VPC and firewall rules
- **Use infrastructure as code** for all resource definitions
- **Document resource dependencies** and relationships
- **Plan for disaster recovery** across regions or clouds
- **Monitor costs** and implement cost optimization strategies
- **Use consistent naming conventions** across all resources


---

# Reference

## Supported Resources

# **Supported Resources Reference**

This document provides a comprehensive reference of all supported cloud resources and their properties that can be defined in the **parent stack**. The parent stack is managed by DevOps teams and provides the core infrastructure that microservices consume.

## **Understanding Simple Container Architecture**

Simple Container uses a **separation of concerns** architecture where:

- **Parent Stack** (`server.yaml`) - DevOps-managed infrastructure and deployment templates
- **Client Stack** (`client.yaml`) - Developer-managed service configurations that consume parent resources

### **Configuration Types and Their Purpose**

- **TemplateType** ‚Üí `templates` section in `server.yaml` - **Deployment patterns** (HOW to deploy services)
- **ResourceType** ‚Üí `resources` section in `server.yaml` - **Shared infrastructure** (provisioned with `sc provision`)
- **AuthType** ‚Üí `auth` section in `secrets.yaml` - **Authentication providers**
- **SecretsType** ‚Üí `secrets` section in `secrets.yaml` - **Secret management**
- **RegistrarType** ‚Üí `registrar` section in `server.yaml` - **Domain registration**
- **StateStorageType** ‚Üí `provisioner.stateStorage` section in `server.yaml` - **Terraform state storage**
- **SecretsProviderType** ‚Üí `provisioner.secretsProvider` section in `server.yaml` - **Secret encryption**

### **How Client Stacks Consume Parent Resources**

Client stacks (`client.yaml`) consume parent stack resources using:

- **`parent`** directive - Specifies which parent stack to use
- **Environment matching** - By default, `stacks.staging` consumes `resources.resources.staging` from parent
- **`parentEnv`** directive - Allows custom stack names to consume specific parent environments
- **`uses`** directive - Specifies which resources from parent environment to consume

```yaml
# client.yaml examples
stacks:
  # Environment matching: 'staging' stack consumes 'resources.resources.staging' from parent
  staging:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [mongodb-shared-us, s3-storage]  # Consume parent resources
      domain: staging.myapp.com
  
  # Custom stack name with parentEnv directive
  customer-a:
    type: cloud-compose
    parent: myorg/infrastructure
    parentEnv: production  # Consumes resources.resources.production from parent
    config:
      uses: [mongodb-shared-us, s3-storage]
      domain: customer-a.myapp.com
```

## **Supported Cloud Providers**

- **AWS** - Amazon Web Services
- **Google Cloud Platform (GCP)** - Google Cloud
- **Kubernetes** - Kubernetes-native resources
- **MongoDB Atlas** - MongoDB Atlas database clusters
- **Cloudflare** - Domain registration and DNS
- **File System** - Local development resources

---

## **AWS Provider**

### **Templates** (`TemplateType` ‚Üí `templates` section in `server.yaml`)

Templates define **deployment patterns** - HOW services are deployed. Client stacks reference these templates to deploy their services.

#### **ECS Fargate** (`ecs-fargate`)

Deployment template for containerized applications on AWS ECS using Fargate.

**Golang Struct Reference:** `pkg/clouds/aws/ecs_fargate.go:EcsFargateConfig`

```yaml
# server.yaml - Parent Stack (DevOps managed)
templates:
  stack-per-app-us:
    type: ecs-fargate
    config: &aws-us-cfg
      credentials: "${auth:aws-us}"
      account: "${auth:aws-us.projectId}"
  
  stack-per-app-eu:
    type: ecs-fargate
    config: &aws-eu-cfg
      credentials: "${auth:aws-eu}"
      account: "${auth:aws-eu.projectId}"

resources:
  resources:
    production:
      template: stack-per-app-us
    staging:
      template: stack-per-app-eu
```

**How Client Stacks Use This Template:**
```yaml
# client.yaml - Client Stack (Developer managed)
stacks:
  production:  # Matches resources.resources.production from parent
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [mongodb-shared, s3-storage]  # Consumes parent resources
      domain: customer-a.myapp.com
      runs: [web-app]
  
  # Or with custom stack name using parentEnv
  customer-a:
    type: cloud-compose
    parent: myorg/infrastructure
    parentEnv: production  # Consumes resources.resources.production from parent
    config:
      uses: [mongodb-shared, s3-storage]
      domain: customer-a.myapp.com
      runs: [web-app]
```

#### **AWS Lambda** (`aws-lambda`)

Deployment template for serverless functions on AWS Lambda.

**Golang Struct Reference:** `pkg/clouds/aws/aws_lambda.go:LambdaInput`

```yaml
# server.yaml - Parent Stack (DevOps managed)
templates:
  lambda-us:
    type: aws-lambda
    config: &aws-us-cfg
      credentials: "${auth:aws-us}"
      account: "${auth:aws-us.projectId}"
  
  lambda-eu:
    type: aws-lambda
    config: &aws-eu-cfg
      credentials: "${auth:aws-eu}"
      account: "${auth:aws-eu.projectId}"

resources:
  resources:
    production:
      template: lambda-us
    staging:
      template: lambda-eu
```

**See Also:**

- [Lambda Functions Examples](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/lambda-functions/) - Complete Lambda configurations with Dockerfile and advanced patterns
- [AI Gateway Example](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/lambda-functions/ai-gateway/) - AWS Bedrock integration with specific IAM roles
- [Storage Service Example](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/lambda-functions/storage-service/) - Scheduled cleanup with cron expressions
- [Scheduler Example](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/lambda-functions/scheduler/) - High-frequency scheduling (every minute)
- [Cost Analytics Example](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/lambda-functions/cost-analytics/) - AWS cost analysis with comprehensive IAM permissions
- [Billing System Example](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/lambda-functions/billing-system/) - Multi-environment with YAML anchors

#### **Static Website** (`aws-static-website`)

Deployment template for static websites on AWS S3 with CloudFront.

**Golang Struct Reference:** `pkg/clouds/aws/static_website.go:StaticSiteInput`

```yaml
# server.yaml - Parent Stack (DevOps managed)
templates:
  static-us:
    type: aws-static-website
    config: &aws-us-cfg
      credentials: "${auth:aws-us}"
      account: "${auth:aws-us.projectId}"
  
  static-eu:
    type: aws-static-website
    config: &aws-eu-cfg
      credentials: "${auth:aws-eu}"
      account: "${auth:aws-eu.projectId}"

resources:
  resources:
    production:
      template: static-us
    staging:
      template: static-eu
```

**See Also:**

- [Static Websites Examples](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/static-websites/) - Complete static website configurations
- [Documentation Site Example](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/static-websites/documentation-site/) - MkDocs documentation deployment
- [Landing Page Example](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/static-websites/landing-page/) - Main website with SPA configuration
- [Admin Dashboard Example](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/static-websites/admin-dashboard/) - Admin UI with multi-environment setup
- [Customer Portal Example](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/static-websites/customer-portal/) - Customer-facing UI deployment
- [Media Store Example](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/static-websites/media-store/) - Media-specific static hosting

### **Resources** (`ResourceType` ‚Üí `resources` section in `server.yaml`)

#### **S3 Bucket** (`s3-bucket`)

Creates and manages AWS S3 buckets.

**Golang Struct Reference:** `pkg/clouds/aws/bucket.go:S3Bucket`

**JSON Schema:** [S3Bucket Schema](https://github.com/simple-container-com/api/tree/main/docs/schemas/aws/s3bucket.json)

```yaml
# server.yaml - Parent Stack
resources:
  resources:
    production:
      resources:
        my-s3-bucket:
          type: s3-bucket
          config:
            # AWS account configuration (inherited from AccountConfig)
            credentials: "${auth:aws-us}"
            account: "${auth:aws-us.projectId}"
            
            # S3 bucket specific properties (from S3Bucket struct)
            name: "my-application-storage"        # Bucket name
            allowOnlyHttps: true                  # Force HTTPS-only access
```

**Client Access:**

When this resource is used in a client stack via the `uses` section, Simple Container automatically injects environment variables and template placeholders for S3 bucket access.

üìñ **For complete details on environment variables and template placeholders, see:** [Template Placeholders Advanced - AWS S3 Bucket](../concepts/template-placeholders-advanced.md#s3-bucket)

#### **ECR Repository** (`ecr-repository`)

Creates and manages AWS Elastic Container Registry repositories.

**Golang Struct Reference:** `pkg/clouds/aws/ecr_repository.go:EcrRepository`

**JSON Schema:** [EcrRepository Schema](https://github.com/simple-container-com/api/tree/main/docs/schemas/aws/ecrrepository.json)

```yaml
# server.yaml - Parent Stack
resources:
  resources:
    production:
      resources:
        my-ecr-repo:
          type: ecr-repository
          config:
            # AWS account configuration (inherited from AccountConfig)
            credentials: "${auth:aws-us}"
            account: "${auth:aws-us.projectId}"
            
            # ECR repository specific properties (from EcrRepository struct)
            name: "my-app"                        # Repository name
            lifecyclePolicy:                      # Image lifecycle management
              rules:
                - rulePriority: 1
                  description: "Keep only 3 last images"
                  selection:
                    tagStatus: "any"              # any, tagged, untagged
                    countType: "imageCountMoreThan"
                    countNumber: 3
                  action:
                    type: "expire"                # Action to take when rule matches
```

#### **RDS PostgreSQL** (`aws-rds-postgres`)

Creates and manages AWS RDS PostgreSQL databases.

**Golang Struct Reference:** `pkg/clouds/aws/rds_postgres.go:PostgresConfig`

```yaml
# server.yaml - Parent Stack
resources:
  resources:
    production:
      resources:
        my-postgres-db:
          type: aws-rds-postgres
          config:
            # AWS account configuration (inherited from AccountConfig)
            credentials: "${auth:aws-us}"
            account: "${auth:aws-us.projectId}"
            
            # PostgreSQL specific properties (from PostgresConfig struct)
            name: "my-postgres-db"               # Database instance identifier
            instanceClass: "db.t3.micro"        # Instance size
            allocateStorage: 20                  # Storage size in GB
            engineVersion: "14.9"               # PostgreSQL version
            username: "postgres"                # Master username
            password: "${env:DB_PASSWORD}"      # Master password
            databaseName: "myapp"               # Initial database name
```

**Client Access:**

When this resource is used in a client stack via the `uses` section, Simple Container automatically injects environment variables and template placeholders for PostgreSQL database connection.

üìñ **For complete details on environment variables and template placeholders, see:** [Template Placeholders Advanced - AWS RDS PostgreSQL](../concepts/template-placeholders-advanced.md#rds-postgresql)

#### **RDS MySQL** (`aws-rds-mysql`)

Creates and manages AWS RDS MySQL databases.

**Golang Struct Reference:** `pkg/clouds/aws/rds_mysql.go:MysqlConfig`

```yaml
# server.yaml - Parent Stack
resources:
  resources:
    production:
      resources:
        my-mysql-db:
          type: aws-rds-mysql
          config:
            # AWS account configuration (inherited from AccountConfig)
            credentials: "${auth:aws-us}"
            account: "${auth:aws-us.projectId}"
            
            # MySQL specific properties (from MysqlConfig struct)
            name: "my-mysql-db"                  # Database instance identifier
            instanceClass: "db.t3.micro"        # Instance size
            allocateStorage: 20                  # Storage size in GB
            engineVersion: "8.0"                # MySQL version
            username: "admin"                   # Master username
            password: "${env:DB_PASSWORD}"      # Master password
            databaseName: "myapp"               # Initial database name
            engineName: "mysql"                 # Engine name (optional)
```

**Client Access:**

When this resource is used in a client stack via the `uses` section, Simple Container automatically injects environment variables and template placeholders for MySQL database connection.

üìñ **For complete details on environment variables and template placeholders, see:** [Template Placeholders Advanced - AWS RDS MySQL](../concepts/template-placeholders-advanced.md#rds-mysql)

### **Authentication** (`AuthType` ‚Üí `auth` section in `secrets.yaml`)

#### **AWS Token Authentication** (`aws-token`)

Configures AWS authentication using access tokens.

**Golang Struct Reference:** `pkg/clouds/aws/auth.go:AccountConfig`

```yaml
# secrets.yaml (managed with: sc secrets add .sc/stacks/<parent>/secrets.yaml)
schemaVersion: 1.0
auth:
  aws-account:
    type: aws-token
    config:
      # AWS account configuration properties (from AccountConfig struct)
      account: "123456789012"                    # AWS account ID
      accessKey: "AKIAIOSFODNN7EXAMPLE"          # Exact literal value - NO placeholders
      secretAccessKey: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"  # Exact literal value
      region: us-east-1                          # AWS region

values:
  # Exact literal values - NO placeholders processed in secrets.yaml
  DATABASE_URL: "postgresql://user:pass@host:5432/db"
  API_KEY: "your-secret-api-key-here"
```

### **Secrets Management** (`SecretsType` ‚Üí `secrets` section in `secrets.yaml`)

#### **AWS Secrets Manager** (`aws-secrets-manager`)

Manages secrets using AWS Secrets Manager.

```yaml
# secrets.yaml (managed with: sc secrets add .sc/stacks/<parent>/secrets.yaml)
schemaVersion: 1.0
auth:
  aws-account:
    type: aws-token
    config:
      account: "123456789012"
      accessKey: "AKIAIOSFODNN7EXAMPLE"
      secretAccessKey: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
      region: us-east-1

values:
  # Exact literal values - NO placeholders processed in secrets.yaml
  DATABASE_PASSWORD: "mySecurePassword123"
  API_KEY: "sk-1234567890abcdef"
  CLOUDFLARE_API_TOKEN: "gEYRal5hQm4XJWE5WROP6DAEsdb3NxOgQUcpKjzB"
```

### **Provisioner Configuration** (goes to `provisioner` section in `server.yaml`)

#### **S3 State Storage** (`s3-bucket`)

Stores Terraform state in AWS S3.

```yaml
# server.yaml
stacks:
  production:
    provisioner:
      stateStorage:
        type: s3-bucket
        config:
          region: us-east-1
          accountId: "123456789012"
          name: "myapp-terraform-state"
```

#### **AWS KMS Secrets Provider** (`aws-kms`)

Encrypts secrets using AWS KMS.

```yaml
# server.yaml
stacks:
  production:
    provisioner:
      secretsProvider:
        type: aws-kms
        config:
          region: us-east-1
          keyId: "arn:aws:kms:us-east-1:123456789012:key/12345678-1234-1234-1234-123456789012"
```

---

## **Google Cloud Platform (GCP) Provider**

### **Templates** (`TemplateType` ‚Üí `templates` section in `server.yaml`)

#### **Cloud Run** (`cloudrun`)

Deploys containerized applications on Google Cloud Run.

**Golang Struct Reference:** `pkg/clouds/gcloud/cloudrun.go:CloudRunInput`

**JSON Schema:** [CloudRunInput Schema](https://github.com/simple-container-com/api/tree/main/docs/schemas/gcp/cloudruninput.json)

```yaml
# server.yaml - Parent Stack (DevOps managed)
templates:
  my-cloudrun-template:
    type: cloudrun
    config:
      # GCP credentials and project (from TemplateConfig struct)
      projectId: "${auth:gcp-main.projectId}"
      credentials: "${auth:gcp-main}"

resources:
  resources:
    production:
      template: my-cloudrun-template
```

**How Client Stacks Use This Template:**
```yaml
# client.yaml - Client Stack (Developer managed)
stacks:
  production:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      uses: [shared-database]  # Consumes parent resources
      domain: myapp.example.com
      runs: [web-app]
```

**See Also:**

- [ECS Deployments Examples](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/ecs-deployments/) - Complete ECS deployment configurations with docker-compose.yaml and Dockerfile
- [Backend Service Example](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/ecs-deployments/backend-service/) - Node.js backend with MongoDB integration
- [Blockchain Service Example](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/ecs-deployments/blockchain-service/) - Blockchain integration with cross-service dependencies

#### **GKE Autopilot** (`gcp-gke-autopilot`)

Template for deploying applications to GKE Autopilot clusters. References GKE cluster and Artifact Registry resources.

**Golang Struct Reference:** `pkg/clouds/gcloud/gke_autopilot.go:GkeAutopilotTemplate`

```yaml
# server.yaml - Parent Stack (DevOps managed)
templates:
  stack-per-app-gke:
    type: gcp-gke-autopilot
    config:
      projectId: "${auth:gcloud.projectId}"
      credentials: "${auth:gcloud}"
      gkeClusterResource: gke-autopilot-res        # References the GKE cluster resource
      artifactRegistryResource: artifact-registry-res  # References the artifact registry resource

resources:
  resources:
    production:
      template: stack-per-app-gke
      resources:
        gke-autopilot-res:
          type: gcp-gke-autopilot-cluster
          config:
            gkeMinVersion: 1.33.4-gke.1245000  # Check: gcloud container get-server-config --location=europe-west3
            projectId: "${auth:gcloud.projectId}"
            credentials: "${auth:gcloud}"
            location: europe-west3
            caddy:
              enable: true
              namespace: caddy
              replicas: 2
        artifact-registry-res:
          type: gcp-artifact-registry
          config:
            projectId: "${auth:gcloud.projectId}"
            credentials: "${auth:gcloud}"
            location: europe-west3
            docker:
              immutableTags: true
```

**See Also:**

- [GKE Autopilot Comprehensive Setup](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/gke-autopilot/comprehensive-setup/) - Complete GCP setup with GKE, Artifact Registry, Pub/Sub, Redis, MongoDB Atlas
- [Parent Stacks Examples](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/parent-stacks/) - Multi-region and hybrid cloud parent stack configurations

#### **Static Website** (`gcp-static-website`)

Hosts static websites on Google Cloud Storage with Cloud CDN.

**Golang Struct Reference:** `pkg/clouds/gcloud/static_website.go:StaticSiteInput`

```yaml
# server.yaml
stacks:
  production:
    templates:
      my-gcp-static-site-template:
        type: gcp-static-website
        config:
          projectId: "my-gcp-project"
          name: "my-static-website"
```

### **Resources** (`ResourceType` ‚Üí `resources` section in `server.yaml`)

#### **GKE Autopilot Cluster** (`gcp-gke-autopilot-cluster`)

Creates and manages Google Kubernetes Engine Autopilot clusters as a resource.

**Golang Struct Reference:** `pkg/clouds/gcloud/gke_autopilot.go:GkeAutopilotResource`

**JSON Schema:** [GkeAutopilotResource Schema](https://github.com/simple-container-com/api/tree/main/docs/schemas/gcp/gkeautopilotresource.json)

```yaml
# server.yaml - Resource Definition
resources:
  resources:
    production:
      resources:
        my-gke-cluster:
          type: gcp-gke-autopilot-cluster
          config:
            projectId: "${auth:gcloud.projectId}"
            credentials: "${auth:gcloud}"
            location: "europe-west3"
            zone: "europe-west3-a"                       # GKE zone (required)
            gkeMinVersion: "1.33.4-gke.1245000"  # Check: gcloud container get-server-config --location=europe-west3
            caddy:
              enable: true
              namespace: caddy
              replicas: 2
              # VPA Configuration for Caddy ingress controller
              vpa:
                enabled: true
                updateMode: "Auto"  # Off, Initial, Auto, InPlaceOrRecreate
                minAllowed:
                  cpu: "50m"
                  memory: "64Mi"
                maxAllowed:
                  cpu: "1"
                  memory: "1Gi"
                controlledResources: ["cpu", "memory"]
              # Optional: Manual resource limits alongside VPA
              resources:
                limits:
                  cpu: "500m"
                  memory: "512Mi"
                requests:
                  cpu: "100m"
                  memory: "128Mi"
        my-artifact-registry:
          type: gcp-artifact-registry
          config:
            projectId: "${auth:gcloud.projectId}"
            credentials: "${auth:gcloud}"
            location: "europe-west3"
            docker:
              immutableTags: false
```

#### **GKE Autopilot Template** (`gcp-gke-autopilot`)

Template for deploying applications to GKE Autopilot clusters. References GKE cluster and Artifact Registry resources.

**Golang Struct Reference:** `pkg/clouds/gcloud/gke_autopilot.go:GkeAutopilotTemplate`

```yaml
# server.yaml - Template Definition
templates:
  gke-app-template:
    type: gcp-gke-autopilot
    config:
      projectId: "${auth:gcloud.projectId}"
      credentials: "${auth:gcloud}"
      gkeClusterResource: my-gke-cluster        # References the GKE cluster resource
      artifactRegistryResource: my-artifact-registry  # References the artifact registry resource

resources:
  resources:
    production:
      template: gke-app-template  # Uses the template defined above
      resources:
        # ... resource definitions as shown above
```

**Complete Example:**

```yaml
# server.yaml - Complete GKE Autopilot Setup
schemaVersion: 1.0
provisioner:
  type: pulumi
  config:
    state-storage:
      type: gcp-bucket
      config:
        credentials: "${auth:gcloud}"
        projectId: "${auth:gcloud.projectId}"
        provision: true
        name: my-app-state
        location: europe-west3

templates:
  gke-stack:
    type: gcp-gke-autopilot
    config:
      projectId: "${auth:gcloud.projectId}"
      credentials: "${auth:gcloud}"
      gkeClusterResource: gke-autopilot-cluster
      artifactRegistryResource: artifact-registry

resources:
  registrar:
    type: cloudflare
    config:
      credentials: "${secret:CLOUDFLARE_API_TOKEN}"
      accountId: "your-account-id"
      zoneName: "example.com"
  resources:
    production:
      template: gke-stack
      resources:
        gke-autopilot-cluster:
          type: gcp-gke-autopilot-cluster
          config:
            projectId: "${auth:gcloud.projectId}"
            credentials: "${auth:gcloud}"
            location: "europe-west3"
            gkeMinVersion: "1.33.4-gke.1245000"  # Check: gcloud container get-server-config --location=europe-west3
            caddy:
              enable: true
              namespace: caddy
              replicas: 2
        artifact-registry:
          type: gcp-artifact-registry
          config:
            projectId: "${auth:gcloud.projectId}"
            credentials: "${auth:gcloud}"
            location: "europe-west3"
            docker:
              immutableTags: false
```

#### **GCP Bucket** (`gcp-bucket`)

Creates and manages Google Cloud Storage buckets with S3-compatible access via HMAC keys.

**Golang Struct Reference:** `pkg/clouds/gcloud/bucket.go:GcpBucket`

**JSON Schema:** [GcpBucket Schema](https://github.com/simple-container-com/api/tree/main/docs/schemas/gcp/gcpbucket.json)

```yaml
# server.yaml - Parent Stack
resources:
  resources:
    production:
      resources:
        my-gcp-bucket:
          type: gcp-bucket
          config:
            # GCP credentials and project (inherited from Credentials)
            projectId: "${auth:gcloud.projectId}"
            credentials: "${auth:gcloud}"
            
            # GCP Bucket specific properties (from GcpBucket struct)
            name: "my-application-storage"           # Bucket name (optional, defaults to resource name)
            location: "US"                           # Bucket location
```

**Client Access:**

When this resource is used in a client stack via the `uses` section, Simple Container automatically injects environment variables and template placeholders for GCS bucket access with S3-compatible HMAC authentication.

üìñ **For complete details on environment variables and template placeholders, see:** [Template Placeholders Advanced - GCP Bucket](../concepts/template-placeholders-advanced.md#gcp-bucket)

#### **Artifact Registry** (`gcp-artifact-registry`)

Creates and manages Google Artifact Registry repositories.

**Golang Struct Reference:** `pkg/clouds/gcloud/artifactregistry.go:ArtifactRegistryConfig`

```yaml
# server.yaml - Parent Stack
resources:
  resources:
    production:
      resources:
        my-artifact-registry:
          type: gcp-artifact-registry
          config:
            # GCP credentials and project (inherited from Credentials)
            projectId: "${auth:gcloud.projectId}"
            credentials: "${auth:gcloud}"
            
            # Artifact Registry specific properties (from ArtifactRegistryConfig struct)
            location: "europe-west3"                 # Registry location
            public: false                            # Whether registry is public (optional)
            docker:                                  # Docker-specific settings (optional)
              immutableTags: false                   # Whether tags are immutable
            domain: "my-domain.com"                  # Custom domain (optional)
            basicAuth:                               # Basic auth configuration (optional)
              username: "registry-user"
              password: "${env:REGISTRY_PASSWORD}"
```

### **Database Resources**

#### **Cloud SQL PostgreSQL** (`gcp-cloudsql-postgres`)

Creates and manages Google Cloud SQL PostgreSQL databases.

**Golang Struct Reference:** `pkg/clouds/gcloud/postgres.go:PostgresGcpCloudsqlConfig`

**JSON Schema:** [PostgresGcpCloudsqlConfig Schema](https://github.com/simple-container-com/api/tree/main/docs/schemas/gcp/postgresgcpcloudsqlconfig.json)

**Configuration Properties:**
```yaml
# server.yaml - Parent Stack
resources:
  resources:
    production:
      resources:
        my-cloudsql-postgres:
          type: gcp-cloudsql-postgres
          config:
            # GCP credentials and project (inherited from Credentials)
            projectId: "${auth:gcloud.projectId}"
            credentials: "${auth:gcloud}"
            
            # PostgreSQL configuration (from PostgresGcpCloudsqlConfig struct)
            project: "my-gcp-project"                    # GCP project ID
            version: "POSTGRES_14"                       # PostgreSQL version
            tier: "db-f1-micro"                         # Instance tier
            region: "us-central1"                       # GCP region
            maxConnections: 100                          # Maximum connections (optional)
            deletionProtection: true                     # Enable deletion protection (optional)
            queryInsightsEnabled: false                  # Enable query insights (optional)
            queryStringLength: 1024                      # Query string length limit (optional)
            usersProvisionRuntime:                       # User provisioning runtime (optional)
              type: "kubernetes"
              resourceName: "postgres-job-runner"
```

**Client Access:**

When this resource is used in a client stack via the `uses` section, Simple Container automatically injects environment variables and template placeholders for PostgreSQL connection details.

üìñ **For complete details on environment variables and template placeholders, see:** [Template Placeholders Advanced - GCP PostgreSQL Cloud SQL](../concepts/template-placeholders-advanced.md#postgresql-cloud-sql)

#### **Redis** (`gcp-redis`)

Creates and manages Google Cloud Memorystore Redis instances.

**Golang Struct Reference:** `pkg/clouds/gcloud/redis.go:RedisConfig`

**JSON Schema:** [RedisConfig Schema](https://github.com/simple-container-com/api/tree/main/docs/schemas/gcp/redisconfig.json)

**Configuration Properties:**
```yaml
# server.yaml - Parent Stack
resources:
  resources:
    production:
      resources:
        my-redis:
          type: gcp-redis
          config:
            # GCP credentials and project (inherited from Credentials)
            projectId: "${auth:gcloud.projectId}"
            credentials: "${auth:gcloud}"
            
            # Redis configuration (from RedisConfig struct)
            project: "my-gcp-project"               # GCP project ID
            version: "6.2"                         # Redis version
            region: "us-central1"                  # GCP region
            memorySizeGb: 1                        # Memory size in GB
            redisConfig:                           # Redis configuration map
              maxmemory-policy: "allkeys-lru"
              timeout: "300"
```

**Client Access:**

When this resource is used in a client stack via the `uses` section, Simple Container automatically injects environment variables and template placeholders for Redis connection details.

üìñ **For complete details on environment variables and template placeholders, see:** [Template Placeholders Advanced - GCP Redis Memorystore](../concepts/template-placeholders-advanced.md#redis-memorystore)

### **Messaging Resources**

#### **Pub/Sub** (`gcp-pubsub`)

Creates and manages Google Cloud Pub/Sub topics and subscriptions.

**Golang Struct Reference:** `pkg/clouds/gcloud/pubsub.go:PubSubConfig`

```yaml
# server.yaml - Parent Stack
resources:
  resources:
    production:
      resources:
        my-pubsub:
          type: gcp-pubsub
          config:
            # GCP credentials and project (inherited from Credentials)
            projectId: "${auth:gcloud.projectId}"
            credentials: "${auth:gcloud}"
            
            # Pub/Sub specific properties (from PubSubConfig struct)
            labels:                                  # Resource labels
              env: production
            topics:                                  # Topics configuration
              - name: "my-topic"
                messageRetentionDuration: "86400s"  # Message retention duration
                labels:
                  type: "application"
            subscriptions:                           # Subscriptions configuration
              - name: "my-subscription"
                topic: "my-topic"
                ackDeadlineSec: 600                  # Acknowledgment deadline in seconds
                exactlyOnceDelivery: true            # Enable exactly-once delivery
                messageRetentionDuration: "86400s"   # Message retention duration
                deadLetterPolicy:                    # Dead letter policy (optional)
                  deadLetterTopic: "projects/my-project/topics/dead-letter"
                  maxDeliveryAttempts: 5
                labels:
                  subscriber: "my-service"
```

### **Authentication & Secrets**

#### **GCP Service Account** (`gcp-service-account`)

Configures GCP authentication using service accounts.

**Configuration Properties:**
```yaml
auth:
  gcp-account:
    type: gcp-service-account
    config:
      projectId: "my-gcp-project"
      serviceAccountKey: "${env:GCP_SERVICE_ACCOUNT_KEY}"
```

#### **GCP Secrets Manager** (`gcp-secrets-manager`)

Manages secrets using Google Secret Manager.

**Configuration Properties:**
```yaml
secrets:
  provider: gcp-secrets-manager
  config:
    projectId: "my-gcp-project"
    secretsPrefix: "myapp-"
```

---

## **Kubernetes Resources**

### **Templates (Compute)**

#### **Kubernetes Cloud Run** (`kubernetes-cloudrun`)

Deploys applications to Kubernetes clusters.

**Golang Struct Reference:** `pkg/clouds/k8s/templates.go:CloudrunTemplate`

```yaml
# server.yaml - Parent Stack
templates:
  k8s-cloudrun-template:
    type: kubernetes-cloudrun
    config:
      # Kubernetes configuration (from KubernetesConfig struct)
      kubeconfig: "${auth:k8s-cluster.kubeconfig}"
      
      # Docker registry credentials (from RegistryCredentials struct)
      registryUrl: "my-registry.com"
      username: "${secret:REGISTRY_USERNAME}"
      password: "${secret:REGISTRY_PASSWORD}"
      
      # CloudrunTemplate specific properties
      caddyResource: "my-caddy-resource"       # Name of the caddy resource in base stack (optional)
      useSSL: true                             # Whether to assume connection must be over HTTPS only (optional, default: true)
```

**See Also:**

- [Kubernetes Native Examples](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/kubernetes-native/streaming-platform/) - Streaming platform with hardcoded IPs, N8N integration, zero-downtime configs
- [Advanced Configs Examples](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/advanced-configs/high-resource/) - High-resource AI development environment with Kubernetes integration

### **Infrastructure Resources**

#### **Caddy Reverse Proxy** (`kubernetes-caddy`)

Deploys Caddy as a reverse proxy and load balancer.

**Golang Struct Reference:** `pkg/clouds/k8s/caddy.go:CaddyConfig`

```yaml
# server.yaml - Parent Stack
resources:
  resources:
    production:
      resources:
        my-caddy:
          type: kubernetes-caddy
          config:
            # Kubernetes configuration (from KubernetesConfig struct)
            kubeconfig: "${auth:k8s-cluster.kubeconfig}"
            
            # CaddyConfig specific properties (from CaddyConfig struct)
            enable: true                             # Enable Caddy deployment (optional)
            namespace: "caddy-system"                # Kubernetes namespace (optional)
            image: "caddy:2.7-alpine"               # Caddy Docker image (optional)
            replicas: 2                             # Number of replicas (optional)
            serviceType: "LoadBalancer"             # Service type (optional, default: LoadBalancer)
            useSSL: true                            # Use SSL by default (optional, default: true)
            usePrefixes: false                      # Use prefixes instead of domains (optional, default: false)
            provisionIngress: false                 # Provision ingress for Caddy (optional, default: false)
            
            # VPA Configuration for automatic resource optimization (optional)
            vpa:
              enabled: true                         # Enable VPA for Caddy (optional)
              updateMode: "Auto"              # VPA update mode: Off, Initial, Auto, InPlaceOrRecreate (optional)
              minAllowed:                           # Minimum allowed resources (optional)
                cpu: "50m"
                memory: "64Mi"
              maxAllowed:                           # Maximum allowed resources (optional)
                cpu: "1"
                memory: "1Gi"
              controlledResources: ["cpu", "memory"] # Resources VPA should control (optional)
            
            # Manual resource limits (optional, can be used alongside VPA)
            resources:
              limits:
                cpu: "500m"
                memory: "512Mi"
              requests:
                cpu: "100m"
                memory: "128Mi"
```

### **Database Operators (Helm Charts)**

#### **PostgreSQL Operator** (`kubernetes-helm-postgres-operator`)

Installs PostgreSQL operator via Helm.

**Golang Struct Reference:** `pkg/clouds/k8s/postgres.go:HelmPostgresOperator`

```yaml
# server.yaml - Parent Stack
resources:
  resources:
    production:
      resources:
        postgres-operator:
          type: kubernetes-helm-postgres-operator
          config:
            # Kubernetes configuration (from KubernetesConfig struct)
            kubeconfig: "${auth:k8s-cluster.kubeconfig}"
            
            # HelmChartConfig properties
            namespace: "postgres-operator"               # Namespace for PostgreSQL instances (optional)
            operatorNamespace: "postgres-operator"       # Namespace for operator itself (optional)
            values:                                      # Helm chart values (optional)
              postgresql:
                image: "postgres:14"
            
            # HelmPostgresOperator specific properties
            volumeSize: "10Gi"                          # Volume size for PostgreSQL instances (optional)
            numberOfInstances: 1                        # Number of PostgreSQL instances (optional)
            version: "14"                               # PostgreSQL version (optional)
            pg_hba:                                     # PostgreSQL HBA entries (optional)
              - "host all all 0.0.0.0/0 md5"
            initSQL: "CREATE DATABASE myapp;"          # Initial SQL to run (optional)
```

#### **MongoDB Operator** (`kubernetes-helm-mongodb-operator`)

Installs MongoDB operator via Helm.

**Golang Struct Reference:** `pkg/clouds/k8s/postgres.go:HelmMongodbOperator`

```yaml
# server.yaml - Parent Stack
resources:
  resources:
    production:
      resources:
        mongodb-operator:
          type: kubernetes-helm-mongodb-operator
          config:
            # Kubernetes configuration (from KubernetesConfig struct)
            kubeconfig: "${auth:k8s-cluster.kubeconfig}"
            
            # HelmChartConfig properties
            namespace: "mongodb-operator"               # Namespace for MongoDB instances (optional)
            operatorNamespace: "mongodb-operator"       # Namespace for operator itself (optional)
            values:                                      # Helm chart values (optional)
              mongodb:
                image: "mongo:6.0"
            
            # HelmMongodbOperator specific properties
            version: "6.0"                              # MongoDB version (optional)
            replicas: 3                                 # Number of MongoDB replicas (optional)
```

#### **RabbitMQ Operator** (`kubernetes-helm-rabbitmq-operator`)

Installs RabbitMQ operator via Helm.

**Golang Struct Reference:** `pkg/clouds/k8s/postgres.go:HelmRabbitmqOperator`

```yaml
# server.yaml - Parent Stack
resources:
  resources:
    production:
      resources:
        rabbitmq-operator:
          type: kubernetes-helm-rabbitmq-operator
          config:
            # Kubernetes configuration (from KubernetesConfig struct)
            kubeconfig: "${auth:k8s-cluster.kubeconfig}"
            
            # HelmChartConfig properties
            namespace: "rabbitmq-operator"              # Namespace for RabbitMQ instances (optional)
            operatorNamespace: "rabbitmq-operator"      # Namespace for operator itself (optional)
            values:                                     # Helm chart values (optional)
              rabbitmq:
                image: "rabbitmq:3.12-management"
            
            # HelmRabbitmqOperator specific properties
            replicas: 3                                # Number of RabbitMQ replicas (optional)
```

#### **Redis Operator** (`kubernetes-helm-redis-operator`)

Installs Redis operator via Helm.

**Golang Struct Reference:** `pkg/clouds/k8s/postgres.go:HelmRedisOperator`

```yaml
# server.yaml - Parent Stack
resources:
  resources:
    production:
      resources:
        redis-operator:
          type: kubernetes-helm-redis-operator
          config:
            # Kubernetes configuration (from KubernetesConfig struct)
            kubeconfig: "${auth:k8s-cluster.kubeconfig}"
            
            # HelmChartConfig properties
            namespace: "redis-operator"                 # Namespace for Redis instances (optional)
            operatorNamespace: "redis-operator"         # Namespace for operator itself (optional)
            values:                                     # Helm chart values (optional)
              redis:
                image: "redis:7.0-alpine"
```

### **Authentication**

#### **Kubeconfig** (`kubeconfig`)

Configures Kubernetes cluster authentication.

**Golang Struct Reference:** `pkg/clouds/k8s/auth.go:KubernetesConfig`

**Configuration Properties:**
```yaml
auth:
  k8s-cluster:
    type: kubeconfig
    config:
      kubeconfig: "${env:KUBECONFIG_CONTENT}"
      context: "my-cluster-context"
      namespace: "default"
```

---

## **MongoDB Atlas Resources**

### **Database Resources**

#### **MongoDB Atlas Cluster** (`mongodb-atlas`)

Creates and manages MongoDB Atlas database clusters.

**Golang Struct Reference:** `pkg/clouds/mongodb/mongodb.go:AtlasConfig`

**JSON Schema:** [AtlasConfig Schema](https://github.com/simple-container-com/api/tree/main/docs/schemas/mongodb/atlasconfig.json)

```yaml
# server.yaml - Parent Stack (production example)
resources:
  resources:
    staging:
      resources:
        mongodb:
          type: mongodb-atlas
          config:
            # Atlas API credentials
            publicKey: "${secret:MONGODB_ATLAS_PUBLIC_KEY}"
            privateKey: "${secret:MONGODB_ATLAS_PRIVATE_KEY}"
            
            # Organization and cluster configuration
            orgId: 67bc72f86e5ef36f7584d7d0              # Atlas organization ID
            projectId: "67bc72f86e5ef36f7584d7d1"          # Atlas project ID (required)
            projectName: "my-staging-project"             # Atlas project name (required)
            instanceSize: "M10"                           # Instance size
            region: "EU_CENTRAL_1"                        # Atlas region
            cloudProvider: AWS                            # Cloud provider
            
            # Access control
            admins: [ "vitaly", "dmitriy" ]              # Admin user emails
            developers: [ ]                               # Developer user emails
            
            # Backup configuration
            backup:
              every: 4h                                   # Backup frequency
              retention: 24h                              # Retention period
    
    production:
      resources:
        mongodb:
          type: mongodb-atlas
          config:
            publicKey: "${secret:MONGODB_ATLAS_PUBLIC_KEY}"
            privateKey: "${secret:MONGODB_ATLAS_PRIVATE_KEY}"
            orgId: 67bc72f86e5ef36f7584d7d0
            projectId: "67bc72f86e5ef36f7584d7d1"          # Atlas project ID (required)
            projectName: "my-production-project"          # Atlas project name (required)
            instanceSize: "M30"                           # Larger instance for production
            region: "EU_CENTRAL_1"
            cloudProvider: AWS
            admins: [ "vitaly", "dmitriy" ]
            developers: [ ]
            backup:
              every: 1h                                   # More frequent backups
              retention: 168h                             # Longer retention (1 week)
```

**Client Access:**

When this resource is used in a client stack via the `uses` section, Simple Container automatically injects environment variables and template placeholders for MongoDB Atlas database connection.

üìñ **For complete details on environment variables and template placeholders, see:** [Template Placeholders Advanced - MongoDB Atlas Cluster](../concepts/template-placeholders-advanced.md#mongodb-atlas-cluster)

**See Also:**

- [ECS Deployments Examples](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/ecs-deployments/) - Services using MongoDB Atlas resources
- [Backend Service Example](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/ecs-deployments/backend-service/) - Node.js backend with MongoDB integration
- [Meteor App Example](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/ecs-deployments/meteor-app/) - Meteor.js application with MongoDB
- [Parent Stacks Examples](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/parent-stacks/aws-multi-region/) - Multi-region setup with MongoDB Atlas

---

## **Cloudflare Resources**

### **DNS & Domain Resources**

#### **Domain Registrar** (`cloudflare`)

Manages domain registration and DNS through Cloudflare. **Special resource type** that goes to `resources.registrar` section.

**Golang Struct Reference:** `pkg/clouds/cloudflare/cloudflare.go:RegistrarConfig`

```yaml
# server.yaml - Parent Stack (production example)
resources:
  registrar:
    type: cloudflare
    config:
      # Cloudflare authentication
      credentials: "${secret:CLOUDFLARE_API_TOKEN}"    # Cloudflare API token
      accountId: 23c5ca78cfb4721d9a603ed695a2623e      # Cloudflare account ID
      zoneName: amagenta.ai                            # DNS zone name
      
      # DNS records configuration - SPF email configuration
      dnsRecords:
        - name: "@"                                    # Root domain SPF record
          type: TXT                                    # TXT record for SPF
          value: v=spf1 include:_spf.google.com ~all   # Google email SPF
          proxied: false                               # SPF records should not be proxied
        - name: "@"                                    # HubSpot email integration
          type: TXT
          value: include:143683367.spf06.hubspotemail.net  # HubSpot SPF
          proxied: false
```

**See Also:**

- [Parent Stacks Examples](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/parent-stacks/aws-multi-region/) - Complete parent stack with Cloudflare DNS configuration
- [GKE Autopilot Setup](https://github.com/simple-container-com/api/tree/main/docs/docs/examples/gke-autopilot/comprehensive-setup/) - GCP setup with Cloudflare domain management

---

## **Provisioner Configuration**

### **State Storage and Secrets Management**

The provisioner manages two key components:
- **State Storage**: Stores Pulumi's state (supports `s3-bucket`, `fs`, `gcp-bucket`, `pulumi-cloud`)
- **Secrets Provider**: Provides encryption for created resources' confidential outputs

### **State Storage Options**

#### **File System State Storage** (`fs`)

Stores Pulumi state locally on the file system for local development.

**Golang Struct Reference:** `pkg/clouds/fs/fs_state.go:FileSystemStateStorage`

```yaml
# server.yaml - Parent Stack
schemaVersion: 1.0
provisioner:
  type: pulumi
  config:
    state-storage:
      type: fs
      config:
        path: file:///${user:homeDir}/.sc/pulumi/state    # Local file system path for Pulumi state
```

### **Secrets Management**

#### **Passphrase Secrets Provider** (`passphrase`)

Encrypts secrets using a passphrase for local development.

**Golang Struct Reference:** `pkg/clouds/fs/fs_state.go:PassphraseSecretsProvider`

```yaml
# server.yaml - Parent Stack
schemaVersion: 1.0
provisioner:
  type: pulumi
  config:
    state-storage:
      type: gcp-bucket
      config:
        credentials: "${auth:gcloud}"
        projectId: "${auth:gcloud.projectId}"
        provision: false
        name: my-sc-state
        location: europe-west3
    secrets-provider:
      type: passphrase
      config:
        passPhrase: pass-phrase              # Passphrase for encrypting secrets
```

---

## **Container Registry Resources**

### **GCP Artifact Registry** (`gcp-artifact-registry`)

Creates and manages Google Cloud Artifact Registry repositories for Docker images.

```yaml
# server.yaml - Parent Stack
resources:
  registrar:
    type: cloudflare
    config:
      credentials: "${secret:CLOUDFLARE_API_TOKEN}"
      accountId: 87152c65fca76d443751a37a91a77c17
      zoneName: mycompany.com
  resources:
    prod:
      template: stack-per-app-gke
      resources: &resources
        company-registry: &registry
          type: gcp-artifact-registry
          config: &registry-cfg
            projectId: "${auth:gcloud.projectId}"      # GCP project ID
            credentials: "${auth:gcloud}"              # GCP authentication
            location: europe-west3                     # Registry location
            docker:                                    # Docker-specific settings
              immutableTags: true                      # Whether tags are immutable
    prod-ru:
      template: static-website
      resources: {}
```

---

## **Alert & Notification Resources**

### **Discord Alerts**

Sends alerts to Discord channels.

**Configuration Properties:**
```yaml
alerts:
  discord:
    webhookUrl: "${env:DISCORD_WEBHOOK_URL}"
    channel: "alerts"
    username: "Simple Container Bot"
```

### **Slack Alerts**

Sends alerts to Slack channels.

**Configuration Properties:**
```yaml
alerts:
  slack:
    webhookUrl: "${env:SLACK_WEBHOOK_URL}"
    channel: "#alerts"
    username: "Simple Container Bot"
```

### **Telegram Alerts**

Sends alerts to Telegram chats.

**Configuration Properties:**
```yaml
alerts:
  telegram:
    botToken: "${env:TELEGRAM_BOT_TOKEN}"
    chatId: "${env:TELEGRAM_CHAT_ID}"
```

### **CloudWatch Monitoring Alerts**

Monitoring alerts for ECS services and Application Load Balancers with CloudWatch integration.

**ECS Service Monitoring:**
```yaml
alerts:
  maxCPU:
    alertName: "High CPU Usage"
    description: "ECS service CPU usage exceeded threshold"
    threshold: 80.0
    periodSec: 300
  maxMemory:
    alertName: "High Memory Usage"
    description: "ECS service memory usage exceeded threshold"
    threshold: 85.0
    periodSec: 300
  maxErrors:
    alertName: "High Error Rate"
    description: "Application error rate exceeded threshold"
    threshold: 10.0
    periodSec: 60
    errorLogMessageRegexp: "ERROR|FATAL|Exception"
```

**Application Load Balancer Monitoring:**
```yaml
alerts:
  serverErrors:
    alertName: "ALB Server Errors"
    description: "5XX error rate exceeded threshold"
    threshold: 10.0
    periodSec: 300
  unhealthyHosts:
    alertName: "ALB Unhealthy Hosts"
    description: "Unhealthy host count exceeded threshold"
    threshold: 1
    periodSec: 300
  responseTime:
    alertName: "ALB High Response Time"
    description: "Response time exceeded threshold"
    threshold: 5.0
    periodSec: 300
```

**Email Notifications:**
```yaml
alerts:
  email:
    addresses:
      - "team-alerts@company.com"
      - "oncall@company.com" 
      - "devops-team@company.com"
  
  # Your alert configurations (serverErrors, unhealthyHosts, etc.)
  serverErrors:
    alertName: "production-server-errors"
    description: "High 5XX error rate detected"
    threshold: 5.0
    periodSec: 300
```

> **üìß Email Integration**: Each notification channel works independently. When email addresses are configured, email notifications are sent alongside any configured webhook channels. Email recipients must confirm their subscription to receive notifications.

**Complete Monitoring Configuration:**
```yaml
alerts:
  # Notification channels
  slack:
    webhookUrl: "${secret:SLACK_WEBHOOK_URL}"
  discord:
    webhookUrl: "${secret:DISCORD_WEBHOOK_URL}"
  email:
    addresses:
      - "alerts@company.com"
      - "devops@company.com"
  
  # ECS monitoring
  maxCPU:
    alertName: "prod-high-cpu"
    description: "Production CPU usage too high"
    threshold: 70.0
    periodSec: 300
  maxMemory:
    alertName: "prod-high-memory"
    description: "Production memory usage too high"
    threshold: 80.0
    periodSec: 300
  
  # Load balancer monitoring
  serverErrors:
    alertName: "prod-server-errors"
    description: "Production server error rate too high"
    threshold: 5.0
    periodSec: 300
  responseTime:
    alertName: "prod-slow-response"
    description: "Production response time too slow"
    threshold: 2.0
    periodSec: 300
```

**Alert Configuration Properties:**
- **`alertName`** - Unique identifier for the CloudWatch alarm
- **`description`** - Human-readable description of the alert
- **`threshold`** - Numeric threshold value that triggers the alarm
- **`periodSec`** - Evaluation period in seconds (60-86400)
- **`errorLogMessageRegexp`** - (maxErrors only) Regex pattern to match error log messages

**CloudWatch Metrics and Dimensions:**
- **`serverErrors`** - Uses `HTTPCode_Target_5XX_Count` metric with LoadBalancer dimension
- **`unhealthyHosts`** - Uses `UnHealthyHostCount` metric with LoadBalancer + TargetGroup dimensions  
- **`responseTime`** - Uses `TargetResponseTime` metric with LoadBalancer dimension
- All metrics use full AWS load balancer identifiers (including AWS-generated suffixes) for reliable targeting

**Notification Channel Properties:**
- **`email.addresses`** - Array of email addresses for email notifications
- **`slack.webhookUrl`** - Slack webhook URL (use secrets management)
- **`discord.webhookUrl`** - Discord webhook URL (use secrets management)
- **`telegram.chatID`** & **`telegram.token`** - Telegram bot configuration

---

## **Complete Example: Multi-Cloud Parent Stack**

Here's a complete example showing how to define resources across multiple cloud providers in a parent stack:

```yaml
# server.yaml - Parent Stack Configuration
schemaVersion: 1.0

# Provisioner configuration
provisioner:
  type: pulumi
  config:
    state-storage:
      type: s3-bucket
      config:
        credentials: "${auth:aws-main}"
        account: "${auth:aws-main.projectId}"
        name: "myapp-terraform-state"
        provision: false
    
    secrets-provider:
      type: aws-kms
      config:
        credentials: "${auth:aws-main}"
        account: "${auth:aws-main.projectId}"
        keyName: "arn:aws:kms:us-east-1:123456789012:key/12345678-1234-1234-1234-123456789012"
        provision: false

# Templates - deployment patterns
templates:
  stack-per-app-us:
    type: ecs-fargate
    config: &aws-us-cfg
      credentials: "${auth:aws-main}"
      account: "${auth:aws-main.projectId}"
  
  gcp-cloudrun:
    type: cloudrun
    config: &gcp-cfg
      credentials: "${auth:gcp-main}"
      projectId: "${auth:gcp-main.projectId}"

# Domain registrar
resources:
  registrar:
    type: cloudflare
    config:
      credentials: "${secret:CLOUDFLARE_API_TOKEN}"
      accountId: 87152c65fca76d443751a37a91a77c17
      zoneName: myapp.com
  
  # Environment-specific resources
  resources:
    production:
      template: stack-per-app-us
      resources:
        # MongoDB Atlas for database
        main-database:
          type: mongodb-atlas
          config:
            publicKey: "${env:MONGODB_ATLAS_PUBLIC_KEY}"
            privateKey: "${env:MONGODB_ATLAS_PRIVATE_KEY}"
            orgId: "60b5d0f0f2a1b2c3d4e5f6g7"
            projectId: "60b5d0f0f2a1b2c3d4e5f6g8"
            instanceSize: "M30"
            region: "US_EAST_1"
            cloudProvider: "AWS"
            admins: ["devops@example.com"]
            developers: ["dev-team@example.com"]
        
        # S3 for file storage
        file-storage:
          type: s3-bucket
          config:
            credentials: "${auth:aws-main}"
            account: "${auth:aws-main.projectId}"
            name: "myapp-files"
            allowOnlyHttps: true
    
    staging:
      template: gcp-cloudrun
      resources:
        # Shared staging database
        staging-database:
          type: mongodb-atlas
          config:
            publicKey: "${env:MONGODB_ATLAS_PUBLIC_KEY}"
            privateKey: "${env:MONGODB_ATLAS_PRIVATE_KEY}"
            orgId: "60b5d0f0f2a1b2c3d4e5f6g7"
            projectId: "60b5d0f0f2a1b2c3d4e5f6g8"
            instanceSize: "M10"
            region: "US_EAST_1"
            cloudProvider: "AWS"
```

---

## **Resource Inheritance and Sharing**

Resources defined in the parent stack can be shared across multiple client stacks:

### **Parent Stack Resource Pool**
```yaml
# server.yaml
resources:
  # Shared database for standard customers
  shared-db:
    type: mongodb-atlas
    config:
      instanceSize: "M30"
      
  # Dedicated database for enterprise customers  
  enterprise-db:
    type: mongodb-atlas
    config:
      instanceSize: "M80"
      dedicatedTenant: true
```

### **Client Stack Resource Selection**
```yaml
# client.yaml
stacks:
  customer-standard:
    parentStack: production
    uses: [shared-db]  # Uses shared resources
    
  customer-enterprise:
    parentStack: production
    uses: [enterprise-db]  # Uses dedicated resources
```

This separation allows DevOps to define resource pools once while giving developers flexibility to choose appropriate resources for their specific needs.

---

## **Best Practices**

### **Resource Naming**
- Use descriptive names that indicate purpose: `user-database`, `file-storage`, `api-cluster`
- Include environment indicators: `prod-database`, `staging-cluster`
- Use consistent naming conventions across your organization

### **Resource Sizing**
- Start with smaller instance sizes and scale up based on actual usage
- Use auto-scaling features where available
- Monitor resource utilization to optimize costs

### **Security**
- Always use encrypted storage and transmission
- Implement proper access controls and authentication
- Use secrets management for sensitive configuration
- Enable audit logging where available

### **High Availability**
- Use multi-AZ deployments for critical resources
- Configure appropriate backup and retention policies
- Implement health checks and monitoring
- Plan for disaster recovery scenarios

### **Cost Optimization**
- Use shared resources for development and testing environments
- Implement resource tagging for cost tracking
- Set up billing alerts and budgets
- Regularly review and optimize resource usage

## **Multidimensional Resource Allocation Examples**

Simple Container's architecture enables sophisticated resource allocation patterns where DevOps defines resource pools once, and developers flexibly allocate customers to appropriate resources.

### **Example: SaaS Platform with Multiple Customer Tiers**

**Parent Stack - Resource Pools (DevOps managed):**
```yaml
# server.yaml - Infrastructure managed by DevOps once
schemaVersion: 1.0

# Deployment templates
templates:
  web-app-template:
    type: ecs-fargate
    config: &aws-cfg
      credentials: "${auth:aws-main}"
      account: "${auth:aws-main.projectId}"
  
  api-service-template:
    type: cloudrun
    config: &gcp-cfg
      credentials: "${auth:gcp-main}"
      projectId: "${auth:gcp-main.projectId}"

# Resource pools
resources:
  resources:
    production:
      template: web-app-template
      resources: &shared-resources
        # Shared databases for standard customers
        mongodb-shared-us:
          type: mongodb-atlas
          config:
            publicKey: "${env:MONGODB_ATLAS_PUBLIC_KEY}"
            privateKey: "${env:MONGODB_ATLAS_PRIVATE_KEY}"
            orgId: "60b5d0f0f2a1b2c3d4e5f6g7"
            projectId: "60b5d0f0f2a1b2c3d4e5f6g8"
            instanceSize: "M30"
            region: "US_EAST_1"
            cloudProvider: "AWS"
            
        mongodb-shared-eu:
          type: mongodb-atlas
          config:
            publicKey: "${env:MONGODB_ATLAS_PUBLIC_KEY}"
            privateKey: "${env:MONGODB_ATLAS_PRIVATE_KEY}"
            orgId: "60b5d0f0f2a1b2c3d4e5f6g7"
            projectId: "60b5d0f0f2a1b2c3d4e5f6g8"
            instanceSize: "M30"
            region: "EU_WEST_1"
            cloudProvider: "AWS"
            
        # Dedicated databases for enterprise
        mongodb-enterprise-1:
          type: mongodb-atlas
          config:
            publicKey: "${env:MONGODB_ATLAS_PUBLIC_KEY}"
            privateKey: "${env:MONGODB_ATLAS_PRIVATE_KEY}"
            orgId: "60b5d0f0f2a1b2c3d4e5f6g7"
            projectId: "60b5d0f0f2a1b2c3d4e5f6g8"
            instanceSize: "M80"
            region: "US_EAST_1"
            cloudProvider: "AWS"
        
        # Storage resources
        s3-shared-storage:
          type: s3-bucket
          config:
            credentials: "${auth:aws-main}"
            account: "${auth:aws-main.projectId}"
            name: "myapp-shared-storage"
            allowOnlyHttps: true
            
        s3-enterprise-storage:
          type: s3-bucket
          config:
            credentials: "${auth:aws-main}"
            account: "${auth:aws-main.projectId}"
            name: "myapp-enterprise-storage"
            allowOnlyHttps: true
    
    staging:
      template: api-service-template
      resources:
        # Smaller staging database
        mongodb-staging:
          type: mongodb-atlas
          config:
            publicKey: "${env:MONGODB_ATLAS_PUBLIC_KEY}"
            privateKey: "${env:MONGODB_ATLAS_PRIVATE_KEY}"
            orgId: "60b5d0f0f2a1b2c3d4e5f6g7"
            projectId: "60b5d0f0f2a1b2c3d4e5f6g8"
            instanceSize: "M10"
            region: "US_EAST_1"
            cloudProvider: "AWS"
```

**Client Stacks - Customer Allocation (Developer managed):**
```yaml
# client.yaml - Flexible customer resource allocation
stacks:
  # Standard US customers - shared resources
  customer-standard-1:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      template: web-app-template
      uses: [mongodb-shared-us, s3-shared-storage]  # Shared resources
      domain: customer1.myapp.com
      runs: [web-app]
      
  customer-standard-2:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      template: web-app-template
      uses: [mongodb-shared-us, s3-shared-storage]  # Same shared resources
      domain: customer2.myapp.com
      runs: [web-app]
  
  # EU customer - EU resources for compliance
  customer-eu:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      template: api-service-template  # Different deployment pattern
      uses: [mongodb-shared-eu, s3-shared-storage]  # EU database
      domain: customer-eu.myapp.com
      runs: [api-service]
  
  # Enterprise customer - dedicated resources
  enterprise-customer:
    type: cloud-compose
    parent: myorg/infrastructure
    config:
      template: web-app-template
      uses: [mongodb-enterprise-1, s3-enterprise-storage]  # Dedicated resources
      domain: enterprise.myapp.com
      runs: [web-app]
      scale:
        min: 5
        max: 20  # Higher scaling for enterprise
```

### **Scaling Benefits Demonstrated**

**1. Resource Pool Management:**
- DevOps defines resource pools once (`mongodb-shared-us`, `mongodb-enterprise-1`)
- Developers allocate customers flexibly using `uses` directive
- Easy migration between tiers by changing `uses` configuration

**2. Cost Optimization:**
- Standard customers share `mongodb-shared-us` (cost-effective)
- Enterprise customers get dedicated `mongodb-enterprise-1` (performance)
- Automatic resource utilization optimization

**3. Geographic Compliance:**
- EU customers automatically use `mongodb-shared-eu` for data residency
- US customers use `mongodb-shared-us`
- Simple configuration change for compliance

**4. Performance Tier Migration:**
```yaml
# Before: Customer on shared resources
customer-upgrade:
  uses: [mongodb-shared-us]
  
# After: Customer on dedicated resources (one line change!)
customer-upgrade:
  uses: [mongodb-enterprise-1]  # Zero downtime migration
```

### **Real-World Scaling Scenarios**

**Adding 100 New Customers:**
```yaml
# Traditional approach: 5000+ lines of infrastructure code
# Simple Container: 5 lines per customer = 500 lines total

customer-001:
  parent: myorg/infrastructure
  config:
    uses: [mongodb-shared-us]
    domain: customer001.myapp.com

customer-002:
  parent: myorg/infrastructure
  config:
    uses: [mongodb-shared-us]
    domain: customer002.myapp.com
    
# ... 98 more customers with minimal configuration
```

**Multi-Region Expansion:**
```yaml
# Add new parent stack for EU region
# .sc/stacks/myapp-eu/server.yaml
resources:
  mongodb-eu-cluster:
    type: mongodb-atlas
    config:
      region: EU_WEST_1

# Client stacks choose regions easily
eu-customer:
  parent: myorg/myapp-eu  # EU parent stack
  config:
    uses: [mongodb-eu-cluster]
```

This comprehensive reference covers all supported resources in Simple Container. The multidimensional resource allocation approach enables organizations to scale from startup to enterprise without operational complexity growth.

For specific implementation examples and tutorials, refer to the [How-To Guides](./howto/) section.

---

## Deployment Schemas

# **Guide for Developers: Configuring All Deployment Types in Simple Container**

Simple Container supports **three deployment types**:

| Deployment Type     | Use Case                      | Example Platforms         |
|---------------------|-------------------------------|---------------------------|
| **`cloud-compose`** | Multi-container microservices | Kubernetes, ECS Fargate   |
| **`single-image`**  | Single-container applications | AWS Lambda, Cloud Run     |
| **`static`**        | Static websites               | AWS S3, GCP Cloud Storage |

This guide explains how to configure each deployment type using **`client.yaml`**.

---

# **1Ô∏è‚É£ `cloud-compose`: Multi-Container Microservices**

‚úÖ Use **`cloud-compose`** for **microservices that require multiple containers, databases, message queues, or networking**.

‚úÖ Requires a **Dockerfile** and a **`docker-compose.yaml`** file.

‚úÖ Works with **Kubernetes, ECS Fargate, Cloud Run, etc.**

## **Example `client.yaml` for `cloud-compose`**
```yaml
---
# File: "myproject/.sc/stacks/myservice/client.yaml"

schemaVersion: 1.0

stacks:
  staging:
    type: cloud-compose
    parent: myproject/devops
    config:
      domain: staging-myservice.myproject.com
      dockerComposeFile: ./docker-compose.yaml
      uses:
        - mongodb  # Uses a MongoDB database from `server.yaml`
      runs:
        - myservice  # Deploys the "myservice" container
      size:
        cpu: 512  # 0.5 vCPU
        memory: 1024  # 1GB RAM
      scale:
        min: 1
        max: 5
        policy:
          cpu:
            max: 70  # Scale up if CPU usage exceeds 70%
      env:
        DATABASE_HOST: "${resource:mongodb.host}"
        DATABASE_NAME: "${resource:mongodb.database}"
        DATABASE_USER: "${resource:mongodb.user}"
      secrets:
        DATABASE_PASSWORD: "${resource:mongodb.password}"
```

### **üîπ Required Files**
- **`Dockerfile`** ‚Üí Defines how the service is built.
- **`docker-compose.yaml`** ‚Üí Defines how the service runs.

### **Example `docker-compose.yaml`**
```yaml
version: '3.8'
services:
  myservice:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      NODE_ENV: production
      DATABASE_URL: ${DATABASE_HOST}
```

### **Deploying to Staging**
```sh
sc deploy -s myservice -e staging
```

---

# **2Ô∏è‚É£ `single-image`: Single-Container Applications**

‚úÖ Use **`single-image`** for **single-container services like AWS Lambda or Cloud Run**.

‚úÖ Only requires a **Dockerfile** (no `docker-compose.yaml` needed).

‚úÖ Supports **cloud-specific configurations** like AWS Lambda settings.

## **Example `client.yaml` for `single-image`**
```yaml
---
# File: "myproject/.sc/stacks/myservice/client.yaml"

schemaVersion: 1.0

stacks:
  staging:
    type: single-image
    template: lambda-eu  # AWS Lambda deployment
    parent: myproject/devops
    config:
      domain: staging-myservice.myproject.com
      image:
        dockerfile: ${git:root}/Dockerfile
      timeout: 180  # AWS Lambda execution timeout
      maxMemory: 2048  # 2GB RAM
      staticEgressIP: true  # Ensures outbound requests use a static IP
      cloudExtras:
        lambdaRoutingType: function-url
        lambdaInvokeMode: RESPONSE_STREAM
      uses:
        - mongodb
      env:
        NODE_ENV: production
      secrets:
        MONGO_URI: "${resource:mongodb.uri}"
```

### **üîπ Required Files**
- **`Dockerfile`** ‚Üí Defines how the service is packaged into a container.

### **Example `Dockerfile`**
```Dockerfile
FROM node:18
WORKDIR /app
COPY . .
RUN npm install
CMD ["node", "server.js"]
```

### **Deploying to AWS Lambda**
```sh
sc deploy -s myservice -e staging
```

---

# **3Ô∏è‚É£ `static`: Deploying Static Websites**

‚úÖ Use **`static`** for **deploying static websites** (e.g., React, Vue, Angular).

‚úÖ Requires a **pre-built directory with static files**.

‚úÖ Supports **public cloud storage, CDN caching, and custom domains**.

## **Example `client.yaml` for `static` Deployment**
```yaml
---
# File: "myproject/.sc/stacks/landing-page/client.yaml"

schemaVersion: 1.0

stacks:
  prod:
    type: static
    parent: myproject/devops
    config:
      bundleDir: ${git:root}/public  # Directory containing built static files
      domain: simple-container.com  # Public domain
      indexDocument: index.html  # Default page served
      errorDocument: index.html  # Handles client-side routing (React, Vue.js)
      location: EUROPE-CENTRAL2
```

### **üîπ Required Files**
- **A built directory (`public/`)** ‚Üí Contains `index.html`, `assets/`, etc.

### **Building a Static Site (Example for React)**
```sh
npm install
npm run build
```
This creates a `public/` directory.

### **Deploying the Static Site**
```sh
sc deploy -s landing-page -e prod
```

---

# **4Ô∏è‚É£ Summary**
| Deployment Type     | Use Case                      | Required Files                      | Example Platforms         |
|---------------------|-------------------------------|-------------------------------------|---------------------------|
| **`cloud-compose`** | Multi-container microservices | `Dockerfile`, `docker-compose.yaml` | Kubernetes, ECS Fargate   |
| **`single-image`**  | Single-container applications | `Dockerfile`                        | AWS Lambda, Cloud Run     |
| **`static`**        | Static websites               | `bundleDir` with HTML/CSS/JS        | AWS S3, GCP Cloud Storage |

---

## Use Cases

# **Main use-cases for Simple Container**

Simple Container is designed to **simplify microservices deployment and infrastructure management** while maintaining cloud flexibility.
This guide outlines the **main use cases** for `sc`, highlighting how it fits into modern software development and DevOps workflows.

## Separation of DevOps and Developer parties

Simple Container allows DevOps of the company to easily set up the basics of the infrastructure using the chosen
cloud provider (be it an AWS or GCP cloud or even a hosted Kubernetes cluster), prepare main secrets and account
configurations ahead of time.
DevOps should simply prepare a basic configuration of their resources and then invoke a single command `sc provision`
which will take care of the initial provision steps, and will create all resources a company needs.

For developers Simple Container provides the high-level abstraction allowing to easily set up their CI/CD pipeline and
deploy their services into the provisioned cloud. With Simple Container adding a new microservice a company needs takes
a couple of very simple steps making the whole process a self-service operation without much help needed from DevOps.


## **1Ô∏è‚É£ Deploying Microservices Without DevOps Involvement - 500x Faster**

‚úÖ **Use Case:** Developers need to deploy new microservices **quickly and independently** without requiring DevOps to configure infrastructure.

‚úÖ **Quantified Benefit:** 
- **Customer Onboarding**: **5 minutes** vs **2-3 days** traditional approaches
- **Configuration**: **5 lines** vs **50+ lines** of YAML per service
- **Developer Autonomy**: **Self-service deployment** vs **DevOps approval bottleneck**

‚úÖ **Real-World Scenario:** Adding 100 new customers
```yaml
# Traditional: 5000+ lines, 200-300 days
# Simple Container: 500 lines, 8.3 hours
customer-001:
  parentEnv: production
  config:
    domain: customer001.myapp.com
    secrets:
      CUSTOMER_SETTINGS: ${env:CUSTOMER_001_SETTINGS}
```

‚úÖ **Ideal for:** Organizations with many microservices, where **DevOps cannot manually provision every service**.

---

## **2Ô∏è‚É£ Managing Infrastructure Centrally Without Complexity - 5x Operational Efficiency**

‚úÖ **Use Case:** DevOps teams need a **single source of truth** for infrastructure such as **databases, storage, networking, and secrets**.

‚úÖ **Quantified Benefit:**
- **DevOps to Customer Ratio**: **1:100+ customers** vs **1:10-20** traditional
- **Configuration Complexity**: **90% reduction** (500 vs 5000+ lines for 100 customers)
- **Infrastructure Drift Risk**: **Low (template-based)** vs **High (manual management)**

‚úÖ **Multi-Dimensional Resource Allocation:**
```yaml
# server.yaml - Define resource pools once
resources:
  production:
    resources:
      # Shared resources for standard customers
      mongodb-shared-us:
        type: mongodb-atlas
        config:
          clusterName: shared-us
          instanceSize: M30
      # Dedicated resources for enterprise
      mongodb-enterprise-1:
        type: mongodb-atlas
        config:
          clusterName: enterprise-1
          instanceSize: M80
          dedicatedTenant: true
```

‚úÖ **Ideal for:** Organizations using **Terraform/Pulumi but want to simplify microservices provisioning**.

---

## **3Ô∏è‚É£ Deploying Services Across Multiple Cloud Providers**

‚úÖ **Use Case:** Organizations want to **deploy services across AWS, GCP, and Kubernetes** without rewriting cloud-specific configurations.

‚úÖ **Benefit:** `sc` abstracts cloud infrastructure, allowing services to run **seamlessly on different platforms**.

‚úÖ **Ideal for:** Companies that need **multi-cloud or hybrid cloud deployments**.

---

## **4Ô∏è‚É£ Simplifying CI/CD with Built-in Deployment Automation**

‚úÖ **Use Case:** Teams want to **automate deployments** without managing complex CI/CD pipeline scripts.

‚úÖ **Benefit:** `sc` provides **built-in deployment commands**, eliminating the need for **manual Helm charts, Terraform pipelines, or custom scripts**.

‚úÖ **Ideal for:** Teams looking to **reduce deployment complexity and improve release speed**.

---

## **5Ô∏è‚É£ Secure Secrets Management Without External Tools**

‚úÖ **Use Case:** Microservices require **environment-specific credentials (e.g., database passwords, API keys)** stored securely.

‚úÖ **Benefit:** `sc` **automatically stores secrets in cloud-native secret managers** and injects them securely into deployed services.

‚úÖ **Ideal for:** Organizations that previously relied on **manual secret injection or external tools like Vault**.

---

## **6Ô∏è‚É£ Deploying Static Websites with Cloud Storage & CDN**

‚úÖ **Use Case:** Teams need to **host static websites (React, Vue, Angular, documentation sites)** on cloud storage with CDN integration.

‚úÖ **Benefit:** `sc` provisions **S3, GCP Cloud Storage, or other cloud storage solutions**, handling **domain setup and caching**.

‚úÖ **Ideal for:** Companies looking for a **fast and automated static site deployment**.

---

## **7Ô∏è‚É£ Migrating Services Between Cloud Providers Without Rewriting Configurations**

‚úÖ **Use Case:** Organizations need to **move workloads from AWS to GCP, or from Kubernetes to ECS** while minimizing downtime.

‚úÖ **Benefit:** `sc` allows teams to **update infrastructure configurations without modifying microservices deployment settings**.

‚úÖ **Ideal for:** Businesses that need **cloud migration flexibility**.

---

## **8Ô∏è‚É£ Scaling Microservices with Cloud-Native Auto-Scaling - Zero Downtime Migrations**

‚úÖ **Use Case:** Services must **scale dynamically based on demand** without manual intervention.

‚úÖ **Quantified Benefit:**
- **Performance Tier Migration**: **One-line change** vs **manual infrastructure rebuild**
- **Multi-region Expansion**: **Single parent stack change** vs **duplicate infrastructure code**
- **Cost Optimization**: **70% reduction** through intelligent resource sharing

‚úÖ **Real-World Scenario:** Performance tier migration
```yaml
# Before: Customer on shared resources
customer-enterprise:
  uses: [mongodb-shared-us]
  
# After: Customer on dedicated resources (one line change!)
customer-enterprise:
  uses: [mongodb-enterprise-dedicated]
  
# Automatic migration, zero downtime, easy rollback
```

‚úÖ **Benefit:** `sc` integrates with **Kubernetes auto-scaling, AWS ECS scaling, and serverless scaling mechanisms**, ensuring optimal resource usage.

‚úÖ **Ideal for:** Organizations handling **high-traffic applications** that require **automatic scaling**.

---

## **9Ô∏è‚É£ Managing Microservices Dependencies Without Manual Configuration**

‚úÖ **Use Case:** A service needs **databases, messaging queues, or cloud storage** but shouldn‚Äôt require developers to configure them manually.

‚úÖ **Benefit:** `sc` **automatically connects services to provisioned resources** such as databases and queues, reducing developer effort.

‚úÖ **Ideal for:** Teams working with **complex microservice architectures**.

---

## **1Ô∏è‚É£0Ô∏è‚É£ Standardizing Microservices Deployment Across Teams**

‚úÖ **Use Case:** Different teams deploy services using **various methods (Terraform, Helm, custom scripts), leading to inconsistency**.

‚úÖ **Benefit:** `sc` ensures **consistent deployment standards across all microservices**, improving maintainability.

‚úÖ **Ideal for:** Enterprises with **multiple teams managing microservices independently**.

---

## **1Ô∏è‚É£0Ô∏è‚É£ Enterprise Scaling Scenarios**

### **Scenario 1: SaaS Company Scaling from 10 to 1000 Customers**

**Challenge:** Traditional approach requires linear DevOps scaling
- **Traditional**: 1 DevOps per 10-20 customers = 50-100 DevOps engineers needed
- **Simple Container**: 1 DevOps per 100+ customers = 10 DevOps engineers needed
- **Savings**: **80% staff reduction**, **$4M+ annual savings**

### **Scenario 2: Multi-Region Enterprise Expansion**

**Challenge:** Duplicate infrastructure for each region
- **Traditional**: Separate Terraform/Pulumi code for each region
- **Simple Container**: Single parent stack per region, customers choose easily
- **Result**: **90% less code**, **instant region switching**

### **Scenario 3: Compliance and Security at Scale**

**Challenge:** Manual secret management across hundreds of services
- **Traditional**: Manual secret rotation, security risks
- **Simple Container**: Automatic namespace isolation, built-in secrets management
- **Result**: **Zero security incidents**, **automated compliance**

---

# **Conclusion: Quantified Scaling Results**

Simple Container **transforms microservices deployment** by providing **measurable scaling advantages**:

‚úÖ **500x faster customer onboarding** (5 minutes vs 2-3 days)

‚úÖ **90% reduction in configuration complexity** (500 vs 5000+ lines)

‚úÖ **5x operational efficiency** (1 DevOps per 100+ vs 10-20 customers)

‚úÖ **70% cost reduction** through intelligent resource sharing

‚úÖ **Zero downtime migrations** with one-line configuration changes

‚úÖ **80% staff reduction** in operational overhead

**Simple Container enables organizations to scale from startup to enterprise without operational complexity growth**, transforming container orchestration from a complex infrastructure challenge into a simple configuration management task.

Organizations adopting `sc` achieve **enterprise-grade scalability** with **startup-level simplicity**.

---

# Advanced

## Scaling Advantages

# Simple Container: Superior Scaling vs Plain Kubernetes/ECS

## Executive Summary

Simple Container (SC) provides significant scaling advantages over plain Kubernetes or ECS deployments managed with Pulumi/Terraform. Through its innovative separation of concerns, built-in secrets management, and developer self-service capabilities, SC enables organizations to scale from startup to enterprise without the operational complexity typically associated with container orchestration platforms.

This document analyzes why Simple Container's architecture fundamentally scales better than traditional infrastructure-as-code approaches for SaaS and multi-tenant deployments.

## Core Scaling Challenges with Traditional Approaches

### Plain Kubernetes + Pulumi/Terraform Limitations

**1. Infrastructure Knowledge Barrier:**
```yaml
# Plain Kubernetes - Developers need deep K8s knowledge
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  namespace: customer-a
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: myapp:latest
        ports:
        - containerPort: 3000
        env:
        - name: MONGO_URL
          valueFrom:
            secretKeyRef:
              name: customer-a-secrets
              key: mongo-url
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
  namespace: customer-a
spec:
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 3000
  type: ClusterIP
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp-ingress
  namespace: customer-a
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
spec:
  tls:
  - hosts:
    - customera.myapp.com
    secretName: customera-tls
  rules:
  - host: customera.myapp.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: myapp-service
            port:
              number: 80
```

**Problems:**

- **Developer Bottleneck**: Requires Kubernetes expertise for every deployment
- **Configuration Complexity**: 50+ lines of YAML for basic service deployment
- **Manual Resource Management**: Developers must understand requests/limits, networking, ingress
- **No Built-in Multi-tenancy**: Each customer requires separate namespace and resource definitions

**2. Secret Management Complexity:**
```bash
# Manual secret creation for each customer
kubectl create secret generic customer-a-secrets \
  --from-literal=mongo-url="mongodb://..." \
  --from-literal=api-key="..." \
  --namespace=customer-a

kubectl create secret generic customer-b-secrets \
  --from-literal=mongo-url="mongodb://..." \
  --from-literal=api-key="..." \
  --namespace=customer-b
```

**Problems:**

- **Manual Secret Provisioning**: No automation for customer-specific secrets
- **No Secret Rotation**: Manual process for updating secrets
- **Security Risks**: Secrets often stored in plain text in configuration files
- **Operational Overhead**: DevOps team manages every customer secret

### Plain ECS + Pulumi/Terraform Limitations

**1. Infrastructure Complexity:**
```typescript
// Pulumi ECS - Complex infrastructure definition
const cluster = new aws.ecs.Cluster("myapp-cluster");

const taskDefinition = new aws.ecs.TaskDefinition("myapp-task", {
    family: "myapp",
    cpu: "1024",
    memory: "2048",
    networkMode: "awsvpc",
    requiresCompatibilities: ["FARGATE"],
    executionRoleArn: executionRole.arn,
    taskRoleArn: taskRole.arn,
    containerDefinitions: JSON.stringify([{
        name: "myapp",
        image: "myapp:latest",
        portMappings: [{
            containerPort: 3000,
            protocol: "tcp"
        }],
        environment: [
            { name: "NODE_ENV", value: "production" }
        ],
        secrets: [
            {
                name: "MONGO_URL",
                valueFrom: mongoSecret.arn
            }
        ],
        logConfiguration: {
            logDriver: "awslogs",
            options: {
                "awslogs-group": logGroup.name,
                "awslogs-region": "us-east-1",
                "awslogs-stream-prefix": "myapp"
            }
        }
    }])
});

const service = new aws.ecs.Service("myapp-service", {
    cluster: cluster.id,
    taskDefinition: taskDefinition.arn,
    launchType: "FARGATE",
    desiredCount: 3,
    networkConfiguration: {
        subnets: privateSubnets.map(s => s.id),
        securityGroups: [securityGroup.id],
        assignPublicIp: false
    },
    loadBalancers: [{
        targetGroupArn: targetGroup.arn,
        containerName: "myapp",
        containerPort: 3000
    }]
});

const alb = new aws.elasticloadbalancingv2.LoadBalancer("myapp-alb", {
    loadBalancerType: "application",
    subnets: publicSubnets.map(s => s.id),
    securityGroups: [albSecurityGroup.id]
});
```

**Problems:**

- **Infrastructure Expertise Required**: Developers need AWS ECS, networking, and security knowledge
- **Manual Load Balancer Management**: Complex ALB configuration for each customer
- **No Built-in Scaling**: Manual auto-scaling configuration
- **Resource Overhead**: Fargate pricing per task vs shared resource utilization

## Simple Container's Scaling Advantages

### 1. Separation of Concerns Architecture

**DevOps Responsibility (Parent Stack - `server.yaml`):**
```yaml
# server.yaml - Infrastructure managed by DevOps once
provisioner:
  type: pulumi
  config:
    state-storage:
      type: s3-bucket
      config:
        bucketName: myapp-pulumi-state
        region: us-east-1
    secrets-provider:
      type: aws-kms
      config:
        kmsKeyId: "${secret:pulumi-kms-key-id}"

templates:
  stack-per-app-k8s:
    type: kubernetes-cloudrun
    config:
      kubeconfig: "${auth:kubernetes}"
      dockerRegistryURL: docker.mycompany.com
      dockerRegistryUsername: "${secret:docker-registry-username}"
      dockerRegistryPassword: "${secret:docker-registry-password}"
      caddyResource: caddy
      useSSL: false

resources:
  prod:
    template: stack-per-app-k8s
    resources:
      # Multiple MongoDB clusters for customer allocation
      mongodb-cluster-us-1a:
        type: mongodb-atlas
        config:
          clusterName: myapp-us-1a
          region: us-east-1
          instanceSize: M30
          
      mongodb-cluster-us-1b:
        type: mongodb-atlas
        config:
          clusterName: myapp-us-1b
          region: us-east-1
          instanceSize: M30
          
      mongodb-enterprise:
        type: mongodb-atlas
        config:
          clusterName: myapp-enterprise
          region: us-east-1
          instanceSize: M60
          dedicatedTenant: true
```

**Developer Responsibility (Service Stack - `client.yaml`):**
```yaml
# client.yaml - Simple customer deployment by developers
schemaVersion: 1.0

stacks:
  # Base environment with shared resources
  prod: &prod
    type: cloud-compose
    parent: simple-container/myapp-infra
    config: &config
      domain: prod.myapp.com
      uses: [mongodb-cluster-us-1a]  # Choose which cluster to use
      runs: [myapp]
      env: &env
        NODE_ENV: production
        PORT: 3000
      secrets: &secrets
        MONGO_URL: ${resource:mongodb-cluster-us-1a.uri}
        
  # Customer deployments - minimal configuration
  customer-a:
    parentEnv: prod
    <<: *prod
    config:
      <<: *config
      domain: customera.myapp.com
      secrets:
        <<: *secrets
        CUSTOMER_SETTINGS: ${env:CUSTOMER_A_SETTINGS}
        
  customer-b:
    parentEnv: prod
    <<: *prod
    config:
      <<: *config
      domain: customerb.myapp.com
      uses: [mongodb-cluster-us-1b]  # Different cluster
      secrets:
        <<: *secrets
        MONGO_URL: ${resource:mongodb-cluster-us-1b.uri}
        CUSTOMER_SETTINGS: ${env:CUSTOMER_B_SETTINGS}
        
  enterprise-customer:
    parentEnv: prod
    <<: *prod
    config:
      <<: *config
      domain: enterprise.myapp.com
      uses: [mongodb-enterprise]  # Dedicated cluster
      scale:
        min: 5
        max: 20
      secrets:
        <<: *secrets
        MONGO_URL: ${resource:mongodb-enterprise.uri}
        ENTERPRISE_SETTINGS: ${env:ENTERPRISE_SETTINGS}
```

**Scaling Benefits:**

- **DevOps Manages Infrastructure Once**: Kubernetes clusters, databases, networking, security
- **Developers Deploy Self-Service**: No infrastructure knowledge required
- **Automatic Resource Provisioning**: Caddy, SSL, scaling, monitoring configured automatically
- **Multi-tenant by Design**: Easy customer onboarding with minimal configuration

### 2. Built-in Secrets Management

**Traditional Approach Problems:**
```bash
# Manual secret management - doesn't scale
kubectl create secret generic customer-secrets \
  --from-literal=api-key="..." \
  --from-literal=db-password="..." \
  --namespace=customer-namespace

# Terraform - secrets in code (security risk)
resource "kubernetes_secret" "customer_secrets" {
  metadata {
    name      = "customer-secrets"
    namespace = "customer-namespace"
  }
  data = {
    api-key     = var.customer_api_key
    db-password = var.customer_db_password
  }
}
```

**Simple Container's Current Secrets Management:**
```yaml
# server.yaml - Current SC secrets management
templates:
  stack-per-app-k8s:
    type: kubernetes-cloudrun
    config:
      kubeconfig: "${auth:kubernetes}"
      dockerRegistryURL: docker.mycompany.com
      dockerRegistryUsername: "${secret:docker-registry-username}"
      dockerRegistryPassword: "${secret:docker-registry-password}"
      caddyResource: caddy
      useSSL: false
          
# client.yaml - Built-in secret references and environment variables
stacks:
  customer-a:
    config:
      secrets:
        # Reference shared secrets from parent stack's secrets.yaml
        DOCKER_PASSWORD: ${secret:docker-registry-password}
        MONGODB_CONNECTION: ${secret:mongodb-connection}
        # Customer-specific secrets via environment variables
        CUSTOMER_SETTINGS: ${env:CUSTOMER_A_SETTINGS}
        API_KEY: ${env:CUSTOMER_A_API_KEY}
```

**Future Enhancement - External Secret Manager Integration:**
```bash
# Would require CI/CD pipeline integration to:
# 1. Read secrets from external secret manager (AWS Secrets Manager, Vault, etc.)
# 2. Inject customer-specific secrets as environment variables
# 3. Deploy with customer-specific environment variables

# Example CI/CD integration (future development needed):
# aws secretsmanager get-secret-value --secret-id "myapp/customer-a/settings"
# export CUSTOMER_A_SETTINGS="$(aws secretsmanager get-secret-value ...)"
# sc deploy -s customer-a -e production
```

**Current SC Scaling Benefits:**

- **Environment Variable Support**: Simple `${env:VARIABLE_NAME}` references in configuration
- **Developer Simplicity**: Environment variables instead of complex Kubernetes secret management
- **Namespace Isolation**: Each customer stack deployed to separate namespace with isolated secrets

**Future Scaling Benefits (with external secret manager integration):**

- **Automatic Secret Injection**: CI/CD pipeline integration for secret retrieval
- **External Secret Manager Integration**: AWS Secrets Manager, Vault, Azure Key Vault
- **Customer-Specific Isolation**: Enhanced secrets isolated per customer via external systems
- **Rotation Support**: External secret rotation without deployment changes

### 3. Multi-Dimensional Resource Allocation

**Traditional Approach - Manual Resource Management:**
```yaml
# Each customer needs separate infrastructure definition
# customer-a-infrastructure.yaml
resource "aws_ecs_cluster" "customer_a" {
  name = "customer-a-cluster"
}

resource "aws_rds_instance" "customer_a_db" {
  identifier = "customer-a-database"
  engine     = "postgres"
  # ... complex configuration
}

# customer-b-infrastructure.yaml  
resource "aws_ecs_cluster" "customer_b" {
  name = "customer-b-cluster"
}

resource "aws_rds_instance" "customer_b_db" {
  identifier = "customer-b-database"
  engine     = "postgres"
  # ... duplicate configuration
}
```

**Simple Container - Flexible Resource Sharing:**
```yaml
# server.yaml - Define resource pools once
resources:
  production:
    resources:
      # Shared resources for standard customers
      mongodb-shared-us:
        type: mongodb-atlas
        config:
          clusterName: shared-us
          instanceSize: M30
          
      mongodb-shared-eu:
        type: mongodb-atlas
        config:
          clusterName: shared-eu
          instanceSize: M30
          
      # Dedicated resources for enterprise
      mongodb-enterprise-1:
        type: mongodb-atlas
        config:
          clusterName: enterprise-1
          instanceSize: M80

# client.yaml - Customers choose resources flexibly
stacks:
  # Standard US customers share resources
  standard-customer-1:
    uses: [mongodb-shared-us]
    
  standard-customer-2:
    uses: [mongodb-shared-us]  # Same shared resource
    
  # EU customer uses EU resources
  eu-customer:
    uses: [mongodb-shared-eu]
    
  # Enterprise customer gets dedicated resources
  enterprise:
    uses: [mongodb-enterprise-1]
```

**Scaling Benefits:**

- **Resource Pool Management**: Define resources once, allocate flexibly
- **Cost Optimization**: Share resources among compatible customers
- **Performance Tiers**: Easy allocation of dedicated vs shared resources
- **Geographic Distribution**: Automatic compliance with data residency
- **Easy Migration**: Move customers between resource pools by changing `uses` directive

### 4. Automatic Operational Features

**Traditional Kubernetes - Manual Configuration:**
```yaml
# Manual HPA configuration for each customer
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: customer-a-hpa
  namespace: customer-a
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80

# Manual ingress for each customer
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: customer-a-ingress
  namespace: customer-a
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  tls:
  - hosts:
    - customera.myapp.com
    secretName: customera-tls
  rules:
  - host: customera.myapp.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: myapp-service
            port:
              number: 80
```

**Simple Container - Best Practice Templates with Client Configuration:**
```yaml
# server.yaml - Best practice template definition
templates:
  stack-per-app-k8s:
    type: kubernetes-cloudrun
    config:
      kubeconfig: "${auth:kubernetes}"
      dockerRegistryURL: docker.mycompany.com
      dockerRegistryUsername: "${secret:docker-registry-username}"
      dockerRegistryPassword: "${secret:docker-registry-password}"
      caddyResource: caddy
      useSSL: false

# client.yaml - Customer-specific configurations
stacks:
  customer-a:
    type: cloud-compose
    config:
      domain: customera.myapp.com
      runs: [myapp]
      scale:
        max: 10
        min: 2
        policy:
          cpu:
            max: 70
          memory:
            max: 75
      env:
        NODE_ENV: production
        PORT: 3000
      secrets:
        CUSTOMER_SETTINGS: ${env:CUSTOMER_A_SETTINGS}
```

**Scaling Benefits:**

- **Best Practice Templates**: SC provides proven Kubernetes deployment patterns
- **Client-Side Configuration**: Scaling, domains, and environment variables defined per customer
- **Consistent Infrastructure**: All customers use same underlying template with custom configurations
- **Simplified Deployment**: Developers configure business logic, not infrastructure complexity

## Scaling Comparison: Real-World Scenarios

### Scenario 1: Adding 100 New Customers

**Traditional Kubernetes/ECS:**
```bash
# For each of 100 customers, DevOps must:
1. Create namespace/cluster
2. Define deployment YAML (50+ lines each)
3. Configure ingress and SSL certificates
4. Set up monitoring and logging
5. Create secrets manually
6. Configure scaling policies
7. Set up backup and disaster recovery

# Result: 5000+ lines of configuration
# Time: 2-3 days per customer = 200-300 days
# Team: Requires DevOps expertise for each deployment
```

**Simple Container:**
```yaml
# DevOps defines infrastructure once (already done)

# For each of 100 customers, developers add:
customer-001:
  parentEnv: production
  config:
    domain: customer001.myapp.com
    secrets:
      CUSTOMER_SETTINGS: ${env:CUSTOMER_001_SETTINGS}

# Result: 5 lines per customer = 500 lines total
# Time: 5 minutes per customer = 8.3 hours total
# Team: Developers can self-serve, no DevOps bottleneck
```

### Scenario 2: Multi-Region Expansion

**Traditional Approach:**
```typescript
// Duplicate entire infrastructure for each region
const usEastCluster = new aws.ecs.Cluster("us-east-cluster");
const usWestCluster = new aws.ecs.Cluster("us-west-cluster");
const euWestCluster = new aws.ecs.Cluster("eu-west-cluster");

// Duplicate networking, security, monitoring for each region
// Manually manage customer allocation across regions
// Complex cross-region secret management
```

**Simple Container:**
```yaml
# .sc/stacks/myapp-us/server.yaml
resources:
  prod:
    resources:
      mongodb-us: { region: us-east-1 }
      
# .sc/stacks/myapp-eu/server.yaml  
resources:
  prod:
    resources:
      mongodb-eu: { region: eu-west-1 }

# client.yaml - Customers choose regions easily
us-customer:
  parent: simple-container/myapp-us
  parentEnv: prod
  
eu-customer:
  parent: simple-container/myapp-eu
  parentEnv: prod
```

### Scenario 3: Performance Tier Migration

**Traditional Approach:**
```bash
# Manual migration process:
1. Create new high-performance infrastructure
2. Update customer deployment configurations
3. Migrate data manually
4. Update DNS and certificates
5. Monitor and rollback if issues
6. Clean up old infrastructure

# High risk, manual process, downtime required
```

**Simple Container:**
```yaml
# Before: Customer on shared resources
customer-enterprise:
  uses: [mongodb-shared-us]
  
# After: Customer on dedicated resources (one line change!)
customer-enterprise:
  uses: [mongodb-enterprise-dedicated]
  
# Automatic migration, zero downtime, easy rollback
```

## Cost and Operational Efficiency

### Development Velocity

**Traditional Approach:**

- **Time to First Deployment**: 2-3 days (infrastructure setup)
- **Developer Onboarding**: 2-4 weeks (Kubernetes/AWS training)
- **Feature Development**: Blocked by infrastructure changes
- **Customer Onboarding**: DevOps bottleneck, 1-2 days per customer

**Simple Container:**

- **Time to First Deployment**: 15 minutes (configuration only)
- **Developer Onboarding**: 1-2 hours (simple YAML configuration)
- **Feature Development**: Independent of infrastructure
- **Customer Onboarding**: Self-service, 5 minutes per customer

### Operational Overhead

**Traditional Approach:**

- **Team Size**: 1 DevOps engineer per 10-20 customers
- **On-call Burden**: Complex troubleshooting across multiple systems
- **Maintenance**: Manual updates for each customer deployment
- **Scaling**: Linear increase in operational complexity

**Simple Container:**

- **Team Size**: 1 DevOps engineer per 100+ customers
- **On-call Burden**: Centralized infrastructure, easier troubleshooting
- **Maintenance**: Template updates apply to all customers
- **Scaling**: Operational complexity remains constant

### Cost Optimization

**Traditional Approach:**
```bash
# ECS Fargate: $0.04048 per vCPU per hour + $0.004445 per GB per hour
# 100 customers √ó 2 vCPU √ó 4GB √ó 24h √ó 30 days = $2,429/month per customer
# Total: $242,900/month for 100 customers

# Kubernetes: Manual resource allocation, often over-provisioned
# 100 customers √ó dedicated nodes = high infrastructure costs
# No resource sharing optimization
```

**Simple Container:**
```bash
# Kubernetes with resource sharing:
# Shared infrastructure: 50% cost reduction through better utilization
# Automatic scaling: 30% cost reduction through right-sizing
# Multi-tenant architecture: 40% cost reduction through resource sharing
# Total: $72,870/month for 100 customers (70% cost reduction)
```

## Security and Compliance Advantages

### Secret Management Security

**Traditional Approach Issues:**

- Secrets often stored in plain text configuration files
- Manual secret rotation processes
- No audit trails for secret access
- Difficult to implement least-privilege access

**Simple Container Security:**
```yaml
# Option 1: Shared secrets.yaml per parent stack
# .sc/stacks/myapp-us/secrets.yaml
secrets:
  mongodb-connection: "mongodb+srv://..."
  docker-registry-password: "secure-password"
  
# Option 2: Environment variables from external secret manager
# CI/CD pipeline reads from AWS Secrets Manager/Vault and injects as env vars
# client.yaml references environment variables
stacks:
  customer-a:
    config:
      env:
        DATABASE_URL: ${env:CUSTOMER_A_DATABASE_URL}
        API_KEY: ${env:CUSTOMER_A_API_KEY}
```

**Benefits:**

- **Namespace Isolation**: Each customer stack deployed to separate namespace with isolated secrets
- **Environment Variable Support**: Simple `${env:VARIABLE_NAME}` references in client configurations
- **Shared Secret Management**: Common secrets managed once per parent stack via secrets.yaml
- **External Integration Ready**: CI/CD can inject customer-specific secrets from external managers

### Multi-Tenant Security

**Traditional Kubernetes:**
```yaml
# Manual namespace isolation - error-prone
apiVersion: v1
kind: Namespace
metadata:
  name: customer-a
  labels:
    customer: customer-a
    
# Manual network policies for each customer
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: customer-a-isolation
  namespace: customer-a
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: customer-a
```

**Simple Container:**
```yaml
# Simple Container provides automatic namespace isolation
# Each deployed stack gets its own Kubernetes namespace
# Namespace-level isolation provides tenant security automatically

# Example: Customer stacks deployed to separate namespaces
# customer-a stack ‚Üí customer-a-namespace
# customer-b stack ‚Üí customer-b-namespace
# enterprise stack ‚Üí enterprise-namespace

# Kubernetes namespace isolation provides:
# - Resource isolation (pods, services, secrets)
# - Network isolation (default namespace boundaries)
# - RBAC isolation (namespace-scoped permissions)
# - Resource quotas (can be applied per namespace)
```

## Conclusion: Simple Container vs Terraform/Pulumi Comparison

### Infrastructure Management Complexity

| Aspect                                | Terraform/Pulumi                   | Simple Container               | Advantage                  |
|---------------------------------------|------------------------------------|--------------------------------|----------------------------|
| **Configuration Lines**               | 5000+ lines for 100 customers      | 500 lines for 100 customers    | **90% reduction**          |
| **Infrastructure Knowledge Required** | Deep cloud expertise needed        | Business logic focus only      | **Developer self-service** |
| **Multi-Tenant Setup**                | Manual per-customer infrastructure | Built-in parentEnv inheritance | **Automatic isolation**    |
| **Secret Management**                 | Manual per-environment setup       | Built-in ${secret:} + ${env:}  | **Unified approach**       |
| **Deployment Complexity**             | Separate Terraform + K8s manifests | Single SC configuration        | **Single source of truth** |

### Operational Scalability

| Metric                         | Terraform/Pulumi              | Simple Container           | Improvement        |
|--------------------------------|-------------------------------|----------------------------|--------------------|
| **DevOps to Customer Ratio**   | 1:10-20 customers             | 1:100+ customers           | **5x efficiency**  |
| **Customer Onboarding Time**   | 2-3 days                      | 5 minutes                  | **500x faster**    |
| **Infrastructure Drift Risk**  | High (manual management)      | Low (template-based)       | **Reduced errors** |
| **Cross-Region Deployment**    | Duplicate infrastructure code | Single parent stack change | **DRY principle**  |
| **Performance Tier Migration** | Manual infrastructure rebuild | One-line uses directive    | **Zero downtime**  |

### Developer Experience

| Feature                     | Terraform/Pulumi         | Simple Container        | Benefit                   |
|-----------------------------|--------------------------|-------------------------|---------------------------|
| **Learning Curve**          | Months (cloud + IaC)     | Days (business config)  | **Faster onboarding**     |
| **Deployment Autonomy**     | Requires DevOps approval | Self-service deployment | **Independent teams**     |
| **Environment Consistency** | Manual synchronization   | Automatic inheritance   | **Reduced bugs**          |
| **Resource Allocation**     | Complex calculations     | Simple uses directive   | **Simplified management** |
| **Scaling Configuration**   | Multiple files/tools     | Single scale block      | **Unified interface**     |

### Cost and Resource Efficiency

| Factor                      | Terraform/Pulumi        | Simple Container        | Savings                     |
|-----------------------------|-------------------------|-------------------------|-----------------------------|
| **Infrastructure Overhead** | Per-customer resources  | Shared resource pools   | **70% cost reduction**      |
| **Operational Staff**       | High DevOps requirement | Minimal DevOps overhead | **80% staff reduction**     |
| **Resource Utilization**    | Often over-provisioned  | Right-sized sharing     | **Better efficiency**       |
| **Maintenance Burden**      | Continuous per-customer | Template updates only   | **Centralized maintenance** |
| **Monitoring Complexity**   | Per-customer setup      | Built-in observability  | **Reduced tooling costs**   |

### Enterprise Readiness

| Capability               | Terraform/Pulumi         | Simple Container             | Advantage                  |
|--------------------------|--------------------------|------------------------------|----------------------------|
| **Multi-Region Support** | Complex state management | Parent stack per region      | **Simplified geography**   |
| **Disaster Recovery**    | Manual backup strategies | Built-in resilience patterns | **Automated DR**           |
| **Compliance Auditing**  | Custom implementation    | Namespace-based isolation    | **Built-in compliance**    |
| **Secret Rotation**      | Manual processes         | External manager integration | **Automated security**     |
| **Access Control**       | Complex IAM policies     | Kubernetes RBAC + namespaces | **Simplified permissions** |

### Real-World Scaling Scenarios

| Scenario                    | Traditional Approach         | Simple Container       | Time Savings           |
|-----------------------------|------------------------------|------------------------|------------------------|
| **Add 100 customers**       | 100 √ó infrastructure setup   | 100 √ó client config    | **95% faster**         |
| **Multi-region expansion**  | Duplicate all infrastructure | Add parent stack       | **90% less code**      |
| **Performance tier change** | Infrastructure migration     | Change uses directive  | **99% faster**         |
| **Security update**         | Update all customer configs  | Update parent template | **One-time change**    |
| **Monitoring rollout**      | Per-customer implementation  | Template update        | **Instant deployment** |

## Why Simple Container Wins

### 1. **Separation of Concerns Architecture**

- **DevOps**: Manages infrastructure complexity once in parent stacks
- **Developers**: Focus on business logic in client configurations
- **Result**: Clear boundaries prevent configuration drift and operational errors

### 2. **Built-in Multi-Tenancy**

- **Native SaaS patterns** with parentEnv inheritance
- **Automatic customer isolation** through Kubernetes namespaces
- **Flexible resource allocation** using parent/parentEnv/uses directives

### 3. **Operational Automation**

- **Best practice templates** with proven deployment patterns
- **Built-in secret management** with ${secret:} and ${env:} support
- **Kubernetes-native features** leveraged automatically

### 4. **Developer Productivity**

- **Self-service deployment** without infrastructure expertise
- **5-minute customer onboarding** vs days with traditional approaches
- **Linear development velocity** regardless of customer count

### 5. **Cost Optimization**

- **70% cost reduction** through intelligent resource sharing
- **1 DevOps engineer per 100+ customers** vs 1 per 10-20 traditional
- **Automatic right-sizing** and scaling optimization

**Simple Container transforms container orchestration from a complex infrastructure challenge into a simple configuration management task, enabling organizations to scale from startup to enterprise without operational complexity growth.**

For SaaS companies and multi-tenant applications, Simple Container provides the perfect balance of developer productivity, operational efficiency, and enterprise-grade capabilities that traditional Terraform/Pulumi + Kubernetes deployments simply cannot match at scale.


---

## Tool Comparisons

# **Comparison: Simple Container vs Pulumi vs Terraform**

This document provides a **detailed comparison** between **Simple Container**, **Pulumi**, and **Terraform** to help DevOps teams and developers choose the best tool for managing cloud infrastructure and microservice deployments.

---

# **1Ô∏è‚É£ Overview of the Tools**

| Tool                 | Purpose                                                            | Primary Users            | Configuration Language                 |
|----------------------|--------------------------------------------------------------------|--------------------------|----------------------------------------|
| **Simple Container** | **CI/CD & infrastructure deployment for microservices**            | Developers & DevOps      | YAML                                   |
| **Pulumi**           | **Infrastructure as Code (IaC) & Cloud Infrastructure Management** | DevOps & Cloud Engineers | TypeScript, Python, Go, C#             |
| **Terraform**        | **Infrastructure as Code (IaC) & Cloud Provisioning**              | DevOps & Cloud Engineers | HCL (HashiCorp Configuration Language) |

---

# **2Ô∏è‚É£ Key Differences**

## **üîπ Purpose & Focus**

| Feature           | Simple Container                                | Pulumi                                                            | Terraform                                                            |
|-------------------|-------------------------------------------------|-------------------------------------------------------------------|----------------------------------------------------------------------|
| **Primary Focus** | **Microservices Deployment & CI/CD Pipelines**  | **Full Infrastructure as Code (IaC) with imperative programming** | **Full Infrastructure as Code (IaC) with declarative configuration** |
| **Best For**      | **Developers & DevOps** deploying microservices | **DevOps & Cloud Engineers** managing infrastructure with code    | **DevOps & Cloud Engineers** managing cloud infrastructure           |
| **Use Cases**     | **CI/CD, Microservices, Secrets Management**    | **Cloud-native deployments, Kubernetes, AWS, GCP, Azure**         | **Cloud Infrastructure, Networking, Security, Compute**              |

‚úÖ **SC is focused on microservice deployment, while Pulumi and Terraform are full-fledged IaC tools.**

---

## **üîπ Configuration Language**

| Feature        | Simple Container                     | Pulumi                                | Terraform                                  |
|----------------|--------------------------------------|---------------------------------------|--------------------------------------------|
| **Language**   | **YAML** (declarative)               | **TypeScript, Python, Go, C#**        | **HCL (HashiCorp Configuration Language)** |
| **Complexity** | **Simple (high-level abstractions)** | **Moderate (imperative programming)** | **Complex (declarative DSL)**              |

‚úÖ **SC uses YAML for simplicity, whereas Pulumi and Terraform require programming or a DSL.**

---

## **üîπ Infrastructure Management**

| Feature                         | Simple Container                              | Pulumi                                         | Terraform                                            |
|---------------------------------|-----------------------------------------------|------------------------------------------------|------------------------------------------------------|
| **Infrastructure Provisioning** | ‚úÖ **Yes (but high-level, via `server.yaml`)** | ‚úÖ **Yes (fine-grained IaC)**                   | ‚úÖ **Yes (fine-grained IaC)**                         |
| **Supports Multi-Cloud**        | ‚úÖ **Yes**                                     | ‚úÖ **Yes**                                      | ‚úÖ **Yes**                                            |
| **State Management**            | ‚úÖ **Built-in (S3, GCS, FS)**                  | ‚úÖ **Built-in via Pulumi Cloud, S3, GCS, etc.** | ‚úÖ **State stored in Terraform Cloud, S3, GCS, etc.** |

‚úÖ **SC abstracts away infrastructure details, while Pulumi and Terraform provide full control.**

---

## **üîπ Microservice Deployment**

| Feature                                   | Simple Container                | Pulumi    | Terraform                                  |
|-------------------------------------------|---------------------------------|-----------|--------------------------------------------|
| **Deploy to Kubernetes (GKE, EKS, AKS)**  | ‚úÖ **Yes (via `cloud-compose`)** | ‚úÖ **Yes** | ‚ùå **Limited (needs Helm or K8S provider)** |
| **Deploy to AWS ECS Fargate**             | ‚úÖ **Yes**                       | ‚úÖ **Yes** | ‚úÖ **Yes**                                  |
| **Deploy to AWS Lambda / GCP Cloud Run**  | ‚úÖ **Yes (via `single-image`)**  | ‚úÖ **Yes** | ‚úÖ **Yes**                                  |
| **Deploy Static Websites (S3, GCS, CDN)** | ‚úÖ **Yes (via `static`)**        | ‚úÖ **Yes** | ‚úÖ **Yes**                                  |

‚úÖ **SC is **built for deployment**, while Pulumi and Terraform require manual setup for CI/CD.**

---

## **üîπ Secrets Management**

| Feature                         | Simple Container                         | Pulumi                                  | Terraform                                               |
|---------------------------------|------------------------------------------|-----------------------------------------|---------------------------------------------------------|
| **Built-in Secrets Management** | ‚úÖ **Yes (via SSH-based encryption)**     | ‚úÖ **Yes (via Pulumi Secrets Provider)** | ‚ùå **No (requires Vault, SOPS, or AWS Secrets Manager)** |
| **Secure Secret Sharing**       | ‚úÖ **Yes**                                | ‚úÖ **Yes**                               | ‚ùå **No (manual setup required)**                        |
| **Encryption Support**          | ‚úÖ **Yes (AWS KMS, GCP KMS, passphrase)** | ‚úÖ **Yes (KMS, GCP, HashiCorp Vault)**   | ‚ùå **No (requires external tools)**                      |

‚úÖ **SC has built-in secrets management, whereas Terraform needs external tools.**

---

## **üîπ CI/CD Automation**

| Feature                       | Simple Container            | Pulumi                                      | Terraform                                   |
|-------------------------------|-----------------------------|---------------------------------------------|---------------------------------------------|
| **Built-in CI/CD Deployment** | ‚úÖ **Yes (via `sc deploy`)** | ‚ùå **No (requires separate CI/CD pipeline)** | ‚ùå **No (requires separate CI/CD pipeline)** |
| **Easy Rollbacks**            | ‚úÖ **Yes**                   | ‚ùå **No (manual rollback required)**         | ‚ùå **No (manual rollback required)**         |

‚úÖ **SC is designed for CI/CD, while Pulumi and Terraform rely on external automation.**

---

# **3Ô∏è‚É£ When to Choose Which Tool?**

‚úÖ **Use Simple Container if:**

- You need **easy CI/CD for microservices**.
- You want **a simple YAML-based approach** for cloud deployments.
- You require **built-in secrets management**.
- You need **fast microservice deployments to Kubernetes, ECS, or Lambda**.

‚úÖ **Use Pulumi if:**

- You need **full Infrastructure as Code (IaC) with a programming language**.
- You want **strong integration with AWS, GCP, Azure**.
- You prefer **fine-grained control over infrastructure**.

‚úÖ **Use Terraform if:**

- You need **full Infrastructure as Code (IaC) with a declarative approach**.
- You want **multi-cloud infrastructure provisioning**.
- You prefer **HCL over programming languages**.

---

# **4Ô∏è‚É£ Detailed Scaling Comparison**

## **Infrastructure Management Complexity**

| Aspect                                | Terraform/Pulumi                   | Simple Container               | Advantage                  |
|---------------------------------------|------------------------------------|--------------------------------|----------------------------|
| **Configuration Lines**               | 5000+ lines for 100 customers      | 500 lines for 100 customers    | **90% reduction**          |
| **Infrastructure Knowledge Required** | Deep cloud expertise needed        | Business logic focus only      | **Developer self-service** |
| **Multi-Tenant Setup**                | Manual per-customer infrastructure | Built-in parentEnv inheritance | **Automatic isolation**    |
| **Secret Management**                 | Manual per-environment setup       | Built-in ${secret:} + ${env:}  | **Unified approach**       |
| **Deployment Complexity**             | Separate Terraform + K8s manifests | Single SC configuration        | **Single source of truth** |

## **Operational Scalability**

| Metric                         | Terraform/Pulumi              | Simple Container           | Improvement        |
|--------------------------------|-------------------------------|----------------------------|--------------------|
| **DevOps to Customer Ratio**   | 1:10-20 customers             | 1:100+ customers           | **5x efficiency**  |
| **Customer Onboarding Time**   | 2-3 days                      | 5 minutes                  | **500x faster**    |
| **Infrastructure Drift Risk**  | High (manual management)      | Low (template-based)       | **Reduced errors** |
| **Cross-Region Deployment**    | Duplicate infrastructure code | Single parent stack change | **DRY principle**  |
| **Performance Tier Migration** | Manual infrastructure rebuild | One-line uses directive    | **Zero downtime**  |


## **Developer Experience**

| Feature                     | Terraform/Pulumi         | Simple Container        | Benefit                   |
|-----------------------------|--------------------------|-------------------------|---------------------------|
| **Learning Curve**          | Months (cloud + IaC)     | Days (business config)  | **Faster onboarding**     |
| **Deployment Autonomy**     | Requires DevOps approval | Self-service deployment | **Independent teams**     |
| **Environment Consistency** | Manual synchronization   | Automatic inheritance   | **Reduced bugs**          |
| **Resource Allocation**     | Complex calculations     | Simple uses directive   | **Simplified management** |
| **Scaling Configuration**   | Multiple files/tools     | Single scale block      | **Unified interface**     |

## **Cost and Resource Efficiency**

| Factor                      | Terraform/Pulumi        | Simple Container        | Savings                     |
|-----------------------------|-------------------------|-------------------------|-----------------------------|
| **Infrastructure Overhead** | Per-customer resources  | Shared resource pools   | **70% cost reduction**      |
| **Operational Staff**       | High DevOps requirement | Minimal DevOps overhead | **80% staff reduction**     |
| **Resource Utilization**    | Often over-provisioned  | Right-sized sharing     | **Better efficiency**       |
| **Maintenance Burden**      | Continuous per-customer | Template updates only   | **Centralized maintenance** |
| **Monitoring Complexity**   | Per-customer setup      | Built-in observability  | **Reduced tooling costs**   |

## **Summary Table**

| Feature                               | Simple Container            | Pulumi                     | Terraform                        |
|---------------------------------------|-----------------------------|----------------------------|----------------------------------|
| **Best Use Case**                     | Microservice deployment     | Full IaC with programming  | Full IaC with declarative config |
| **Configuration Language**            | YAML                        | TypeScript, Python, Go, C# | HCL                              |
| **Infrastructure Provisioning**       | Limited (via `server.yaml`) | ‚úÖ Yes                      | ‚úÖ Yes                            |
| **Cloud Provider Support**            | AWS, GCP, Kubernetes        | AWS, GCP, Azure, K8s       | AWS, GCP, Azure, K8s             |
| **Secrets Management**                | ‚úÖ Built-in                  | ‚úÖ Built-in                 | ‚ùå External required              |
| **CI/CD Built-in**                    | ‚úÖ Yes                       | ‚ùå No                       | ‚ùå No                             |
| **State Management**                  | ‚úÖ Yes                       | ‚úÖ Yes                      | ‚úÖ Yes                            |
| **Automated Microservice Deployment** | ‚úÖ Yes                       | ‚ùå No                       | ‚ùå No                             |
| **Scaling Efficiency**                | **5x DevOps efficiency**    | Manual scaling required    | Manual scaling required          |
| **Customer Onboarding**               | **5 minutes**               | Days to weeks              | Days to weeks                    |

---

# **5Ô∏è‚É£ Real-World Scaling Scenarios**

## **Scenario 1: Adding 100 New Customers**

**Terraform/Pulumi Approach:**
```bash
# For each of 100 customers, DevOps must:
1. Create separate infrastructure definitions
2. Configure networking, security, monitoring
3. Set up customer-specific resources
4. Manual secret management

# Result: 5000+ lines of configuration
# Time: 2-3 days per customer = 200-300 days
# Team: Requires DevOps expertise for each deployment
```

**Simple Container Approach:**
```yaml
# DevOps defines infrastructure once (already done)

# For each of 100 customers, developers add:
customer-001:
  parentEnv: production
  config:
    domain: customer001.myapp.com
    secrets:
      CUSTOMER_SETTINGS: ${env:CUSTOMER_001_SETTINGS}

# Result: 5 lines per customer = 500 lines total
# Time: 5 minutes per customer = 8.3 hours total
# Team: Developers can self-serve, no DevOps bottleneck
```

## **Scenario 2: Multi-Region Expansion**

**Traditional Approach:**
```typescript
// Duplicate entire infrastructure for each region
const usEastCluster = new aws.ecs.Cluster("us-east-cluster");
const usWestCluster = new aws.ecs.Cluster("us-west-cluster");
const euWestCluster = new aws.ecs.Cluster("eu-west-cluster");

// Duplicate networking, security, monitoring for each region
// Manually manage customer allocation across regions
```

**Simple Container:**
```yaml
# .sc/stacks/myapp-us/server.yaml
resources:
  prod:
    resources:
      mongodb-us: { region: us-east-1 }
      
# .sc/stacks/myapp-eu/server.yaml  
resources:
  prod:
    resources:
      mongodb-eu: { region: eu-west-1 }

# client.yaml - Customers choose regions easily
us-customer:
  parent: simple-container/myapp-us
  parentEnv: prod
  
eu-customer:
  parent: simple-container/myapp-eu
  parentEnv: prod
```

# **6Ô∏è‚É£ Conclusion**

- **Use Simple Container** (`sc`) for **fast microservice deployments with built-in CI/CD and superior scaling**.
- **Use Pulumi** if you need **fine-grained control over cloud resources with imperative programming**.
- **Use Terraform** if you need **declarative IaC for provisioning and managing cloud infrastructure**.

**For organizations scaling microservices, Simple Container provides:**

- **500x faster customer onboarding**
- **90% reduction in configuration complexity**
- **70% cost reduction through resource sharing**
- **5x operational efficiency improvement**

# **Migrating from Terraform or Pulumi to Simple Container: Key Benefits**

Migrating from **Terraform or Pulumi** to **Simple Container** offers a **simplified approach to microservice deployment** 
while maintaining **cloud provider flexibility**. 

Unlike Terraform and Pulumi, which require **manual infrastructure provisioning and CI/CD setup**, `sc` **automates deployments** and abstracts infrastructure complexity.

---

# **1Ô∏è‚É£ Key Benefits of Migrating to `sc`**

## **üîπ 1. Developers Can Add Microservices Easily (Without DevOps Involvement)**

‚úÖ **No need to manually configure cloud resources for each microservice**.

‚úÖ **No Terraform modules or Pulumi scripts required for every new service**.

‚úÖ **Developers only define a `client.yaml`** alongside familiar tools like **Dockerfile & docker-compose**.

**üîπ Example: Adding a New Microservice (`myservice`)**
Instead of modifying Terraform or Pulumi configurations, **developers only create a simple `client.yaml`**:

```yaml
---
# File: ".sc/stacks/myservice/client.yaml"

schemaVersion: 1.0

stacks:
  staging:
    type: cloud-compose
    parent: myproject/devops
    config:
      dockerComposeFile: ./docker-compose.yaml  # Familiar tooling
      uses:
        - mongodb  # No need to define database again (DevOps-managed)
      runs:
        - myservice
      env:
        DATABASE_HOST: "${resource:mongodb.host}"
        DATABASE_USER: "${resource:mongodb.user}"
      secrets:
        DATABASE_PASSWORD: "${resource:mongodb.password}"
```

‚úÖ **With Terraform/Pulumi, adding a microservice means modifying infrastructure code. With `sc`, developers just create `client.yaml`.**

**üîπ Deployment Command (No Separate CI/CD Setup Required)**
```sh
sc deploy -s myservice -e staging
```
‚úÖ **No need to define ECS tasks or Kubernetes manifests‚Äî`sc` handles everything.**

---

## **üîπ 2. DevOps Focuses Only on Core Infrastructure (No Need to Manage Each Microservice)**

‚úÖ **Single source of truth for infrastructure (`server.yaml`)**.

‚úÖ **No need to update Terraform or Pulumi for every new microservice**.

‚úÖ **Developers are isolated from cloud infrastructure complexities**.

**üîπ Example: DevOps Defines Infrastructure Once (`server.yaml`)**
```yaml
---
# File: ".sc/stacks/devops/server.yaml"

schemaVersion: 1.0

provisioner:
  type: pulumi
  config:
    state-storage:
      type: s3-bucket
      config:
        credentials: "${auth:aws}"
        bucketName: myproject-sc-state

resources:
  staging:
    template: stack-per-app
    resources:
      mongodb:
        type: mongodb-atlas
        config:
          admins: [ "admin" ]
          instanceSize: "M10"
          region: "US_WEST_2"
          cloudProvider: AWS
          privateKey: "${secret:MONGODB_ATLAS_PRIVATE_KEY}"
          publicKey: "${secret:MONGODB_ATLAS_PUBLIC_KEY}"
```
‚úÖ **Once this is set up, DevOps never needs to modify it for new microservices.**

---

# **2Ô∏è‚É£ Why `sc` Is Better for Microservices Deployment?**
| Feature                             | Terraform / Pulumi                        | Simple Container                       |
|-------------------------------------|-------------------------------------------|----------------------------------------|
| **Adding a New Microservice**       | ‚ùå Requires Terraform/Pulumi modifications | ‚úÖ Developers only create `client.yaml` |
| **Cloud Infrastructure Management** | ‚úÖ Yes                                     | ‚úÖ Yes (but DevOps-only)                |
| **Microservice Deployment**         | ‚ùå Requires CI/CD setup                    | ‚úÖ Built-in (`sc deploy`)               |
| **Secrets Management**              | ‚ùå External tools required                 | ‚úÖ Built-in (`sc secrets`)              |
| **Multi-Cloud Support**             | ‚úÖ Yes                                     | ‚úÖ Yes                                  |

---

# **3Ô∏è‚É£ Summary**

‚úÖ **Developers can add microservices easily** without modifying infrastructure code.

‚úÖ **DevOps only manages core cloud resources** in a centralized `server.yaml`.

‚úÖ **Secrets and deployments are automated**, reducing manual work.

‚úÖ **No Terraform/Pulumi modifications needed for each new service**.

---

## Best Practices

# Best Practices

This guide covers best practices for using Simple Container effectively in production environments.

## Configuration Management

### Template Organization

- **Use descriptive template names** that clearly indicate their purpose
- **Group related templates** in logical sections within your configuration
- **Maintain consistent naming conventions** across environments

```yaml
templates:
  # Good: Clear, descriptive names
  api-backend-service:
    type: ecs-fargate
    config:
      credentials: "${auth:aws}"
      account: "${auth:aws.projectId}"
  
  # Avoid: Generic or unclear names
  service1:
    type: ecs-fargate
```

### Environment Management

- **Use environment-specific resource naming** to avoid conflicts
- **Leverage environment variables** for configuration that changes between environments
- **Implement proper secret management** for sensitive data

```yaml
resources:
  resources:
    prod:
      api-db:
        name: "myapp-prod-database"
        instanceClass: "db.r5.large"
    staging:
      api-db:
        name: "myapp-staging-database" 
        instanceClass: "db.t3.medium"
```

## Security Best Practices

### Secret Management

- **Never commit secrets to version control**
- **Use Simple Container's secret management** for all sensitive data
- **Always add secrets.yaml to SC managed secrets** using `sc secrets add .sc/stacks/<parent>/secrets.yaml`
- **secrets.yaml contains ONLY exact literal values** - NO placeholders or variables are processed
- **Never use environment variable placeholders** in secrets.yaml (e.g., `${env:AWS_ACCESS_KEY}` will NOT work)
- **Rotate secrets regularly** and update configurations accordingly
- **Use least-privilege access** for all cloud resources

#### Proper secrets.yaml Workflow

1. **Create secrets.yaml with exact values:**
   ```yaml
   # .sc/stacks/devops/secrets.yaml
   schemaVersion: 1.0
   auth:
     aws-account:
       type: aws-token
       config:
         account: "123456789012"
         accessKey: "AKIAIOSFODNN7EXAMPLE"  # Exact literal value
         secretAccessKey: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
         region: us-east-1
   values:
     DATABASE_PASSWORD: "mySecurePassword123"
     API_KEY: "sk-1234567890abcdef"
   ```

2. **Add to Simple Container's managed secrets:**
   ```bash
   sc secrets add .sc/stacks/devops/secrets.yaml
   ```

3. **Never commit the raw secrets.yaml file to version control** - only the encrypted version should be committed

### Network Security

- **Configure security groups** to allow only necessary traffic
- **Use private subnets** for databases and internal services
- **Implement proper CORS policies** for web applications
- **Enable HTTPS/TLS** for all public-facing services

## Scaling and Performance

### Resource Allocation

- **Start with conservative resource allocations** and scale up based on monitoring
- **Use auto-scaling policies** for variable workloads
- **Monitor resource utilization** and adjust configurations accordingly

### Database Optimization

- **Use read replicas** for read-heavy workloads
- **Implement connection pooling** for database connections
- **Configure appropriate backup retention** policies
- **Monitor database performance** metrics

## Deployment Strategies

### Blue-Green Deployments

- **Use separate environments** for blue-green deployments
- **Implement health checks** to verify deployment success
- **Plan rollback strategies** for failed deployments

### Gradual Rollouts

- **Deploy to staging first** before production
- **Use feature flags** for gradual feature rollouts
- **Monitor application metrics** during deployments

## Monitoring and Observability

### Logging

- **Implement structured logging** across all services
- **Use consistent log formats** for easier parsing
- **Set up log aggregation** for centralized monitoring
- **Configure appropriate log retention** policies

### Metrics and Alerting

- **Define key performance indicators** (KPIs) for your applications
- **Set up proactive alerting** for critical issues
- **Monitor both infrastructure and application metrics**
- **Create dashboards** for operational visibility

## Cost Optimization

### Resource Right-Sizing

- **Regularly review resource utilization** and adjust instance sizes
- **Use spot instances** where appropriate for cost savings
- **Implement auto-scaling** to avoid over-provisioning
- **Clean up unused resources** regularly

### Storage Optimization

- **Use appropriate storage classes** for different data access patterns
- **Implement lifecycle policies** for data archival
- **Monitor storage costs** and optimize accordingly

## Development Workflow

### Version Control

- **Use semantic versioning** for your configurations
- **Maintain separate branches** for different environments
- **Implement code review processes** for configuration changes
- **Tag releases** for easy rollback capabilities

### Testing

- **Test configurations** in staging environments before production
- **Implement automated testing** for critical deployment paths
- **Validate resource configurations** before deployment
- **Use infrastructure testing tools** where appropriate

## Troubleshooting

### Common Issues

- **Resource naming conflicts** - Use environment-specific prefixes
- **Permission issues** - Verify IAM roles and policies
- **Network connectivity** - Check security groups and routing
- **Secret access** - Ensure proper secret configuration

### Debugging Tools

- **Use Simple Container's status commands** to check deployment state
- **Enable verbose logging** for detailed troubleshooting
- **Check cloud provider logs** for infrastructure issues
- **Monitor application logs** for service-specific problems

## Team Collaboration

### Documentation

- **Document your architecture decisions** and configuration choices
- **Maintain up-to-date deployment guides** for your team
- **Create runbooks** for common operational tasks
- **Share knowledge** through internal documentation

### Access Management

- **Implement role-based access control** for different team members
- **Use separate credentials** for different environments
- **Regularly audit access permissions** and remove unused accounts
- **Implement approval processes** for production changes

## Compliance and Governance

### Audit Trail

- **Maintain logs** of all configuration changes
- **Implement approval workflows** for sensitive changes
- **Use version control** for all configuration files
- **Regular compliance reviews** of your infrastructure

### Data Protection

- **Implement data encryption** at rest and in transit
- **Configure proper backup strategies** for critical data
- **Ensure compliance** with relevant regulations (GDPR, HIPAA, etc.)
- **Regular security assessments** of your infrastructure

## Performance Optimization

### Application Performance

- **Use CDNs** for static content delivery
- **Implement caching strategies** at multiple layers
- **Optimize database queries** and indexing
- **Monitor application response times** and optimize bottlenecks

### Infrastructure Performance

- **Choose appropriate instance types** for your workloads
- **Use placement groups** for high-performance computing
- **Optimize network configurations** for low latency
- **Monitor infrastructure metrics** and adjust accordingly


---

# AI Assistant

## AI Assistant Overview

# AI Assistant

Simple Container's AI-powered project onboarding assistant provides intelligent setup and configuration guidance with two distinct modes aligned with our separation of concerns philosophy.

## üéØ Two-Mode Architecture

### **Developer Mode** üßë‚Äçüíª
**For Application Teams**
- **Purpose**: Set up application-specific configurations
- **Generates**: `client.yaml`, `docker-compose.yaml`, `Dockerfile`
- **Analysis**: Automatic project tech stack detection
- **Focus**: Application deployment, scaling, dependencies

### **DevOps Mode** üõ†Ô∏è
**For Infrastructure Teams**  
- **Purpose**: Set up shared infrastructure and resources
- **Generates**: `server.yaml`, `secrets.yaml`, provisioner config
- **Analysis**: Cloud provider selection and resource planning
- **Focus**: Shared resources, templates, infrastructure management

## üöÄ Quick Start

### For Developers
```bash
# Analyze current project and generate application configs
sc assistant dev setup

# Interactive mode with guided setup
sc assistant dev setup --interactive

# Search documentation
sc assistant search "docker compose with postgres"
```

### For DevOps Teams
```bash
# Set up infrastructure for a new project
sc assistant devops setup

# Configure cloud provider and shared resources
sc assistant devops setup --cloud aws --interactive

# Start MCP server for external tools
sc assistant mcp --port 9999
```

## üìã Available Commands

| Command | Mode | Description |
|---------|------|-------------|
| `sc assistant dev setup` | Developer | Generate client.yaml and compose files |
| `sc assistant dev analyze` | Developer | Analyze project tech stack |
| `sc assistant devops setup` | DevOps | Configure server.yaml and secrets |
| `sc assistant devops resources` | DevOps | Manage shared resources |
| `sc assistant search` | Both | Semantic documentation search |
| `sc assistant chat` | Both | Interactive assistant (Phase 3) |
| `sc assistant mcp` | Both | Start MCP server for external tools |

## üé≠ Mode Comparison

| Aspect | Developer Mode | DevOps Mode |
|--------|----------------|-------------|
| **Target Users** | Application developers | Infrastructure teams |
| **Project Analysis** | ‚úÖ Full tech stack detection | ‚ùå Not needed |
| **Cloud Selection** | ‚ö™ Uses existing infrastructure | ‚úÖ Primary decision point |
| **File Generation** | `client.yaml`, `docker-compose.yaml` | `server.yaml`, `secrets.yaml` |
| **Resource Focus** | Application dependencies | Shared infrastructure |
| **Complexity** | Higher (project analysis) | Lower (guided selection) |

## üîÑ Typical Workflow

### 1. DevOps Team Setup (First)
```bash
# DevOps sets up shared infrastructure
sc assistant devops setup --cloud aws
# Generates: server.yaml, secrets.yaml
# Creates: Database, storage, networking resources
```

### 2. Developer Team Setup (Second)
```bash  
# Developers set up their applications
sc assistant dev setup
# Generates: client.yaml, docker-compose.yaml  
# References: Parent resources from DevOps team
```

## üåü Key Features

### **Intelligent Project Analysis** (Developer Mode)
- **Language Detection**: Node.js, Python, Go, Java, etc.
- **Framework Recognition**: Express, Django, Gin, Spring, etc.
- **Dependency Analysis**: Database, cache, messaging requirements
- **Architecture Patterns**: Microservice, monolith, serverless detection

### **Cloud Provider Integration** (DevOps Mode)
- **AWS**: ECS Fargate, RDS, S3, ECR, Lambda
- **GCP**: GKE Autopilot, Cloud SQL, Cloud Storage, Cloud Run
- **Azure**: Container Apps, PostgreSQL, Blob Storage (Phase 2)
- **Kubernetes**: Native deployments, Helm operators

### **Semantic Documentation Search** (Both Modes)
- **10,000+ Documents**: Indexed docs, examples, schemas
- **Sub-100ms Search**: Fast semantic similarity matching
- **Context Aware**: Results tailored to current project type
- **Multi-Source**: Documentation, examples, JSON schemas

## üìö Documentation Structure

- **[Getting Started](getting-started.md)** - Quick setup for both modes
- **[Developer Mode](developer-mode.md)** - Application team workflows
- **[DevOps Mode](devops-mode.md)** - Infrastructure team workflows  
- **[MCP Integration](mcp-integration.md)** - External tool integration
- **[Commands Reference](commands.md)** - Complete command documentation
- **[Examples](examples/)** - Real-world usage scenarios
- **[Troubleshooting](troubleshooting.md)** - Common issues and solutions

## üîß Integration Examples

### Windsurf IDE Integration
```json
{
  "tools": [{
    "name": "simple-container-assistant",
    "type": "mcp",
    "endpoint": "http://localhost:9999/mcp"
  }]
}
```

### VS Code Integration  
```json
{
  "simple-container.assistant.mode": "developer",
  "simple-container.assistant.autoAnalyze": true
}
```

## üéØ Success Metrics

- **Setup Time**: From 30+ minutes to under 5 minutes
- **Configuration Accuracy**: 95%+ generated configs work without modification  
- **User Adoption**: Target 80%+ of new users use assistant for initial setup
- **Documentation Discovery**: 90%+ accuracy in finding relevant docs

## üöÄ Roadmap

### **Phase 1** ‚úÖ - Foundation (Complete)
- Documentation embedding system
- MCP server implementation  
- Semantic search capabilities
- CLI command structure

### **Phase 2** üîÑ - Analysis & Generation (In Progress)
- Project analysis engine
- Two-mode architecture implementation
- File generation system
- Cloud provider templates

### **Phase 3** üìã - Interactive Experience
- Chat interface implementation
- LLM integration (langchaingo)
- Conversational project setup
- Advanced context management

### **Phase 4** üèÅ - Enterprise Features
- Advanced analytics and insights
- Team collaboration features
- Custom template creation
- Enterprise security and compliance

Ready to get started? Choose your path:
- üßë‚Äçüíª **[Developer Mode Setup ‚Üí](developer-mode.md)**
- üõ†Ô∏è **[DevOps Mode Setup ‚Üí](devops-mode.md)**


---

## Getting Started

# Getting Started with AI Assistant

Get up and running with Simple Container's AI Assistant in under 10 minutes. This guide covers both **Developer** and **DevOps** workflows.

## üéØ Prerequisites

### System Requirements
- Simple Container CLI installed (`sc` command available)
- Docker installed (for local development)
- Cloud provider account (AWS, GCP, or Kubernetes cluster)

### Installation Check
```bash
# Verify Simple Container is installed
sc --version
# Should output: Simple Container v1.x.x

# Check if AI assistant is available
sc assistant --help
# Should show assistant subcommands

# Test documentation search
sc assistant search "getting started"
```

## üé≠ Choose Your Role

Simple Container AI Assistant has two distinct modes based on your team role:

<div style="display: flex; gap: 20px; margin: 20px 0;">

<div style="flex: 1; border: 2px solid #0066cc; border-radius: 8px; padding: 20px;">
<h3>üë©‚Äçüíª I'm a Developer</h3>
<p><strong>You build applications</strong></p>
<ul>
<li>Work with existing infrastructure</li>
<li>Need client.yaml and docker-compose files</li>
<li>Focus on application deployment</li>
<li>Use shared resources from DevOps team</li>
</ul>
<p><a href="#developer-setup">‚Üí Developer Setup Guide</a></p>
</div>

<div style="flex: 1; border: 2px solid #cc6600; border-radius: 8px; padding: 20px;">
<h3>üîß I'm DevOps/Infrastructure</h3>
<p><strong>You manage shared infrastructure</strong></p>
<ul>
<li>Set up cloud resources and databases</li>
<li>Need server.yaml and secrets.yaml files</li>
<li>Create templates for dev teams</li>
<li>Manage environments and security</li>
</ul>
<p><a href="#devops-setup">‚Üí DevOps Setup Guide</a></p>
</div>

</div>

## üîß DevOps Setup

**DevOps teams should set up infrastructure FIRST** before developers can deploy applications.

### Step 1: Initialize Infrastructure Project
```bash
# Create infrastructure directory
mkdir mycompany-infrastructure
cd mycompany-infrastructure

# Initialize Simple Container
sc init
```

### Step 2: Run Infrastructure Wizard
```bash
# Start interactive setup wizard
sc assistant devops setup --interactive
```

The wizard will guide you through:

#### **Cloud Provider Selection**
```
üåê Select your primary cloud provider:
1. AWS (Recommended for most teams)
2. GCP (Google Cloud Platform) 
3. Kubernetes (Cloud-agnostic)

Choice [1-3]: 1 ‚úÖ
```

#### **Environment Configuration**
```
üìä Configure environments:
‚úÖ Development (docker-compose locally)
‚úÖ Staging (cloud resources, cost-optimized)
‚úÖ Production (cloud resources, high availability)

Additional environments (preview, testing)? (y/n): n
```

#### **Resource Selection**
```
üéØ Select shared resources:

Databases:
‚òëÔ∏è PostgreSQL (recommended for most apps)
‚òê MongoDB Atlas (document database)
‚òê MySQL (legacy compatibility)
‚òëÔ∏è Redis (caching and sessions)

Storage:
‚òëÔ∏è S3-compatible bucket (file uploads)
‚òê CDN (static asset delivery)

Compute:
‚òëÔ∏è ECS Fargate (containerized applications)
‚òê Lambda (serverless functions)

Continue? (y/n): y ‚úÖ
```

### Step 3: Review Generated Files
```bash
# Check generated infrastructure files
ls -la .sc/stacks/infrastructure/
# server.yaml    - Infrastructure resources and templates  
# secrets.yaml   - Authentication and sensitive configuration

# Review server configuration
cat .sc/stacks/infrastructure/server.yaml
```

### Step 4: Configure Secrets
```bash
# Add cloud provider credentials
sc secrets add aws-access-key
# Enter value: AKIA...

sc secrets add aws-secret-key  
# Enter value: secret-key...

sc secrets add staging-db-password
# Enter value: secure-staging-password

sc secrets add prod-db-password
# Enter value: ultra-secure-production-password
```

### Step 5: Deploy Infrastructure
```bash
# Deploy staging environment
sc provision -s infrastructure -e staging

# Deploy production environment
sc provision -s infrastructure -e production

# Verify deployment by checking configuration files
cat .sc/stacks/infrastructure/server.yaml | grep -A 10 "resources:"
```

### Step 6: Share with Development Teams
```bash
# Review resource information for developers
echo "Available resources for developers:"
cat .sc/stacks/infrastructure/server.yaml

# Share this information with your development teams:
# - Available resources (databases, cache, storage)
# - Parent stack name: "infrastructure"  
# - Environment names: "staging", "production"
```

## üë©‚Äçüíª Developer Setup

**Developers use existing infrastructure** set up by the DevOps team.

### Step 1: Navigate to Your Application
```bash
# Go to your application directory
cd my-awesome-app

# Verify this is a valid project directory
ls -la
# Should see package.json, requirements.txt, go.mod, or similar
```

### Step 2: Analyze Your Project
```bash
# Let AI assistant analyze your project
sc assistant dev analyze

# Example output:
# üîç Analyzing project at: .
# 
# üéØ Analysis Results:
#    Language: Node.js (v18.x)
#    Framework: Express.js  
#    Architecture: REST API
#    Dependencies: PostgreSQL, Redis detected
#    Confidence: 92%
#
# üìã Recommendations:
#    ‚úÖ Use PostgreSQL database resource
#    ‚úÖ Use Redis cache resource  
#    ‚úÖ Use ECS Fargate template
#    ‚úÖ Configure health check endpoint
```

### Step 3: Generate Application Configuration
```bash
# Generate client.yaml and docker-compose.yaml
sc assistant dev setup

# Interactive mode for customization
sc assistant dev setup --interactive
```

### Step 4: Review Generated Files
```bash
# Check generated files
ls -la
# client.yaml           - Simple Container app configuration
# docker-compose.yaml   - Local development environment
# Dockerfile            - Container image definition (if needed)

# Review application configuration  
cat .sc/stacks/my-awesome-app/client.yaml
```

### Step 5: Set Up Local Development
```bash
# Start local development environment
docker-compose up -d

# Check services are running
docker-compose ps
# Should show your app + database + redis containers

# Run your application
npm start  # or python manage.py runserver, go run main.go
```

### Step 6: Deploy to Staging
```bash
# Deploy to staging environment  
sc deploy -s my-awesome-app -e staging

# Verify deployment is working
curl https://staging-api.yourcompany.com/health

# Check application logs via Docker (if needed)
docker logs my-awesome-app_app_1
```

### Step 7: Deploy to Production
```bash
# Deploy to production environment
sc deploy -s my-awesome-app -e production

# Scaling is configured in client.yaml config.scale section
# Edit .sc/stacks/my-awesome-app/client.yaml to update scaling

# Verify production deployment is working  
curl https://api.yourcompany.com/health
```

## üîÑ Complete Workflow Example

Here's how DevOps and Developer teams work together:

### 1. **DevOps Team** - Infrastructure Setup
```bash
# 1. DevOps creates infrastructure project
mkdir acme-infrastructure && cd acme-infrastructure
sc init

# 2. Set up shared resources  
sc assistant devops setup --cloud aws --interactive

# 3. Configure secrets and deploy
sc secrets add aws-access-key aws-secret-key
sc provision -s infrastructure -e staging
sc provision -s infrastructure -e production

# 4. Share resource info with developers
echo "Infrastructure ready! Developers can reference:"
echo "- Parent: infrastructure"  
echo "- Resources: postgres-db, redis-cache, uploads-bucket"
echo "- Environments: staging, production"
```

### 2. **Developer Team** - Application Setup
```bash
# 1. Developer works on their app
cd my-express-api
sc assistant dev analyze
# Detects: Node.js + Express + PostgreSQL + Redis

# 2. Generate application configs
sc assistant dev setup
# Creates: client.yaml, docker-compose.yaml, Dockerfile

# 3. Local development
docker-compose up -d  # Starts local db + redis
npm run dev           # Starts application

# 4. Deploy to staging
sc deploy -s my-awesome-app -e staging
# Uses shared staging database and cache

# 5. Deploy to production  
sc deploy -s my-awesome-app -e production
# Uses shared production database and cache
```

## üéØ Common Scenarios

### **Scenario 1: New Company Setup**
```bash
# Step 1: DevOps sets up foundation
sc assistant devops setup --cloud aws --envs staging,production
sc provision -s infrastructure -e staging,production

# Step 2: Multiple dev teams deploy apps
cd team-a/api-service && sc assistant dev setup && sc deploy -s api-service -e staging
cd team-b/web-app && sc assistant dev setup && sc deploy -s web-app -e staging  
cd team-c/worker && sc assistant dev setup && sc deploy -s worker -e staging
```

### **Scenario 2: Adding New Environment**
```bash
# DevOps adds preview environment
sc assistant devops resources --env preview --copy-from staging
sc provision -s infrastructure -e preview

# Developers can now deploy to preview
sc deploy -s myapp -e preview
```

### **Scenario 3: New Resource Required**
```bash
# DevOps adds MongoDB to existing infrastructure
sc assistant devops resources --add mongodb --env staging,production
sc provision -s infrastructure -e staging,production

# Developers update their apps to use MongoDB
sc assistant dev setup --update --add-resource mongodb
sc deploy -s myapp -e staging
```

## üîç Validation Steps

### **DevOps Validation**
```bash
# Verify infrastructure deployment by checking files
ls -la .sc/stacks/infrastructure/

# Verify infrastructure configuration
cat .sc/stacks/infrastructure/server.yaml | grep -A 5 "resources:"

# Verify secrets are properly configured
sc secrets list
```

### **Developer Validation**  
```bash
# Verify local development works
docker-compose up -d
curl http://localhost:3000/health

# Verify staging deployment works
sc deploy -s myapp -e staging  
curl https://staging-api.mycompany.com/health

# Verify production deployment works
sc deploy -s myapp -e production
curl https://api.mycompany.com/health
```

## ‚ùó Common Issues

### **DevOps Issues**
```bash
# Issue: Cloud credentials not working
# Solution: Re-add credentials with correct permissions
sc secrets add aws-access-key --overwrite

# Issue: Resource naming conflicts  
# Solution: Use unique prefixes
sc assistant devops setup --prefix mycompany

# Issue: Environment isolation problems
# Solution: Verify separate resource names per environment
```

### **Developer Issues**
```bash
# Issue: Parent stack not found
# Solution: Ensure DevOps has deployed infrastructure first
ls -la .sc/stacks/  # Should show "infrastructure" directory

# Issue: Resource references not working
# Solution: Check resource names match server.yaml
sc assistant search "template placeholders"

# Issue: Local development not working
# Solution: Check docker-compose services are running
docker-compose ps
docker-compose logs
```

## üöÄ Next Steps

### **For DevOps Teams**
1. **[Advanced DevOps Configuration ‚Üí](devops-mode.md)**
2. **[Multi-cloud Setup ‚Üí](examples/multi-cloud.md)**
3. **[Monitoring and Alerting ‚Üí](examples/monitoring.md)**
4. **[Security Best Practices ‚Üí](examples/security.md)**

### **For Developer Teams**
1. **[Advanced Developer Workflows ‚Üí](developer-mode.md)**
2. **[Framework-specific Guides ‚Üí](examples/)**
3. **[Local Development Tips ‚Üí](examples/local-dev.md)**
4. **[Debugging and Troubleshooting ‚Üí](troubleshooting.md)**

### **For Both Teams**
1. **[MCP Integration with IDEs ‚Üí](mcp-integration.md)**
2. **[CI/CD Pipeline Setup ‚Üí](examples/cicd.md)**  
3. **[Performance Optimization ‚Üí](examples/performance.md)**
4. **[Cost Management ‚Üí](examples/cost-optimization.md)**

## üí¨ Getting Help

### **Documentation Search**
```bash
# Search for specific topics
sc assistant search "database configuration"
sc assistant search "scaling applications"  
sc assistant search "troubleshooting deployment"
```

### **Interactive Help**
```bash
# Start MCP server for IDE integration
sc assistant mcp --port 9999

# Use interactive chat (Phase 3)
sc assistant chat
```

### **Community Support**
- **[Simple Container Documentation](https://docs.simple-container.com)**
- **[GitHub Issues](https://github.com/simple-container-com/api/issues)**
- **[Community Slack](https://slack.simple-container.com)**
- **[Stack Overflow Tag: simple-container](https://stackoverflow.com/questions/tagged/simple-container)**


---

## Developer Mode

# Developer Mode

Developer Mode is designed for **application teams** who need to set up their applications to work with existing Simple Container infrastructure managed by DevOps teams.

## üéØ Overview

**Developer Mode Focus:**
- Generate `client.yaml` configurations for your applications
- Create optimized `docker-compose.yaml` for local development
- Generate `Dockerfile` based on detected tech stack
- Connect to shared resources provided by DevOps team
- Handle application-specific scaling and deployment settings

## üöÄ Quick Start

### Basic Project Setup
```bash
# Navigate to your project directory
cd my-awesome-app

# Auto-generate configurations based on project analysis
sc assistant dev setup

# Interactive mode for guided setup
sc assistant dev setup --interactive

# Specify target environment
sc assistant dev setup --env staging
```

### Analyze Your Project First
```bash
# Get detailed project analysis
sc assistant dev analyze

# Output example:
# üîç Analyzing project at: .
#
# üéØ Analysis Results:
#    Language: Node.js (18.x)
#    Framework: Express.js
#    Architecture: REST API
#    Dependencies: PostgreSQL, Redis
#    Recommendations: 3 resources, 2 optimizations
```

## üîç Project Analysis Engine

Developer Mode automatically detects:

### **Supported Languages & Frameworks**

| Language    | Frameworks Detected                                | Configuration Files                              |
|-------------|----------------------------------------------------|--------------------------------------------------|
| **Node.js** | Express, Koa, Fastify, NestJS, Next.js, React, Vue | `package.json`, `package-lock.json`              |
| **Python**  | Django, Flask, FastAPI, Tornado                    | `requirements.txt`, `setup.py`, `pyproject.toml` |
| **Go**      | Gin, Echo, Fiber, Gorilla/Mux                      | `go.mod`, `go.sum`                               |
| **Java**    | Spring Boot, Quarkus, Micronaut                    | `pom.xml`, `build.gradle`                        |
| **PHP**     | Laravel, Symfony, CodeIgniter                      | `composer.json`                                  |
| **Ruby**    | Rails, Sinatra, Hanami                             | `Gemfile`, `Gemfile.lock`                        |

### **Database Detection**
- **PostgreSQL**: `pg`, `psycopg2`, `sequelize`, `typeorm`
- **MongoDB**: `mongoose`, `pymongo`, `mongo-go-driver`
- **MySQL**: `mysql2`, `pymysql`, `gorm`
- **Redis**: `redis`, `ioredis`, `go-redis`
- **SQLite**: `sqlite3`, Development databases

### **Architecture Patterns**
- **Microservice**: Multiple service directories, docker-compose with multiple services
- **Monolith**: Single large application with MVC structure
- **Static Site**: HTML/CSS/JS, static site generators (Gatsby, Next.js export)
- **Serverless**: Lambda functions, Vercel/Netlify configs

## üìÅ Generated Files

### 1. **client.yaml** - Application Configuration
```yaml
schemaVersion: 1.0

stacks:
  my-awesome-app:
    type: cloud-compose
    parent: infrastructure  # References DevOps-managed resources
    parentEnv: staging
    config:
      dockerComposeFile: docker-compose.yaml  # REQUIRED: Reference to local compose file
      uses: [postgres-db, redis-cache]  # Shared resources from parent
      runs: [web-app, worker]  # Services from docker-compose
      domain: my-awesome-app.mycompany.com  # Optional: DNS domain (requires registrar)
      scale:
        min: 1
        max: 3
      env:
        NODE_ENV: production  # Non-sensitive environment variables only
        PORT: 3000
        # Database connections use auto-injected environment variables:
        # PostgreSQL: PGHOST, PGPORT, PGUSER, PGDATABASE, PGPASSWORD
        # Redis: REDIS_HOST, REDIS_PORT
      secrets:
        JWT_SECRET: "${secret:jwt-secret}"
        API_KEY: "${secret:third-party-api-key}"
        DATABASE_URL: "${secret:database-url}"  # Connection strings in secrets
        REDIS_URL: "${secret:redis-url}"        # Not in env section
```

### 2. **docker-compose.yaml** - Local Development
```yaml
version: '3.8'

services:
  web-app:
    build: .
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=development
      - DATABASE_URL=postgresql://user:pass@postgres:5432/myapp
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgres
      - redis
    volumes:
      - .:/app
      - node_modules:/app/node_modules

  worker:
    build: .
    command: npm run worker
    environment:
      - NODE_ENV=development
      - DATABASE_URL=postgresql://user:pass@postgres:5432/myapp
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgres
      - redis

  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: myapp
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

volumes:
  node_modules:
  postgres_data:
```

### 3. **Dockerfile** - Optimized Container Image
```dockerfile
# Multi-stage build for Node.js
FROM node:18-alpine AS dependencies
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production --silent

FROM node:18-alpine AS build
WORKDIR /app
COPY package*.json ./
RUN npm ci --silent
COPY . .
RUN npm run build

FROM node:18-alpine AS runtime
WORKDIR /app
RUN addgroup -g 1001 -S nodejs && \
    adduser -S nextjs -u 1001
COPY --from=dependencies /app/node_modules ./node_modules
COPY --from=build --chown=nextjs:nodejs /app/dist ./dist
COPY --from=build --chown=nextjs:nodejs /app/package*.json ./

USER nextjs
EXPOSE 3000
CMD ["npm", "start"]
```

## üéõÔ∏è Command Options

### **Setup Command Options**
```bash
# Basic setup
sc assistant dev setup

# Interactive mode with prompts
sc assistant dev setup --interactive

# Specify target environment
sc assistant dev setup --env production

# Skip certain file generation
sc assistant dev setup --skip-dockerfile --skip-compose

# Use specific parent stack
sc assistant dev setup --parent my-infrastructure

# Override detected language/framework
sc assistant dev setup --language python --framework django

# Generate for specific cloud provider
sc assistant dev setup --cloud gcp
```

### **Analyze Command Options**
```bash
# Basic analysis
sc assistant dev analyze

# Detailed output with recommendations
sc assistant dev analyze --detailed

# Export analysis to file
sc assistant dev analyze --output analysis.json

# Analyze specific directory
sc assistant dev analyze --path ./services/api
```

## üîó Connecting to Shared Resources

Developer Mode assumes DevOps has already set up shared infrastructure. Your `client.yaml` references these resources:

### **Resource Consumption Pattern**
```yaml
# In your client.yaml
config:
  uses: [postgres-db, redis-cache, s3-uploads]  # Managed by DevOps
  env:
    # Non-sensitive configuration only
    NODE_ENV: production
    PORT: 3000
    # Database connections use auto-injected environment variables:
    # PostgreSQL: PGHOST, PGPORT, PGUSER, PGDATABASE, PGPASSWORD
    # Redis: REDIS_HOST, REDIS_PORT
    # S3: S3_BUCKET, S3_REGION, S3_ACCESS_KEY, S3_SECRET_KEY
  secrets:
    # Sensitive connection strings go in secrets section
    DATABASE_URL: "${secret:database-url}"
    REDIS_URL: "${secret:redis-url}"
    S3_ACCESS_KEY: "${secret:s3-access-key}"
```

### **Parent Stack Reference**
```yaml
# References DevOps-managed server.yaml
stacks:
  my-app:
    parent: infrastructure  # Points to .sc/stacks/infrastructure/server.yaml
    parentEnv: staging      # Uses staging environment resources
```

## üéØ Framework-Specific Optimizations

### **Node.js Projects**
- **Package Manager Detection**: npm, yarn, pnpm
- **Build Tools**: Webpack, Vite, Rollup, esbuild
- **Frameworks**: Express routing, Next.js static export, React SPA
- **Testing**: Jest, Mocha, Cypress configurations

### **Python Projects**
- **Dependency Management**: pip, pipenv, poetry, conda
- **WSGI/ASGI**: Gunicorn, Uvicorn, Hypercorn
- **Frameworks**: Django settings, Flask configs, FastAPI metadata
- **Virtual Environments**: Automatic detection and setup

### **Go Projects**
- **Module System**: go.mod parsing, vendor detection
- **Build Optimization**: Multi-stage Docker builds, CGO_ENABLED=0
- **Frameworks**: Gin middleware, Echo routing, Fiber configuration
- **Performance**: Scratch-based final images, static compilation

## üöÄ Deployment Strategies

### **Local Development**
```bash
# Start local environment
docker-compose up -d

# Run application
npm run dev  # or python manage.py runserver, go run main.go
```

### **Staging Deployment**
```bash
# Deploy to staging (uses shared staging resources)
sc deploy -s myapp -e staging

# Application scaling is configured in client.yaml config.scale section
# Edit client.yaml to update scaling configuration
```

### **Production Deployment**
```bash
# Deploy to production (uses shared production resources)
sc deploy -s myapp -e production

# Verify deployment is working
curl https://my-app.yourcompany.com/health
```

## üí° Best Practices

### **Configuration Management**
- ‚úÖ **Use Environment Variables**: Never hardcode secrets or config
- ‚úÖ **Reference Shared Resources**: Use `uses: [resource-name]` array for resource consumption
- ‚úÖ **Environment Separation**: Different configs for dev/staging/prod
- ‚úÖ **Secret Management**: Use `${secret:name}` for sensitive data (connection strings, API keys)
- ‚úÖ **Security Separation**: env: for non-sensitive config, secrets: for sensitive data

### **Docker Optimization**
- ‚úÖ **Multi-stage Builds**: Separate build and runtime environments
- ‚úÖ **Layer Caching**: Order instructions for optimal caching
- ‚úÖ **Non-root Users**: Run containers as non-privileged users
- ‚úÖ **Health Checks**: Add proper health check endpoints

### **Local Development**
- ‚úÖ **Volume Mounts**: Enable hot reloading with volume mounts
- ‚úÖ **Service Dependencies**: Use `depends_on` for service ordering
- ‚úÖ **Port Mapping**: Consistent port mapping across environments
- ‚úÖ **Environment Parity**: Match production environment closely

## üîç Troubleshooting

### **Analysis Issues**
```bash
# No technology detected
sc assistant dev analyze --verbose
# Check if package.json, requirements.txt, go.mod, etc. exist

# Wrong framework detected
sc assistant dev setup --framework express --skip-analysis

# Complex monorepo structure
sc assistant dev analyze --path ./services/api
```

### **Generated Configuration Issues**
```bash
# Parent stack not found
# Ensure DevOps has set up infrastructure first
sc assistant devops setup  # Run this first

# Resource references not working
# Check that resource names match server.yaml
sc assistant search "resource placeholders"
```

### **Docker Issues**
```bash
# Build failures
# Check generated Dockerfile syntax
docker build -t test .

# Multi-platform builds
sc assistant dev setup --platform linux/amd64,linux/arm64
```

## üìã Examples by Framework

### **Express.js API**
- [Express REST API with PostgreSQL](examples/express-api.md)
- [Express GraphQL with MongoDB](examples/express-graphql.md)
- [Express Microservices](examples/express-microservices.md)

### **Django Application**
- [Django REST API with PostgreSQL](examples/django-api.md)
- [Django Monolith with Redis](examples/django-monolith.md)
- [Django with Celery Workers](examples/django-celery.md)

### **Go Applications**
- [Gin REST API](examples/go-gin-api.md)
- [Go Microservices with gRPC](examples/go-microservices.md)
- [Go CLI Tools](examples/go-cli.md)

## üîó Next Steps

1. **[Set up your first project ‚Üí](getting-started.md#developer-setup)**
2. **[Learn about resource consumption ‚Üí](../concepts/template-placeholders.md)**
3. **[Deploy to staging ‚Üí](../guides/deployment.md)**
4. **[Scale your application ‚Üí](../advanced/scaling-advantages.md)**


---

## DevOps Mode

# DevOps Mode

DevOps Mode is designed for **infrastructure teams** who manage shared resources and provide the foundation for application teams to build upon.

## üéØ Overview

**DevOps Mode Focus:**
- Generate `server.yaml` with shared infrastructure resources
- Create `secrets.yaml` for authentication and sensitive configuration
- Set up cloud provider integrations and credentials
- Define reusable templates for application teams
- Manage environment-specific resource configurations

## üöÄ Quick Start

### Basic Infrastructure Setup
```bash
# Start interactive infrastructure setup
sc assistant devops setup

# Set up for specific cloud provider
sc assistant devops setup --cloud aws

# Configure multi-environment infrastructure
sc assistant devops setup --envs staging,production

# Set up specific resource types
sc assistant devops resources --add database,cache,storage
```

### Resource Management
```bash
# List available resource types
sc assistant devops resources --list

# Add specific resources interactively
sc assistant devops resources --add postgres --interactive

# Generate secrets configuration
sc assistant devops secrets --cloud aws
```

## üèóÔ∏è Infrastructure Wizard

DevOps Mode uses an interactive wizard approach instead of project analysis:

### **1. Cloud Provider Selection**
```
üåê Select your primary cloud provider:

1. AWS (Amazon Web Services)
   ‚úÖ ECS Fargate, RDS, S3, ECR, Lambda
   
2. GCP (Google Cloud Platform)  
   ‚úÖ GKE Autopilot, Cloud SQL, Cloud Storage, Cloud Run
   
3. Azure (Microsoft Azure) [Coming Soon]
   ‚è≥ Container Apps, PostgreSQL, Blob Storage
   
4. Kubernetes (Cloud-agnostic)
   ‚úÖ Native K8s, Helm operators, YAML manifests

5. Hybrid (Multiple providers)
   üîß Advanced configuration required

Choice [1-5]: 1
```

### **2. Environment Configuration**
```
üìä Configure your environments:

‚úÖ Development (local docker-compose)
‚úÖ Staging (cloud resources, lower specs)
‚úÖ Production (cloud resources, high availability)

Additional environments? (testing, preview, etc.): testing
```

### **3. Resource Selection**
```
üéØ Select shared resources to provision:

Databases:
‚òëÔ∏è PostgreSQL (recommended for most apps)
‚òê MongoDB (document database)
‚òê MySQL (legacy compatibility)
‚òê Redis (caching & sessions)

Storage:
‚òëÔ∏è S3-compatible bucket (file uploads)
‚òê CDN (static asset distribution)

Compute:
‚òëÔ∏è ECS Fargate (containerized apps)
‚òê Lambda functions (serverless)
‚òê Static site hosting

Monitoring:
‚òê Application monitoring
‚òê Log aggregation
‚òê Alerting (Slack/Email)
```

### **4. Template Definition**
```
üìã Create application templates:

Template Name: web-app
Description: Standard web application template
Supported Stacks: [PostgreSQL, Redis, S3]
Target: ECS Fargate deployment

Template Name: api-service  
Description: Microservice API template
Supported Stacks: [PostgreSQL, Redis]
Target: ECS Fargate with ALB

Create additional templates? (y/n): n
```

## üìÅ Generated Files

### 1. **server.yaml** - Infrastructure Configuration

```yaml
schemaVersion: 1.0

# Provisioner configuration
provisioner:
  pulumi:
    backend: s3
    state-storage:
      type: s3-bucket
      bucketName: mycompany-sc-state
      region: us-east-1
    secrets-provider:
      type: aws-kms
      kmsKeyId: "arn:aws:kms:us-east-1:123456789:key/abc123"
  auth:
    aws: "${auth:aws}"

# Reusable templates for application teams
templates:
  web-app:
    type: ecs-fargate
    
  api-service:
    type: ecs-fargate

# Shared infrastructure resources
resources:
  # DNS and domain management
  registrar:
    type: cloudflare
    config:
      credentials: "${auth:cloudflare}"
      accountId: "${secret:cloudflare-account-id}"
      zoneName: "mycompany.com"
      dnsRecords:
        - name: "api"
          type: "CNAME"
          value: "staging-api.mycompany.com"
        - name: "app"
          type: "CNAME" 
          value: "staging-app.mycompany.com"
  
  # Environment-specific resources
  resources:
    # Staging environment
    staging:
      resources:
        # NOTE: ECR repositories are automatically created by ecs-fargate - no manual definition needed
        
        # Database
        postgres-db:
          type: aws-rds-postgres
          name: mycompany-staging-db
          instanceClass: db.t3.micro
          allocateStorage: 20
          engineVersion: "15.4"
          username: dbadmin
          password: "${secret:staging-db-password}"
          databaseName: applications
          
        # Additional storage
        cache-bucket:
          type: s3-bucket
          name: mycompany-staging-cache
          allowOnlyHttps: true
          
        # Storage
        uploads-bucket:
          type: s3-bucket
          name: mycompany-staging-uploads
          allowOnlyHttps: true

    # Production environment
    production:
      resources:
        # NOTE: ECR repositories are automatically created by ecs-fargate - no manual definition needed
        
        # Database with high availability
        postgres-db:
          type: aws-rds-postgres
          name: mycompany-prod-db
          instanceClass: db.r5.large
          allocatedStorage: 100
          engineVersion: "15.4"
          username: dbadmin
          password: "${secret:prod-db-password}"
          databaseName: applications
          
        # Production storage
        prod-cache-bucket:
          type: s3-bucket
          name: mycompany-prod-cache
          allowOnlyHttps: true
          
        # Production uploads
        uploads-bucket:
          type: s3-bucket
          name: mycompany-prod-uploads
          allowOnlyHttps: true
```

### 2. **secrets.yaml** - Authentication & Secrets
```yaml
# Authentication for cloud providers
auth:
  aws:
    account: "123456789012"
    accessKey: "${secret:aws-access-key}"
    secretAccessKey: "${secret:aws-secret-key}"
    region: us-east-1
    
  cloudflare:
    credentials: "${secret:cloudflare-api-token}"

# Secret values (managed with sc secrets add)
values:
  # Database passwords
  staging-db-password: "staging-secure-password-123"
  prod-db-password: "production-ultra-secure-password-456"
  
  # Cloud credentials
  aws-access-key: "AKIA..."
  aws-secret-key: "secret..."
  cloudflare-api-token: "token..."
  cloudflare-account-id: "account..."
  
  # Application secrets
  jwt-secret: "super-secret-jwt-key"
  third-party-api-key: "external-service-key"
```

### 3. **cfg.default.yaml** - Simple Container Configuration
```yaml
# Simple Container local configuration
privateKeyPath: ~/.ssh/id_rsa
publicKeyPath: ~/.ssh/id_rsa.pub
projectName: mycompany-infrastructure
```

## üéõÔ∏è Command Options

### **Setup Command Options**
```bash
# Interactive wizard (recommended)
sc assistant devops setup --interactive

# Specify cloud provider
sc assistant devops setup --cloud aws
sc assistant devops setup --cloud gcp  
sc assistant devops setup --cloud kubernetes

# Multi-cloud setup
sc assistant devops setup --cloud aws,gcp --primary aws

# Environment configuration
sc assistant devops setup --envs development,staging,production
sc assistant devops setup --skip-env development

# Resource selection
sc assistant devops setup --resources database,cache,storage
sc assistant devops setup --database postgres --cache redis

# Template creation
sc assistant devops setup --templates web-app,api-service,worker
```

### **Resource Management Options**
```bash
# List available resource types by provider
sc assistant devops resources --list --cloud aws
sc assistant devops resources --list --cloud gcp

# Add specific resources
sc assistant devops resources --add postgres --env staging
sc assistant devops resources --add s3-bucket --env production

# Resource templates
sc assistant devops resources --template database-cluster
sc assistant devops resources --template cache-cluster

# Bulk resource management
sc assistant devops resources --file resources.yaml
```

### **Secrets Management Options**
```bash
# Initialize secrets for cloud provider
sc assistant devops secrets --init --cloud aws

# Add authentication credentials
sc assistant devops secrets --auth aws --interactive

# Generate random secrets
sc assistant devops secrets --generate jwt-secret,api-key

# Import from existing system
sc assistant devops secrets --import-from aws-secrets-manager
```

## üåê Cloud Provider Templates

### **AWS Configuration**
```yaml
# AWS-optimized resources
provisioner:
  pulumi:
    backend: s3
    state-storage:
      type: s3-bucket
      bucketName: "${var:company}-sc-state-${var:environment}"
      region: "${var:aws-region}"
    secrets-provider:
      type: aws-kms
      kmsKeyId: "alias/simple-container-${var:environment}"

templates:
  aws-web-app:
    type: ecs-fargate
    
  aws-api-service:
    type: ecs-fargate
```

### **GCP Configuration**
```yaml
# GCP-optimized resources  
provisioner:
  pulumi:
    backend: gcs
    state-storage:
      type: gcp-bucket
      bucketName: "${var:company}-sc-state-${var:environment}"
      location: "${var:gcp-region}"
    secrets-provider:
      type: gcp-secret-manager
      projectId: "${var:gcp-project-id}"

templates:
  gcp-web-app:
    type: gcp-cloud-run
    artifactRegistryResource: web-registry
    cloudSqlResource: postgres-db
    
  gcp-gke-app:
    type: gcp-gke-autopilot
    gkeClusterResource: gke-cluster
    artifactRegistryResource: app-registry
```

### **Kubernetes Configuration**
```yaml
# Kubernetes-native resources
provisioner:
  pulumi:
    backend: kubernetes
    state-storage:
      type: kubernetes-secret
      namespace: simple-container-system
      secretName: pulumi-state
    secrets-provider:
      type: kubernetes-secret
      namespace: simple-container-system

templates:
  k8s-web-app:
    type: kubernetes-deployment
    helmOperatorResource: postgres-operator
    ingressResource: main-ingress
    
  k8s-worker:
    type: kubernetes-job
    cronSchedule: "0 2 * * *"
    helmOperatorResource: redis-operator
```

## üéØ Resource Categories

### **Compute Resources**
| Resource Type | AWS | GCP | Kubernetes |
|---------------|-----|-----|------------|
| **Container Platform** | ECS Fargate | Cloud Run | Deployment |
| **Kubernetes Cluster** | EKS | GKE Autopilot | Native |
| **Serverless Functions** | Lambda | Cloud Functions | Knative |
| **Static Sites** | S3 + CloudFront | Cloud Storage + CDN | Ingress |

### **Database Resources**  
| Resource Type | AWS | GCP | Kubernetes |
|---------------|-----|-----|------------|
| **PostgreSQL** | RDS PostgreSQL | Cloud SQL PostgreSQL | Helm PostgreSQL |
| **MongoDB** | DocumentDB | MongoDB Atlas | Helm MongoDB |
| **MySQL** | RDS MySQL | Cloud SQL MySQL | Helm MySQL |
| **Redis** | S3 + Apps | GCP Redis | Helm Redis |

### **Storage Resources**
| Resource Type | AWS | GCP | Kubernetes |
|---------------|-----|-----|------------|
| **Object Storage** | S3 | Cloud Storage | MinIO |
| **Block Storage** | EBS | Persistent Disk | PV/PVC |
| **File Storage** | EFS | Filestore | NFS |
| **CDN** | CloudFront | Cloud CDN | Ingress |

## üí° Best Practices

### **Environment Strategy**
- ‚úÖ **Staging Mirrors Production**: Same resources, smaller scale
- ‚úÖ **Development Uses Local**: Docker Compose for dev environment
- ‚úÖ **Testing Environment**: Separate environment for CI/CD
- ‚úÖ **Preview Environments**: Dynamic environments for feature branches

### **Resource Naming**
- ‚úÖ **Consistent Naming**: Use company/project prefixes
- ‚úÖ **Environment Suffixes**: `-staging`, `-prod`, `-dev`
- ‚úÖ **Resource Type Prefixes**: `db-`, `cache-`, `bucket-`
- ‚úÖ **Avoid Hardcoding**: Use template placeholders

### **Security Configuration**
- ‚úÖ **Secrets Management**: Use cloud-native secret stores
- ‚úÖ **Least Privilege**: Minimal IAM permissions
- ‚úÖ **Network Security**: VPCs, security groups, firewalls
- ‚úÖ **Encryption**: Encrypt data at rest and in transit

### **Cost Optimization**
- ‚úÖ **Right-sizing**: Start small, scale based on usage
- ‚úÖ **Reserved Instances**: Use reserved capacity for production
- ‚úÖ **Auto-scaling**: Configure automatic scaling policies
- ‚úÖ **Resource Cleanup**: Implement automatic cleanup for dev/test

## üîß Advanced Configuration

### **Multi-Region Setup**
```yaml
# Multiple environments across regions
resources:
  resources:
    production:
      resources:
        primary-registry:
          type: ecr-repository
          name: mycompany-prod-primary
          
        global-db:
          type: aws-rds-postgres
          name: mycompany-prod-db
          instanceClass: db.r5.large
          allocateStorage: 100
          databaseName: myapp
          engineVersion: "15.4"
          username: dbadmin
          password: "${secret:prod-db-password}"
```

### **Multiple Resource Examples**
```yaml
# Additional resource types
resources:
  resources:
    production:
      resources:
        storage-bucket:
          type: s3-bucket
          name: mycompany-prod-storage
          allowOnlyHttps: true
          
        backup-bucket:
          type: s3-bucket  
          name: mycompany-prod-backups
          allowOnlyHttps: true
```

## üìã Team Workflows

### **1. Initial Infrastructure Setup**
```bash
# DevOps team sets up foundation
sc assistant devops setup --interactive

# Review generated configuration
cat .sc/stacks/infrastructure/server.yaml

# Set up secrets
sc secrets add aws-access-key
sc secrets add aws-secret-key
sc secrets add staging-db-password

# Deploy infrastructure
sc provision -s infrastructure -e staging
sc provision -s infrastructure -e production
```

### **2. Application Team Enablement**
```bash
# Share infrastructure details with dev teams
# Review generated server.yaml to see available resources
cat .sc/stacks/infrastructure/server.yaml

# Developers can now reference these resources
# in their client.yaml files using:
# parent: infrastructure
# uses: [postgres-db, redis-cache, uploads-bucket]
```

### **3. Resource Updates**
```bash
# Add new resource to infrastructure
sc assistant devops resources --add mongodb --env staging

# Update existing resource
sc assistant devops resources --update postgres-db --scale-up

# Deploy changes
sc provision -s infrastructure -e staging
```

## üîç Troubleshooting

### **Setup Issues**
```bash
# Cloud credentials not configured
sc assistant devops secrets --init --cloud aws

# Permission denied errors
# Check IAM permissions for Simple Container operations

# State backend issues
# Verify S3 bucket exists and is accessible
aws s3 ls s3://mycompany-sc-state/
```

### **Resource Conflicts**
```bash
# Resource name conflicts
# Use unique prefixes: company-env-resource format
# Example: mycompany-staging-postgres

# Environment isolation
# Ensure separate resources per environment
# Never share production resources with staging
```

### **Template Issues**
```bash
# Template not found errors
# Ensure template names match between server.yaml and client.yaml

# Resource references not working
# Check resource names are correct and environment matches
cat .sc/stacks/infrastructure/server.yaml | grep -A 20 "resources:"
```

## üìã Examples by Cloud Provider

### **AWS Examples**
- [AWS Multi-tier Architecture](examples/aws-multi-tier.md)
- [AWS Microservices with ECS](examples/aws-microservices.md)
- [AWS Serverless Setup](examples/aws-serverless.md)

### **GCP Examples**
- [GCP with GKE Autopilot](examples/gcp-gke.md)
- [GCP Cloud Run Services](examples/gcp-cloud-run.md)
- [GCP Hybrid Architecture](examples/gcp-hybrid.md)

### **Kubernetes Examples**
- [On-premises Kubernetes](examples/k8s-on-prem.md)
- [Multi-cloud Kubernetes](examples/k8s-multi-cloud.md)
- [Kubernetes with Operators](examples/k8s-operators.md)

## üîó Next Steps

1. **[Complete infrastructure setup ‚Üí](getting-started.md#devops-setup)**
2. **[Enable application teams ‚Üí](../concepts/parent-stacks.md)**
3. **[Monitor and scale resources ‚Üí](../advanced/scaling-advantages.md)**
4. **[Set up CI/CD pipelines ‚Üí](../guides/deployment.md)**


---

## Commands Reference

# Commands Reference

Complete reference for all Simple Container AI Assistant commands, organized by mode and functionality.

## üìã Command Overview

| Command                                      | Mode      | Description                      |
|----------------------------------------------|-----------|----------------------------------|
| [`sc assistant dev`](#developer-commands)    | Developer | Application-focused commands     |
| [`sc assistant devops`](#devops-commands)    | DevOps    | Infrastructure-focused commands  |
| [`sc assistant search`](#search-commands)    | Both      | Documentation search             |
| [`sc assistant chat`](#interactive-commands) | Both      | Interactive assistance (Phase 3) |
| [`sc assistant mcp`](#mcp-commands)          | Both      | MCP server management            |

## üßë‚Äçüíª Developer Commands

### `sc assistant dev setup`

Generate application configuration files based on project analysis.

**Usage:**
```bash
sc assistant dev setup [options]
```

**Options:**
| Flag | Description | Default |
|------|-------------|---------|
| `--interactive, -i` | Interactive mode with prompts | `false` |
| `--env <environment>` | Target environment | `staging` |
| `--parent <stack>` | Parent stack name | `infrastructure` |
| `--skip-analysis` | Skip automatic project analysis | `false` |
| `--skip-dockerfile` | Skip Dockerfile generation | `false` |
| `--skip-compose` | Skip docker-compose.yaml generation | `false` |
| `--language <lang>` | Override detected language | Auto-detected |
| `--framework <framework>` | Override detected framework | Auto-detected |
| `--cloud <provider>` | Target cloud provider | From parent stack |
| `--output-dir <dir>` | Output directory | `.sc/stacks/<project-name>/` |

**Examples:**
```bash
# Basic setup with auto-detection
sc assistant dev setup

# Interactive setup with prompts
sc assistant dev setup --interactive

# Target production environment
sc assistant dev setup --env production

# Skip Docker files, only generate Simple Container config
sc assistant dev setup --skip-dockerfile --skip-compose

# Override detected tech stack
sc assistant dev setup --language python --framework django

# Use specific parent infrastructure
sc assistant dev setup --parent my-company-infra --env staging
```

**Generated Files:**
- `client.yaml` - Simple Container application configuration
- `docker-compose.yaml` - Local development environment
- `Dockerfile` - Container image definition (if not exists)

### `sc assistant dev analyze`

Analyze project structure and detect technology stack.

**Usage:**
```bash
sc assistant dev analyze [options]
```

**Options:**
| Flag | Description | Default |
|------|-------------|---------|
| `--detailed` | Show detailed analysis output | `false` |
| `--path <directory>` | Project path to analyze | `.` |
| `--output <file>` | Export analysis to JSON file | Console output |
| `--format <format>` | Output format (json, yaml, table) | `table` |

**Examples:**
```bash
# Basic project analysis
sc assistant dev analyze

# Detailed analysis with recommendations
sc assistant dev analyze --detailed

# Analyze specific directory
sc assistant dev analyze --path ./services/api

# Export analysis to file
sc assistant dev analyze --output analysis.json --format json

# Analyze and show in table format
sc assistant dev analyze --format table
```

**Output Example:**
```
üîç Project Analysis Results

üìä Technology Stack:
   Language:     Node.js (18.x)
   Framework:    Express.js
   Architecture: REST API
   Confidence:   95%

üì¶ Dependencies:
   ‚úÖ express      ^4.18.0   (Web framework)
   ‚úÖ pg           ^8.8.0    (PostgreSQL client)
   ‚úÖ redis        ^4.3.0    (Cache client)
   ‚úÖ jsonwebtoken ^8.5.1    (Authentication)

üéØ Recommendations:
   üîπ Add PostgreSQL database resource
   üîπ Add Redis cache resource
   üîπ Configure health check endpoint
   üîπ Add request logging middleware
```

## üõ†Ô∏è DevOps Commands

### `sc assistant devops setup`

Set up infrastructure configuration with interactive wizard.

**Usage:**
```bash
sc assistant devops setup [options]
```

**Options:**
| Flag | Description | Default |
|------|-------------|---------|
| `--interactive, -i` | Interactive wizard mode | `true` |
| `--cloud <provider>` | Cloud provider (aws, gcp, k8s) | Interactive selection |
| `--envs <environments>` | Comma-separated environments | `staging,production` |
| `--resources <types>` | Comma-separated resource types | Interactive selection |
| `--templates <names>` | Template names to create | Interactive selection |
| `--prefix <name>` | Resource name prefix | Project name |
| `--region <region>` | Default cloud region | Provider default |
| `--output-dir <dir>` | Output directory | `.sc/stacks/infrastructure/` |

**Examples:**
```bash
# Interactive wizard (recommended for first-time setup)
sc assistant devops setup

# AWS setup with specific environments
sc assistant devops setup --cloud aws --envs staging,production,testing

# Quick setup with common resources
sc assistant devops setup --cloud gcp --resources database,cache,storage

# Multi-cloud setup
sc assistant devops setup --cloud aws,gcp --primary aws

# Custom prefix and region
sc assistant devops setup --prefix mycompany --region us-west-2
```

**Generated Files:**
- `server.yaml` - Infrastructure resources and templates
- `secrets.yaml` - Authentication and sensitive configuration
- `cfg.default.yaml` - Default Simple Container settings

### `sc assistant devops resources`

Manage shared infrastructure resources.

**Usage:**
```bash
sc assistant devops resources [action] [options]
```

**Actions:**
| Action | Description |
|--------|-------------|
| `--list` | List available resource types |
| `--add <type>` | Add resource to infrastructure |
| `--remove <name>` | Remove resource from infrastructure |
| `--update <name>` | Update existing resource |
| `--template <name>` | Create resource template |

**Options:**
| Flag | Description | Default |
|------|-------------|---------|
| `--cloud <provider>` | Filter by cloud provider | All providers |
| `--env <environment>` | Target environment | All environments |
| `--interactive, -i` | Interactive resource configuration | `false` |
| `--copy-from <env>` | Copy resource from another environment | None |
| `--scale-up` | Increase resource capacity | Current settings |
| `--scale-down` | Decrease resource capacity | Current settings |

**Examples:**
```bash
# List all available resource types
sc assistant devops resources --list

# List AWS-specific resources
sc assistant devops resources --list --cloud aws

# Add PostgreSQL database interactively
sc assistant devops resources --add postgres --interactive

# Add Redis cache to staging environment
sc assistant devops resources --add redis --env staging

# Copy production database config to staging (with smaller instance)
sc assistant devops resources --add postgres --env staging --copy-from production --scale-down

# Update existing database to larger instance
sc assistant devops resources --update postgres-db --scale-up
```

### `sc assistant devops secrets`

Manage authentication credentials and secrets.

**Usage:**
```bash
sc assistant devops secrets [action] [options]
```

**Actions:**
| Action | Description |
|--------|-------------|
| `--init` | Initialize secrets configuration |
| `--auth <provider>` | Configure cloud provider authentication |
| `--generate <names>` | Generate random secrets |
| `--import-from <source>` | Import secrets from external system |
| `--rotate <names>` | Rotate existing secrets |

**Options:**
| Flag | Description | Default |
|------|-------------|---------|
| `--cloud <provider>` | Cloud provider for authentication | Current provider |
| `--interactive, -i` | Interactive secret entry | `false` |
| `--length <n>` | Generated secret length | `32` |
| `--export-to <file>` | Export secrets template | Console |

**Examples:**
```bash
# Initialize secrets for AWS
sc assistant devops secrets --init --cloud aws

# Configure AWS credentials interactively
sc assistant devops secrets --auth aws --interactive

# Generate application secrets
sc assistant devops secrets --generate jwt-secret,api-key,encryption-key

# Import secrets from AWS Secrets Manager
sc assistant devops secrets --import-from aws-secrets-manager

# Rotate database passwords
sc assistant devops secrets --rotate staging-db-password,prod-db-password
```

## üîç Search Commands

### `sc assistant search`

Search Simple Container documentation using semantic similarity.

**Usage:**
```bash
sc assistant search <query> [options]
```

**Options:**
| Flag | Description | Default |
|------|-------------|---------|
| `--limit <n>` | Maximum results to return | `5` |
| `--type <type>` | Document type (docs, examples, schemas) | `all` |
| `--provider <name>` | Filter by cloud provider | `all` |
| `--format <format>` | Output format (table, json, detailed) | `table` |
| `--threshold <n>` | Minimum similarity score (0.0-1.0) | `0.7` |

**Examples:**
```bash
# Basic documentation search
sc assistant search "PostgreSQL database configuration"

# Limit results and search only examples
sc assistant search "Node.js deployment" --limit 10 --type examples

# Search for AWS-specific resources
sc assistant search "S3 bucket setup" --provider aws

# High-precision search with JSON output
sc assistant search "GKE autopilot" --threshold 0.9 --format json

# Search for schema information
sc assistant search "client.yaml structure" --type schemas
```

**Output Example:**
```
üîç Search Results for: "PostgreSQL database configuration"

‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ # ‚îÇ Document                ‚îÇ Type     ‚îÇ Similarity‚îÇ Preview                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1 ‚îÇ supported-resources.md  ‚îÇ docs     ‚îÇ 0.894     ‚îÇ PostgreSQL database configuration.  ‚îÇ
‚îÇ   ‚îÇ                         ‚îÇ          ‚îÇ           ‚îÇ Use aws-rds-postgres for AWS...    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 2 ‚îÇ postgres-example.md     ‚îÇ examples ‚îÇ 0.847     ‚îÇ Complete PostgreSQL setup example  ‚îÇ
‚îÇ   ‚îÇ                         ‚îÇ          ‚îÇ           ‚îÇ with environment variables...       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 3 ‚îÇ rds-postgres.json       ‚îÇ schemas  ‚îÇ 0.798     ‚îÇ JSON schema for AWS RDS PostgreSQL  ‚îÇ
‚îÇ   ‚îÇ                         ‚îÇ          ‚îÇ           ‚îÇ resource configuration...           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Found 3 relevant documents in 89ms
```

## ü§ñ Interactive Commands

### `sc assistant chat`

Start interactive chat session with AI assistant (Phase 3).

**Usage:**
```bash
sc assistant chat [options]
```

**Options:**
| Flag | Description | Default |
|------|-------------|---------|
| `--mode <mode>` | Chat mode (developer, devops, general) | Auto-detect |
| `--context <path>` | Project context directory | `.` |
| `--model <name>` | LLM model to use | `gpt-3.5-turbo` |
| `--save-session` | Save conversation history | `false` |
| `--load-session <file>` | Load previous conversation | None |

**Examples:**
```bash
# Start general chat (uses stored API key if available)
sc assistant chat

# Developer-focused chat with project context
sc assistant chat --mode developer --context .

# DevOps chat for infrastructure questions
sc assistant chat --mode devops

# Use specific API key for this session
sc assistant chat --openai-key sk-your-key-here

# Save conversation for later reference
sc assistant chat --save-session conversation.json
```

**Note:** If you have stored your OpenAI API key using `/apikey set` in a previous session, it will be automatically loaded. You can manage your stored API key using the `/apikey` command within the chat interface.

**Chat Commands:**
- `/help` - Show available chat commands
- `/search <query>` - Search documentation inline
- `/analyze` - Analyze current project
- `/setup` - Generate configuration files
- `/switch <mode>` - Switch between dev and devops modes
- `/clear` - Clear conversation history
- `/status` - Show current session status
- `/apikey <set|delete|status> [provider]` - Manage LLM provider API keys
- `/provider <list|switch|info> [provider]` - Manage LLM providers
- `/history [clear]` - Show or clear command history
- `/exit` - Exit chat session

**Input Features:**
- **Tab** - Autocomplete commands and subcommands (shows suggestions if multiple matches)
- **‚Üë/‚Üì Arrow Keys** - Navigate command history
- **Ctrl+C** - Exit chat session
- Type `/` to see command suggestions
- Type `/command ` (with space) and Tab to see subcommand options

### Multi-Provider Support

The assistant supports multiple LLM providers with persistent API key storage:

**Supported Providers:**
- **OpenAI** - GPT models (gpt-3.5-turbo, gpt-4, etc.)
- **Ollama** - Local or self-hosted models (llama2, mistral, etc.)
- **Anthropic** - Claude models
- **DeepSeek** - DeepSeek models
- **Yandex** - Yandex ChatGPT

### API Key Management

**Setting an API Key:**
```bash
# Interactive provider selection (recommended)
üí¨ /apikey set

üìã Select a provider to configure:

  1. OpenAI ‚úì (configured)
  2. Ollama (not configured)
  3. Anthropic (not configured)
  4. DeepSeek (not configured)
  5. Yandex ChatGPT (not configured)

Enter number (1-5) or 'q' to cancel: 2
‚úì Selected: Ollama

üîë Enter your Ollama API key: [hidden input]
üåê Enter Ollama base URL (press Enter for http://localhost:11434): 
ü§ñ Enter default model (press Enter for llama2): 
‚úÖ Ollama API key saved successfully

# Or specify provider directly
üí¨ /apikey set openai
üîë Enter your OpenAI API key: [hidden input]
‚úÖ OpenAI API key saved successfully

# Set Anthropic API key
üí¨ /apikey set anthropic
üîë Enter your Anthropic API key: [hidden input]
‚úÖ Anthropic API key saved successfully
```

**Checking API Key Status:**
```bash
# Show all configured providers
üí¨ /apikey status
üìã Configured Providers:

  ‚Ä¢ OpenAI (default): sk-proj...xyz
  ‚Ä¢ Ollama: none...xyz
    Base URL: http://localhost:11434

Stored in: /Users/username/.sc/assistant-config.json

# Show specific provider
üí¨ /apikey status openai
‚úÖ OpenAI API key is configured: sk-proj...xyz
   Stored in: /Users/username/.sc/assistant-config.json
```

**Deleting Stored API Key:**
```bash
# Delete default provider's API key
üí¨ /apikey delete
‚úÖ OpenAI API key deleted successfully

# Delete specific provider's API key
üí¨ /apikey delete ollama
‚úÖ Ollama API key deleted successfully
```

### Provider Management

**List Configured Providers:**
```bash
üí¨ /provider list
üìã Available Providers:

  ‚Ä¢ OpenAI ‚≠ê (current)
  ‚Ä¢ Ollama
  ‚Ä¢ Anthropic

Use '/provider switch <provider>' to change the default provider
```

**Switch Between Providers:**
```bash
# Interactive selection (recommended)
üí¨ /provider switch

üìã Select a provider to switch to:

  1. OpenAI ‚≠ê (current)
  2. Ollama
  3. Anthropic

Enter number (1-3) or 'q' to cancel: 2
‚úì Selected: Ollama
‚úÖ Switched to Ollama and reloaded successfully!
You can continue chatting with the new provider.

# Or specify provider directly
üí¨ /provider switch ollama
‚úÖ Switched to Ollama and reloaded successfully!
You can continue chatting with the new provider.
```

**View Provider Info:**
```bash
üí¨ /provider info
‚ÑπÔ∏è  OpenAI Configuration:

  Provider: openai
  API Key: sk-proj...xyz

# View specific provider
üí¨ /provider info ollama
‚ÑπÔ∏è  Ollama Configuration:

  Provider: ollama
  API Key: none...xyz
  Base URL: http://localhost:11434
  Default Model: llama2
```

**API Key Priority:**

The assistant checks for API keys in the following order:
1. Command line flag: `--openai-key sk-...`
2. Environment variable: `OPENAI_API_KEY`
3. Stored config (default provider): `~/.sc/assistant-config.json`
4. Interactive prompt (with option to save)

**First-Time Setup:**

When you start chat without an API key, you'll be prompted to enter it and asked if you want to save it:

```bash
sc assistant chat
‚ö†Ô∏è  OpenAI API key not found
...
üîë Enter your OpenAI API key: [hidden input]
üíæ Save this API key for future sessions? (Y/n): y
‚úÖ API key saved to ~/.sc/assistant-config.json
```

**Provider Display on Start:**

When you start a chat session, the assistant shows which provider is being used:

```bash
sc assistant chat
‚úÖ Using stored OpenAI API key

üöÄ Simple Container AI Assistant
...
```

**Security Notes:**
- API keys are stored in `~/.sc/assistant-config.json` with restricted permissions (0600)
- Keys are masked when displayed (e.g., `sk-proj...xyz`)
- The config file is stored in your home directory and not committed to version control

## üåê MCP Commands

### `sc assistant mcp`

Start MCP (Model Context Protocol) server for external tool integration.

**Usage:**
```bash
sc assistant mcp [options]
```

**Options:**
| Flag | Description | Default |
|------|-------------|---------|
| `--host <address>` | Server host address | `localhost` |
| `--port <number>` | Server port | `9999` |
| `--verbose, -v` | Verbose logging | `false` |
| `--config <file>` | Configuration file path | `.sc/mcp-config.yaml` |
| `--auth` | Enable API key authentication | `false` |
| `--cors-origin <origins>` | CORS allowed origins | `*` |
| `--rate-limit <n>` | Requests per minute | `unlimited` |

**Examples:**
```bash
# Start MCP server on default port
sc assistant mcp

# Start on specific host and port
sc assistant mcp --host 0.0.0.0 --port 8080

# Start with authentication and rate limiting
sc assistant mcp --auth --rate-limit 120

# Start with custom configuration
sc assistant mcp --config /etc/simple-container/mcp.yaml

# Start with verbose logging for debugging
sc assistant mcp --verbose
```

**Server Endpoints:**
- `GET /health` - Health check
- `GET /capabilities` - Server capabilities
- `POST /mcp` - JSON-RPC MCP endpoint
- `GET /metrics` - Prometheus metrics
- `GET /debug/memory` - Memory usage info

## üîß Global Options

All `sc assistant` commands support these global options:

| Flag | Description | Default |
|------|-------------|---------|
| `--help, -h` | Show command help | |
| `--version` | Show assistant version | |
| `--config <file>` | Configuration file | `.sc/cfg.default.yaml` |
| `--profile <name>` | Simple Container profile | `default` |
| `--verbose, -v` | Verbose output | `false` |
| `--quiet, -q` | Minimal output | `false` |
| `--no-color` | Disable colored output | `false` |
| `--json` | JSON output format | `false` |

## üìä Command Chaining

Commands can be chained together for complex workflows:

```bash
# Analyze project, then set up configuration
sc assistant dev analyze --output analysis.json && \
sc assistant dev setup --skip-analysis

# Set up infrastructure, then start MCP server
sc assistant devops setup && \
sc assistant mcp --port 9999

# Search for examples, then generate configuration
sc assistant search "Express.js API setup" --limit 1 && \
sc assistant dev setup --framework express
```

## üîç Exit Codes

All commands return standard exit codes:

| Code | Meaning |
|------|---------|
| `0` | Success |
| `1` | General error |
| `2` | Command line argument error |
| `3` | Configuration error |
| `4` | Project analysis error |
| `5` | File generation error |
| `6` | Network/MCP server error |

## üí° Tips and Best Practices

### **Command Aliases**
Add to your shell configuration:
```bash
# ~/.bashrc or ~/.zshrc
alias sca='sc assistant'
alias scad='sc assistant dev'
alias scao='sc assistant devops'
alias scas='sc assistant search'
```

### **Configuration Management**
```bash
# Set default options in environment
export SC_ASSISTANT_MODE=developer
export SC_ASSISTANT_CLOUD=aws
export SC_ASSISTANT_ENV=staging

# Use configuration file for complex setups
cat > ~/.sc/assistant-config.yaml << EOF
mode: developer
defaults:
  cloud: aws
  environment: staging
  parent: infrastructure
search:
  limit: 10
  threshold: 0.8
EOF
```

### **Automation Scripts**
```bash
#!/bin/bash
# setup-new-project.sh
set -e

echo "Setting up new project with Simple Container AI Assistant"

# Analyze project
sc assistant dev analyze --detailed

# Generate configuration
sc assistant dev setup --interactive

# Start local development
docker-compose up -d

echo "Project setup complete! Run 'sc deploy -e staging' to deploy."
```

## üîó See Also

- **[Getting Started Guide](getting-started.md)** - Basic usage walkthrough
- **[Developer Mode Guide](developer-mode.md)** - Detailed developer workflows
- **[DevOps Mode Guide](devops-mode.md)** - Infrastructure management
- **[MCP Integration](mcp-integration.md)** - External tool integration
- **[Troubleshooting](troubleshooting.md)** - Common issues and solutions


---

## MCP Integration

# MCP Integration

The Model Context Protocol (MCP) integration allows external tools like **Windsurf**, **Cursor**, and other AI-powered IDEs to access Simple Container's knowledge base and project context.

## üéØ Overview

**MCP Server Features:**
- **Semantic Documentation Search** - Query 10,000+ indexed documents
- **Project Context Analysis** - Get project structure and Simple Container configuration
- **Resource Information** - Access details about supported cloud resources
- **Configuration Validation** - Verify Simple Container configurations
- **Real-time Assistance** - Live help during development

## üöÄ Quick Start

### Start MCP Server
```bash
# Start on default port (9999)
sc assistant mcp

# Custom host and port
sc assistant mcp --host 0.0.0.0 --port 8080

# Start in stdio mode (for IDE integration)
sc assistant mcp --stdio

# Start with verbose logging
sc assistant mcp --verbose
```

### Test Server

#### **HTTP Mode Testing**
```bash
# Health check
curl http://localhost:9999/health

# Get capabilities
curl http://localhost:9999/capabilities | jq

# Test ping
curl -X POST http://localhost:9999/mcp \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","method":"ping","id":"test"}'
```

#### **Stdio Mode Testing (MCP Compliant)**
```bash
# Test full MCP initialization sequence
printf '{"jsonrpc":"2.0","method":"initialize","params":{"protocolVersion":"2024-11-05","capabilities":{},"clientInfo":{"name":"test-client","version":"1.0.0"}},"id":1}\n{"jsonrpc":"2.0","method":"notifications/initialized"}\n{"jsonrpc":"2.0","method":"tools/list","id":2}\n' | sc assistant mcp --stdio

# Test tool execution
printf '{"jsonrpc":"2.0","method":"initialize","params":{"protocolVersion":"2024-11-05","capabilities":{},"clientInfo":{"name":"test-client","version":"1.0.0"}},"id":1}\n{"jsonrpc":"2.0","method":"notifications/initialized"}\n{"jsonrpc":"2.0","method":"tools/call","params":{"name":"search_documentation","arguments":{"query":"kubernetes deployment"}},"id":3}\n' | sc assistant mcp --stdio

# Test resources
printf '{"jsonrpc":"2.0","method":"initialize","params":{"protocolVersion":"2024-11-05","capabilities":{},"clientInfo":{"name":"test-client","version":"1.0.0"}},"id":1}\n{"jsonrpc":"2.0","method":"notifications/initialized"}\n{"jsonrpc":"2.0","method":"resources/list","id":4}\n' | sc assistant mcp --stdio
```

## üîå IDE Integrations

### **Windsurf IDE Integration**

#### **Option 1: Global MCP Configuration**

In Windsurf IDE Settings ‚Üí MCP Servers, add:

```json
{
  "simple-container": {
    "command": "sc",
    "args": ["assistant", "mcp", "--stdio"],
    "env": {
      "PATH": "/usr/local/bin:/usr/bin:/bin"
    }
  }
}
```

**Note**: The MCP server now fully complies with MCP specification 2024-11-05, including:
- Proper initialization handshake (`initialize` ‚Üí `notifications/initialized`)
- Standard MCP methods (`tools/list`, `tools/call`, `resources/list`, `resources/read`)
- Protocol version negotiation and capability declaration
- Clean JSON-RPC 2.0 format with no embedded newlines

#### **Option 2: Project-Specific Configuration**

Create `.windsurf/tools.json` in your project:
```json
{
  "version": "1.0",
  "tools": [
    {
      "name": "simple-container-assistant",
      "type": "mcp",
      "endpoint": "http://localhost:9999/mcp",
      "description": "Simple Container AI assistant with documentation search and project analysis",
      "capabilities": [
        "search_documentation",
        "get_project_context",
        "get_supported_resources",
        "analyze_project",
        "setup_simple_container"
      ],
      "autoStart": true,
      "icon": "üöÄ"
    }
  ],
  "settings": {
    "simple-container": {
      "mode": "developer",
      "autoAnalyze": true,
      "searchLimit": 10
    }
  }
}
```

### **Cursor IDE Integration**

Add to `.cursor/config.json`:
```json
{
  "mcp_servers": {
    "simple-container": {
      "url": "http://localhost:9999/mcp",
      "name": "Simple Container Assistant",
      "description": "Documentation search and project context for Simple Container",
      "methods": [
        "search_documentation",
        "get_project_context",
        "get_supported_resources",
        "analyze_project",
        "setup_simple_container"
      ],
      "autoConnect": true
    }
  },
  "ai": {
    "providers": {
      "simple-container": {
        "endpoint": "http://localhost:9999/mcp",
        "context": "simple-container-project"
      }
    }
  }
}
```

### **VS Code Extension**

Install Simple Container extension and configure:
```json
// settings.json
{
  "simple-container.assistant.enabled": true,
  "simple-container.assistant.mcpEndpoint": "http://localhost:9999/mcp",
  "simple-container.assistant.mode": "developer",
  "simple-container.assistant.searchOnType": true,
  "simple-container.assistant.contextAware": true
}
```

## üîß Troubleshooting

### **Common Issues**

#### **"Failed to initialize server" in Windsurf**
- **Cause**: MCP server not responding to initialization sequence
- **Solution**: Ensure `sc` command is in PATH and server starts correctly
- **Test**: Run `sc assistant mcp --stdio` manually and test with initialization sequence

#### **"Server not found" errors**
- **Cause**: Incorrect server name or configuration format
- **Solution**: Ensure server name in configuration matches exactly
- **Test**: Verify `.windsurf/tools.json` syntax with JSON validator

#### **Tool calls not working**
- **Cause**: Server not properly initialized before tool calls
- **Solution**: Ensure proper MCP initialization sequence (initialize ‚Üí initialized ‚Üí tool calls)
- **Test**: Use manual stdio testing commands above

#### **Signal handling issues**
- **Cause**: Server not responding to SIGTERM
- **Solution**: Updated implementation now handles signals properly in stdio mode
- **Test**: Start server and test with `timeout 2s sc assistant mcp --stdio`

### **Debug Commands**

```bash
# Check if sc command is available
which sc

# Test server startup
sc assistant mcp --stdio &
sleep 1
pkill -f "sc assistant mcp"

# Verify MCP protocol compliance
printf '{"jsonrpc":"2.0","method":"initialize","params":{"protocolVersion":"2024-11-05","capabilities":{},"clientInfo":{"name":"debug","version":"1.0.0"}},"id":1}\n' | sc assistant mcp --stdio
```

## üì° MCP API Reference

### **Available Methods**

#### **Standard MCP Methods (2024-11-05)**
| Method            | Purpose                    | Status |
|-------------------|----------------------------|--------|
| `initialize`      | Protocol initialization    | ‚úÖ      |
| `ping`            | Connectivity test          | ‚úÖ      |
| `tools/list`      | List available tools       | ‚úÖ      |
| `tools/call`      | Execute tools              | ‚úÖ      |
| `resources/list`  | List available resources   | ‚úÖ      |
| `resources/read`  | Read resource content      | ‚úÖ      |

#### **Available Tools (via `tools/call`)**
| Tool Name                 | Purpose                                                    | Status |
|---------------------------|------------------------------------------------------------|--------|
| `search_documentation`    | Semantic doc search                                        | ‚úÖ      |
| `get_project_context`     | Basic project info & SC config                             | ‚úÖ      |
| `analyze_project`         | Detailed analysis & recommendations                        | ‚úÖ      |
| `get_supported_resources` | Resource catalog                                           | ‚úÖ      |
| `setup_simple_container`  | Initialize SC configuration using built-in setup           | ‚úÖ      |
| `get_current_config`      | **NEW** - Read and parse existing configuration files      | ‚úÖ      |
| `add_environment`         | **NEW** - Add new environment/stack to client.yaml        | ‚úÖ      |
| `modify_stack_config`     | **NEW** - Modify existing stack configuration             | ‚úÖ      |
| `add_resource`            | **NEW** - Add new resource to server.yaml                 | ‚úÖ      |

**Note**: Legacy direct method calls have been removed. All functionality is now accessed through standard MCP `tools/call` method for better compliance and cleaner architecture.

#### **üîß Configuration Modification Tools**

The MCP server now includes powerful configuration modification capabilities:

**üéØ Developer Tools (client.yaml modifications):**
- **`get_current_config`** - Read and parse existing configuration files
- **`add_environment`** - Add new stack environments (prod, staging, etc.)
- **`modify_stack_config`** - Change deployment types, scaling, domains

**‚öôÔ∏è DevOps Tools (server.yaml modifications):**
- **`add_resource`** - Add new resources (databases, caches, queues)

**Example Usage:**

```bash
# Read current client configuration
{"name": "get_current_config", "arguments": {"config_type": "client"}}

# Add production environment  
{"name": "add_environment", "arguments": {
    "stack_name": "prod",
    "deployment_type": "cloud-compose", 
    "parent": "mycompany/infrastructure",
    "parent_env": "production"
}}

# Change deployment type for staging
{"name": "modify_stack_config", "arguments": {
    "stack_name": "staging", 
    "changes": {"type": "single-image"}
}}

# Add MongoDB Atlas cluster
{"name": "add_resource", "arguments": {
    "resource_name": "mongodb-atlas",
    "resource_type": "mongodb-atlas",
    "environment": "prod",
    "config": {"tier": "M10", "region": "us-east-1"}
}}
```

**Key Differences:**
- **`get_project_context`**: Returns basic project info and Simple Container configuration status
- **`analyze_project`**: Returns detailed tech stack analysis, recommendations, and architectural insights
- **Configuration tools**: Enable direct modification of client.yaml and server.yaml files with automatic backups

**ü§ñ LLM Enhancement Available:**
The `analyze_project` tool now supports optional LLM enhancement for:
- **Smarter Architecture Detection**: AI-powered validation of detected patterns
- **Context-Aware Recommendations**: Deployment and optimization suggestions based on Simple Container best practices
- **Performance Insights**: AI-generated performance and cost optimization recommendations
- **Integration Guidance**: Specific Simple Container resource recommendations and migration strategies

*Note: LLM enhancement requires API keys and is optional - the analyzer works fully without it.*

### **1. Search Documentation**

Search Simple Container documentation using semantic similarity:

```bash
curl -X POST http://localhost:9999/mcp \
  -H "Content-Type: application/json" \
  -d '{
    "jsonrpc": "2.0",
    "method": "search_documentation",
    "params": {
      "query": "PostgreSQL database configuration with Simple Container",
      "limit": 5,
      "type": "docs"
    },
    "id": "search-1"
  }'
```

**Response:**
```json
{
  "jsonrpc": "2.0",
  "result": {
    "documents": [
      {
        "id": "supported-resources.md_chunk_15",
        "content": "PostgreSQL database configuration. Use aws-rds-postgres for AWS or gcp-cloudsql-postgres for Google Cloud...",
        "path": "docs/docs/reference/supported-resources.md",
        "type": "docs",
        "similarity": 0.894,
        "metadata": {
          "file_name": "supported-resources.md",
          "provider": "aws,gcp",
          "resource_type": "postgres"
        }
      },
      {
        "id": "postgres-example.md_chunk_3",
        "content": "Complete PostgreSQL setup example with environment variables and connection pooling...",
        "path": "docs/docs/examples/databases/postgres-example.md",
        "type": "examples",
        "similarity": 0.847,
        "metadata": {
          "file_name": "postgres-example.md",
          "category": "database"
        }
      }
    ],
    "total": 2,
    "query": "PostgreSQL database configuration with Simple Container",
    "timestamp": "2024-10-05T13:19:02Z"
  },
  "id": "search-1"
}
```

### **2. Get Project Context**

Analyze the current project and Simple Container configuration:

```bash
curl -X POST http://localhost:9999/mcp \
  -H "Content-Type: application/json" \
  -d '{
    "jsonrpc": "2.0",
    "method": "get_project_context",
    "params": {
      "path": "."
    },
    "id": "context-1"
  }'
```

**Response:**
```json
{
  "jsonrpc": "2.0",
  "result": {
    "path": "/home/user/my-awesome-app",
    "name": "my-awesome-app",
    "sc_config_exists": true,
    "sc_config_path": "/home/user/my-awesome-app/.sc",
    "tech_stack": {
      "language": "javascript",
      "framework": "express",
      "runtime": "nodejs",
      "version": "18.x",
      "confidence": 0.95
    },
    "resources": [
      {
        "type": "postgres-db",
        "name": "PostgreSQL Database",
        "provider": "aws",
        "status": "configured"
      },
      {
        "type": "redis-cache",
        "name": "Redis Cache",
        "provider": "aws",
        "status": "configured"
      }
    ],
    "recommendations": [
      "Add health check endpoint",
      "Configure connection pooling",
      "Add request logging middleware"
    ],
    "metadata": {
      "analyzed_at": "2024-10-05T13:19:02Z",
      "mcp_version": "1.0",
      "simple_container_version": "1.5.0"
    }
  },
  "id": "context-1"
}
```

### **3. Get Supported Resources**

Get information about all available Simple Container resources:

```bash
curl -X POST http://localhost:9999/mcp \
  -H "Content-Type: application/json" \
  -d '{
    "jsonrpc": "2.0",
    "method": "get_supported_resources",
    "id": "resources-1"
  }'
```

**Response:**
```json
{
  "jsonrpc": "2.0",
  "result": {
    "resources": [
      {
        "type": "s3-bucket",
        "name": "S3 Bucket",
        "provider": "aws",
        "description": "Amazon S3 storage bucket for file uploads and static assets",
        "properties": ["name", "allowOnlyHttps", "corsConfig"],
        "schema_url": "/schemas/aws/s3bucket.json"
      },
      {
        "type": "aws-rds-postgres",
        "name": "PostgreSQL RDS",
        "provider": "aws",
        "description": "Amazon RDS PostgreSQL managed database",
        "properties": ["name", "instanceClass", "allocatedStorage", "engineVersion"],
        "schema_url": "/schemas/aws/rds-postgres.json"
      },
      {
        "type": "gcp-gke-autopilot-cluster",
        "name": "GKE Autopilot Cluster",
        "provider": "gcp",
        "description": "Google Kubernetes Engine autopilot cluster",
        "properties": ["name", "location", "gkeMinVersion"],
        "schema_url": "/schemas/gcp/gke-autopilot.json"
      }
    ],
    "providers": [
      {
        "name": "aws",
        "display_name": "Amazon Web Services",
        "resources": ["s3-bucket", "aws-rds-postgres", "aws-rds-mysql", "ecr-repository"]
      },
      {
        "name": "gcp",
        "display_name": "Google Cloud Platform",
        "resources": ["gcp-bucket", "gcp-gke-autopilot-cluster", "gcp-cloudsql-postgres"]
      }
    ],
    "total": 37
  },
  "id": "resources-1"
}
```

### **4. Analyze Project** (Developer Mode Only)

Perform detailed project analysis with tech stack detection:

```bash
curl -X POST http://localhost:9999/mcp \
  -H "Content-Type: application/json" \
  -d '{
    "jsonrpc": "2.0",
    "method": "analyze_project",
    "params": {
      "path": "."
    },
    "id": "analyze-1"
  }'
```

**Response:**
```json
{
  "jsonrpc": "2.0",
  "result": {
    "path": "/home/user/my-awesome-app",
    "tech_stacks": [
      {
        "language": "javascript",
        "framework": "express",
        "runtime": "nodejs",
        "version": "18.x",
        "dependencies": [
          {"name": "express", "version": "^4.18.0", "type": "runtime"},
          {"name": "pg", "version": "^8.8.0", "type": "runtime"},
          {"name": "redis", "version": "^4.3.0", "type": "runtime"}
        ],
        "confidence": 0.95,
        "evidence": ["package.json found", "express dependency found"]
      },
      {
        "language": "docker",
        "runtime": "docker",
        "framework": "nodejs",
        "confidence": 0.8,
        "evidence": ["Dockerfile found", "node:18 base image detected"]
      }
    ],
    "primary_stack": {
      "language": "javascript",
      "framework": "express"
    },
    "architecture": "standard-web-app",
    "recommendations": [
      {
        "type": "resource",
        "category": "database",
        "priority": "high",
        "title": "PostgreSQL Database",
        "description": "Add PostgreSQL database resource for data persistence",
        "resource": "aws-rds-postgres",
        "action": "add_resource"
      },
      {
        "type": "resource",
        "category": "cache",
        "priority": "medium",
        "title": "Redis Cache",
        "description": "Add Redis cache for session storage and caching",
        "resource": "redis-cache",
        "action": "add_resource"
      }
    ],
    "confidence": 0.92,
    "metadata": {
      "analyzed_at": "2024-10-05T13:19:02Z"
    }
  },
  "id": "analyze-1"
}
```

### **5. Setup Simple Container** ‚≠ê **NEW**

Initialize Simple Container configuration using the built-in setup command. This tool uses the actual Simple Container setup process instead of generating generic files.

```bash
curl -X POST http://localhost:9999/mcp \
  -H "Content-Type: application/json" \
  -d '{
    "jsonrpc": "2.0",
    "method": "tools/call",
    "params": {
      "name": "setup_simple_container",
      "arguments": {
        "path": ".",
        "environment": "staging",
        "parent": "infrastructure",
        "interactive": false
      }
    },
    "id": "setup-1"
  }'
```

**Parameters:**
- `path` (string, optional): Project path (default: current directory)
- `environment` (string, optional): Target environment (default: "development")
- `parent` (string, optional): Parent stack name
- `deployment_type` (string, optional): Deployment type - "auto" (default), "static", "single-image", "cloud-compose"
- `interactive` (boolean, optional): Run in interactive mode (default: false for MCP)

**Two-Phase Workflow:**

**Phase 1 - Get Options:**
```bash
curl -X POST http://localhost:9999/mcp \
  -H "Content-Type: application/json" \
  -d '{
    "jsonrpc": "2.0",
    "method": "tools/call",
    "params": {
      "name": "setup_simple_container",
      "arguments": {
        "path": ".",
        "deployment_type": "auto"
      }
    },
    "id": "setup-options"
  }'
```

**Phase 1 Response:**
```json
{
  "jsonrpc": "2.0",
  "result": {
    "content": [{
      "type": "text",
      "text": "üîç Project Analysis Complete!\n\nüìÇ Project: my-app\nüíª Tech Stack: go gorilla-mux\nüèóÔ∏è Architecture: standard-web-app\n\nüìã **Choose Deployment Type:**\n\n**1. static** - Static site deployment\n**2. single-image** - Single container deployment\n**3. cloud-compose** - Multi-container deployment\n\nüéØ **Recommended**: cloud-compose\n\nExample: {\"path\": \".\", \"deployment_type\": \"cloud-compose\", \"environment\": \"staging\"}"
    }]
  }
}
```

**Phase 2 - Execute Setup:**
```bash
curl -X POST http://localhost:9999/mcp \
  -H "Content-Type: application/json" \
  -d '{
    "jsonrpc": "2.0",
    "method": "tools/call",
    "params": {
      "name": "setup_simple_container",
      "arguments": {
        "path": ".",
        "deployment_type": "cloud-compose",
        "environment": "staging",
        "parent": "infrastructure"
      }
    },
    "id": "setup-execute"
  }'
```

**Response:**
```json
{
  "jsonrpc": "2.0",
  "result": {
    "content": [
      {
        "type": "text",
        "text": "‚úÖ Simple Container setup completed successfully!\nüìÅ Project path: .\nüåç Environment: staging\nüë®‚Äçüë©‚Äçüëß‚Äçüë¶ Parent stack: infrastructure\nüìÑ Files created: [client.yaml, docker-compose.yaml, Dockerfile]"
      }
    ],
    "isError": false
  },
  "id": "setup-1"
}
```

**Key Benefits:**
- **Uses Actual Setup Process**: Leverages the same logic as `sc assistant dev setup`
- **Project Analysis**: Automatically analyzes project and detects deployment type
- **Deployment Type Confirmation**: Shows detected type and allows override
- **Schema-Compliant Files**: Generates validated client.yaml, docker-compose.yaml, and Dockerfile
- **No Random Files**: Unlike generic LLM generation, uses Simple Container's proven setup workflow

### **6. Generate Configuration** (Legacy)

Generate Simple Container configuration files:

```bash
curl -X POST http://localhost:9999/mcp \
  -H "Content-Type: application/json" \
  -d '{
    "jsonrpc": "2.0",
    "method": "generate_configuration",
    "params": {
      "project_path": ".",
      "project_type": "nodejs",
      "config_type": "client_yaml",
      "options": {
        "framework": "express",
        "database": "postgresql",
        "cache": "redis"
      }
    },
    "id": "generate-1"
  }'
```

## üéõÔ∏è Server Configuration

### **Environment Variables**
```bash
# MCP server configuration
export SC_MCP_HOST=localhost
export SC_MCP_PORT=9999
export SC_MCP_CORS_ORIGIN="*"
export SC_MCP_LOG_LEVEL=info

# Documentation search
export SC_EMBEDDING_MODEL=text-embedding-3-small
export SC_SEARCH_LIMIT=10

# Enable/disable features
export SC_MCP_ENABLE_ANALYSIS=true
export SC_MCP_ENABLE_GENERATION=true
```

### **Configuration File**
Create `.sc/mcp-config.yaml`:
```yaml
server:
  host: localhost
  port: 9999
  cors:
    enabled: true
    origins: ["*"]
    methods: ["GET", "POST", "OPTIONS"]

logging:
    level: info
    format: json

features:
  documentation_search: true
  project_analysis: true
  configuration_generation: true

search:
  default_limit: 10
  max_limit: 50
  embedding_model: text-embedding-3-small

security:
  api_key_required: false
  rate_limiting:
    enabled: false
    requests_per_minute: 60
```

## üîê Security Configuration

### **API Key Authentication**
```bash
# Enable API key authentication
export SC_MCP_API_KEY=your-secret-api-key

# Client requests must include header:
# Authorization: Bearer your-secret-api-key
```

### **CORS Configuration**
```bash
# Restrict origins for production
export SC_MCP_CORS_ORIGIN="https://windsurf.dev,https://cursor.sh"

# Or configure specific domains
export SC_MCP_CORS_ORIGIN="*.mycompany.com"
```

### **Rate Limiting**
```yaml
# In .sc/mcp-config.yaml
security:
  rate_limiting:
    enabled: true
    requests_per_minute: 120
    burst_limit: 20
    whitelist_ips: ["127.0.0.1", "10.0.0.0/8"]
```

## üìä Monitoring and Logging

### **Health Monitoring**
```bash
# Health check endpoint
curl http://localhost:9999/health

# Response:
{
  "status": "healthy",
  "timestamp": "2024-10-05T13:19:02Z",
  "version": "1.0",
  "name": "simple-container-mcp",
  "uptime": "2h30m15s",
  "requests_served": 1247,
  "documentation_count": 10543
}
```

### **Metrics Endpoint**
```bash
# Prometheus-compatible metrics
curl http://localhost:9999/metrics

# Output:
# mcp_requests_total{method="search_documentation"} 823
# mcp_requests_total{method="get_project_context"} 156
# mcp_request_duration_seconds{method="search_documentation"} 0.045
# mcp_documentation_search_results_total 4115
```

### **Logging Configuration**
```bash
# Structured JSON logging
export SC_MCP_LOG_FORMAT=json
export SC_MCP_LOG_LEVEL=info

# Log to file
export SC_MCP_LOG_FILE=/var/log/simple-container/mcp.log

# Sample log entry:
{
  "timestamp": "2024-10-05T13:19:02Z",
  "level": "info",
  "method": "search_documentation",
  "query": "postgres configuration",
  "results": 5,
  "duration_ms": 45,
  "client_ip": "127.0.0.1"
}
```

## üöÄ Performance Optimization

### **Caching Configuration**
```yaml
# In .sc/mcp-config.yaml
cache:
  enabled: true
  ttl: 300  # 5 minutes
  max_size: 1000

  # Cache documentation search results
  search_results:
    enabled: true
    ttl: 900  # 15 minutes

  # Cache project context
  project_context:
    enabled: true
    ttl: 60   # 1 minute
```

### **Connection Pooling**
```yaml
server:
  connection_pool:
    max_connections: 100
    idle_timeout: 300
    read_timeout: 30
    write_timeout: 30
```

### **Memory Management**
```bash
# Configure memory limits
export SC_MCP_MAX_MEMORY=512MB
export SC_MCP_GC_INTERVAL=60s

# Monitor memory usage
curl http://localhost:9999/debug/memory
```

## üîß Troubleshooting

### **Connection Issues**
```bash
# Check if server is running
curl http://localhost:9999/health

# Check port availability
netstat -tlnp | grep 9999

# Test with different port
sc assistant mcp --port 9998
```

### **Search Issues**
```bash
# Verify embeddings are generated
ls -la pkg/assistant/embeddings/embedded_docs.go

# Regenerate embeddings if missing
welder run generate-embeddings

# Test search directly
sc assistant search "test query"
```

### **IDE Integration Issues**
```bash
# Check MCP server logs
sc assistant mcp --verbose

# Validate JSON-RPC requests
curl -X POST http://localhost:9999/mcp \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","method":"ping","id":"test"}' | jq
```

## üìã Examples

### **Custom Client Implementation**
```python
import requests
import json

class SimpleContainerMCP:
    def __init__(self, endpoint="http://localhost:9999/mcp"):
        self.endpoint = endpoint

    def search_docs(self, query, limit=5):
        payload = {
            "jsonrpc": "2.0",
            "method": "search_documentation",
            "params": {"query": query, "limit": limit},
            "id": f"search-{hash(query)}"
        }

        response = requests.post(self.endpoint, json=payload)
        return response.json()

    def get_context(self, path="."):
        payload = {
            "jsonrpc": "2.0",
            "method": "get_project_context",
            "params": {"path": path},
            "id": "context"
        }

        response = requests.post(self.endpoint, json=payload)
        return response.json()

# Usage
client = SimpleContainerMCP()
docs = client.search_docs("AWS S3 configuration")
context = client.get_context()
```

### **Shell Integration**
```bash
# Create helper functions in .bashrc
sc_search() {
    curl -s -X POST http://localhost:9999/mcp \
        -H "Content-Type: application/json" \
        -d "{\"jsonrpc\":\"2.0\",\"method\":\"search_documentation\",\"params\":{\"query\":\"$1\",\"limit\":3},\"id\":\"shell\"}" \
        | jq -r '.result.documents[].content' \
        | head -200
}

sc_context() {
    curl -s -X POST http://localhost:9999/mcp \
        -H "Content-Type: application/json" \
        -d '{"jsonrpc":"2.0","method":"get_project_context","params":{"path":"."},"id":"shell"}' \
        | jq '.result'
}

# Usage
sc_search "postgres setup"
sc_context
```

## üîó Next Steps

1. **[Set up IDE integration ‚Üí](examples/ide-integration.md)**
2. **[Build custom MCP clients ‚Üí](examples/custom-clients.md)**
3. **[Monitor MCP server performance ‚Üí](examples/monitoring.md)**
4. **[Secure MCP in production ‚Üí](examples/security.md)**


---

## Template Requirements

# Template Configuration Requirements

This document clarifies the **required configuration** for each template type to prevent misleading documentation and AI misinformation.

## üö® CRITICAL: ALL Template Types Require Configuration

**NEVER show template examples without their required config sections!**

**This applies to ALL template types:**
- `ecs-fargate` (AWS)
- `gcp-static-website` (GCP)  
- `kubernetes-cloudrun` (Kubernetes)
- `aws-lambda` (AWS)
- `aws-static-website` (AWS)
- And ALL other template types

**Every template needs authentication, project IDs, and provider-specific configuration.**

## ECS Fargate (`ecs-fargate`)

**‚ùå WRONG - Incomplete Example:**
```yaml
templates:
  web-app:
    type: ecs-fargate
    # Missing required config!
```

**‚úÖ CORRECT - Complete Example:**
```yaml
templates:
  web-app:
    type: ecs-fargate
    config:
      credentials: "${auth:aws}"        # Required: AWS authentication
      account: "${auth:aws.projectId}"  # Required: AWS account/project ID
```

**Required Configuration:**
- `credentials`: AWS authentication reference (e.g., `"${auth:aws}"`)
- `account`: AWS account/project ID (e.g., `"${auth:aws.projectId}"`)

**Optional Configuration:**
- `region`: AWS region (defaults to template-specific region)
- Additional AWS-specific settings

## AWS Static Website (`aws-static-website`)

**Required Configuration:**
- `credentials`: AWS authentication
- `account`: AWS account/project ID

## AWS Lambda (`aws-lambda`)

**Required Configuration:**
- `credentials`: AWS authentication
- `account`: AWS account/project ID

## GCP Static Website (`gcp-static-website`)

**‚ùå WRONG - Incomplete Example:**
```yaml
templates:
  static-website:
    type: gcp-static-website
    # Missing required config!
```

**‚úÖ CORRECT - Complete Example:**
```yaml
templates:
  static-website:
    type: gcp-static-website
    config:
      projectId: "${auth:gcloud.projectId}"  # Required: GCP project ID
      credentials: "${auth:gcloud}"          # Required: GCP authentication
```

**Required Configuration:**
- `projectId`: GCP project ID reference (e.g., `"${auth:gcloud.projectId}"`)
- `credentials`: GCP authentication reference (e.g., `"${auth:gcloud}"`)

## Kubernetes CloudRun (`kubernetes-cloudrun`)

**‚ùå WRONG - Incomplete Example:**
```yaml
templates:
  k8s-app:
    type: kubernetes-cloudrun
    # Missing required config!
```

**‚úÖ CORRECT - Complete Example:**
```yaml
templates:
  stack-per-app-k8s:
    type: kubernetes-cloudrun
    config:
      kubeconfig: "${auth:kubernetes}"                              # Required: Kubernetes auth
      dockerRegistryURL: index.docker.io                          # Required: Docker registry
      dockerRegistryUsername: "${secret:docker-registry-username}" # Required: Registry auth
      dockerRegistryPassword: "${secret:docker-registry-password}" # Required: Registry auth
      caddyResource: caddy                                         # Optional: Load balancer resource
```

**Required Configuration:**
- `kubeconfig`: Kubernetes authentication reference
- `dockerRegistryURL`: Docker registry URL
- `dockerRegistryUsername`: Docker registry username secret
- `dockerRegistryPassword`: Docker registry password secret

**Optional Configuration:**
- `caddyResource`: Load balancer resource reference

## Documentation Guidelines

1. **Always include `config` section** in template examples
2. **Add comments** explaining required vs optional fields
3. **Never show incomplete examples** that could mislead users
4. **Reference this document** when creating new template documentation

## AI Training Data

This document should be indexed by the AI assistant to provide accurate template configuration guidance.


---

## Troubleshooting

# Troubleshooting

Common issues and solutions for Simple Container AI Assistant, organized by mode and functionality.

## üîç Quick Diagnostics

### **Health Check Commands**
```bash
# Check Simple Container installation
sc --version

# Verify AI assistant is available
sc assistant --help

# Test documentation search
sc assistant search "test" --limit 1

# Check MCP server status
curl -f http://localhost:9999/health || echo "MCP server not running"

# Verify embeddings are generated
ls -la pkg/assistant/embeddings/embedded_docs.go
```

### **System Requirements Check**
```bash
# Check Docker installation
docker --version && docker-compose --version

# Check Go installation (for building)
go version

# Check available memory
free -h

# Check disk space
df -h .
```

## üßë‚Äçüíª Developer Mode Issues

### **‚ùå Project Analysis Issues**

#### **Problem: No technology stack detected**
```
Error: no technology stacks detected in project
```

**Causes & Solutions:**

1. **Missing configuration files**
   ```bash
   # Check for language-specific files
   ls -la package.json requirements.txt go.mod composer.json Gemfile
   
   # If missing, create minimal configuration
   echo '{"name": "my-app", "version": "1.0.0"}' > package.json  # Node.js
   echo "Flask==2.0.0" > requirements.txt  # Python
   go mod init my-app  # Go
   ```

2. **Unsupported language/framework**
   ```bash
   # Override with manual specification
   sc assistant dev setup --language python --framework django --skip-analysis
   ```

3. **Complex monorepo structure**
   ```bash
   # Analyze specific service directory
   sc assistant dev analyze --path ./services/api
   sc assistant dev setup --path ./services/api
   ```

#### **Problem: Wrong framework detected**
```
Detected: React (confidence: 0.7)
Expected: Express.js API
```

**Solutions:**
```bash
# Override detection
sc assistant dev setup --framework express --skip-analysis

# Check package.json dependencies
cat package.json | jq '.dependencies'

# Verify main entry point
cat package.json | jq '.main,.scripts.start'
```

### **‚ùå Configuration Generation Issues**

#### **Problem: Generated client.yaml references non-existent resources**
```yaml
# In generated client.yaml
uses: [postgres-db, redis-cache]  # Resources don't exist
```

**Solutions:**

1. **Ensure DevOps has deployed infrastructure**
   ```bash
   # Check if infrastructure stack exists
   ls -la .sc/stacks/ | grep infrastructure
   
   # If not found, DevOps team needs to run:
   sc assistant devops setup
   sc provision -s infrastructure -e staging
   ```

2. **Check resource names match server.yaml**
   ```bash
   # View available resources
   cat .sc/stacks/infrastructure/server.yaml | grep -A 20 "resources:"
   
   # Update client.yaml with correct names
   vim .sc/stacks/my-app/client.yaml
   ```

3. **Use different parent stack**
   ```bash
   # Generate with specific parent
   sc assistant dev setup --parent my-infrastructure
   ```

#### **Problem: Docker Compose fails to start**
```
Error: Service 'postgres' failed to build: no such file or directory
```

**Solutions:**

1. **Missing Docker dependencies**
   ```bash
   # Check Docker is running
   docker info
   
   # Check docker-compose syntax
   docker-compose config
   
   # Pull required images
   docker-compose pull
   ```

2. **Port conflicts**
   ```bash
   # Check what's using ports
   netstat -tlnp | grep -E ':5432|:6379|:3000'
   
   # Kill conflicting processes or change ports in docker-compose.yaml
   ```

3. **Volume permission issues**
   ```bash
   # Fix volume permissions
   sudo chown -R $USER:$USER ./data
   
   # Or use different volume mount approach
   ```

### **‚ùå Deployment Issues**

#### **Problem: Parent stack not found**
```
Error: stack "infrastructure" not found in environment "staging"
```

**Solutions:**

1. **Verify infrastructure is deployed**
   ```bash
   # Check available stacks
   ls -la .sc/stacks/
   
   # If infrastructure missing, coordinate with DevOps team
   ```

2. **Check environment name spelling**
   ```bash
   # Check client.yaml for environment references
   grep -r "parentEnv:" .sc/stacks/*/client.yaml
   
   # Update client.yaml with correct environment
   ```

3. **Deploy infrastructure first**
   ```bash
   # DevOps team should run:
   sc provision -s infrastructure -e staging
   ```

## üõ†Ô∏è DevOps Mode Issues

### **‚ùå Infrastructure Setup Issues**

#### **Problem: Cloud credentials not working**
```
Error: unable to configure credentials for AWS
```

**Solutions:**

1. **Re-add credentials with correct permissions**
   ```bash
   # Remove old credentials
   sc secrets remove aws-access-key aws-secret-key
   
   # Add new credentials
   sc secrets add aws-access-key
   sc secrets add aws-secret-key
   
   # Verify credentials work
   aws sts get-caller-identity  # Test AWS CLI
   ```

2. **Check IAM permissions**
   ```json
   {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Effect": "Allow",
         "Action": [
           "s3:*",
           "rds:*", 
           "ecs:*",
           "ec2:*",
           "iam:PassRole"
         ],
         "Resource": "*"
       }
     ]
   }
   ```

3. **Use different credential method**
   ```bash
   # Use AWS CLI profile instead
   export AWS_PROFILE=simple-container
   
   # Or use IAM roles for EC2/ECS
   ```

#### **Problem: Resource naming conflicts**
```
Error: S3 bucket "myapp-staging-uploads" already exists
```

**Solutions:**

1. **Use unique prefixes**
   ```bash
   # Set up with company prefix
   sc assistant devops setup --prefix mycompany-$(date +%Y%m%d)
   ```

2. **Choose different region**
   ```bash
   # Set up in different region
   sc assistant devops setup --region us-west-2
   ```

3. **Clean up existing resources**
   ```bash
   # Remove conflicting resources (be careful!)
   sc destroy -s infrastructure -e staging --target s3-bucket
   ```

### **‚ùå Multi-Environment Issues**

#### **Problem: Environment isolation problems**
```
Production database accidentally connected to staging app
```

**Solutions:**

1. **Use strict naming conventions**
   ```yaml
   # In server.yaml, always use environment suffixes
   resources:
     staging:
       postgres-db:
         name: myapp-staging-db  # Always include environment
     production:
       postgres-db:
         name: myapp-production-db  # Different name
   ```

2. **Separate AWS accounts/GCP projects**
   ```bash
   # Use different cloud accounts per environment
   sc assistant devops secrets --auth aws-staging
   sc assistant devops secrets --auth aws-production
   ```

3. **Add environment validation**
   ```yaml
   # In client.yaml, validate environment matches
   stacks:
     my-app:
       parent: infrastructure
       parentEnv: staging  # Must match deployment environment
   ```

## üîç Search & Documentation Issues

### **‚ùå Documentation Search Issues**

#### **Problem: Search returns no results**
```
üîç Searching documentation for: "database setup"
Found 0 relevant documents
```

**Solutions:**

1. **Regenerate embeddings**
   ```bash
   # Check if embeddings exist
   ls -la pkg/assistant/embeddings/embedded_docs.go
   
   # Regenerate embeddings
   welder run generate-embeddings
   
   # Or manually
   go run cmd/embed-docs/main.go --docs-path ./docs --output ./pkg/assistant/embeddings/embedded_docs.go
   ```

2. **Check search query**
   ```bash
   # Try broader search terms
   sc assistant search "database"
   sc assistant search "postgres"
   sc assistant search "configuration"
   
   # Try different document types
   sc assistant search "database" --type examples
   ```

3. **Lower similarity threshold**
   ```bash
   # Allow less precise matches
   sc assistant search "database setup" --threshold 0.5
   ```

#### **Problem: Search results not relevant**
```
Search: "Node.js deployment"
Results: Python Django examples
```

**Solutions:**

1. **Use more specific queries**
   ```bash
   # Be more specific
   sc assistant search "Node.js Express deployment ECS"
   
   # Use exact framework names
   sc assistant search "Express.js container"
   ```

2. **Filter by provider or type**
   ```bash
   # Filter by cloud provider
   sc assistant search "Node.js" --provider aws
   
   # Filter by document type
   sc assistant search "Node.js" --type examples
   ```

## üåê MCP Server Issues

### **‚ùå Server Connection Issues**

#### **Problem: MCP server won't start**
```
Error: listen tcp :9999: bind: address already in use
```

**Solutions:**

1. **Check port availability**
   ```bash
   # Find what's using the port
   lsof -i :9999
   
   # Use different port
   sc assistant mcp --port 9998
   
   # Or kill the process using the port
   kill $(lsof -t -i :9999)
   ```

2. **Check firewall/network**
   ```bash
   # Test local connection
   telnet localhost 9999
   
   # Check firewall rules
   sudo ufw status  # Ubuntu
   firewall-cmd --list-ports  # RHEL/CentOS
   ```

#### **Problem: IDE can't connect to MCP server**
```
Failed to connect to Simple Container MCP server at localhost:9999
```

**Solutions:**

1. **Verify server is running**
   ```bash
   # Check server health
   curl http://localhost:9999/health
   
   # Check server logs
   sc assistant mcp --verbose
   ```

2. **Check IDE configuration**
   ```json
   // Verify .windsurf/tools.json
   {
     "tools": [{
       "name": "simple-container-assistant",
       "type": "mcp", 
       "endpoint": "http://localhost:9999/mcp"  // Correct endpoint
     }]
   }
   ```

3. **Test with curl**
   ```bash
   # Test JSON-RPC endpoint
   curl -X POST http://localhost:9999/mcp \
     -H "Content-Type: application/json" \
     -d '{"jsonrpc":"2.0","method":"ping","id":"test"}'
   ```

### **‚ùå Authentication Issues**

#### **Problem: API key authentication failing**
```
Error: unauthorized - invalid API key
```

**Solutions:**

1. **Check API key configuration**
   ```bash
   # Verify environment variable is set
   echo $SC_MCP_API_KEY
   
   # Restart server with API key
   export SC_MCP_API_KEY=your-secret-key
   sc assistant mcp
   ```

2. **Update client configuration**
   ```json
   // Add API key to IDE config
   {
     "tools": [{
       "endpoint": "http://localhost:9999/mcp",
       "headers": {
         "Authorization": "Bearer your-secret-key"
       }
     }]
   }
   ```

## üöÄ Performance Issues

### **‚ùå Slow Analysis/Generation**

#### **Problem: Project analysis takes too long**
```
Analyzing project... (taking over 60 seconds)
```

**Solutions:**

1. **Exclude large directories**
   ```bash
   # Add .scignore file
   cat > .scignore << EOF
   node_modules/
   .git/
   dist/
   build/
   __pycache__/
   .venv/
   EOF
   ```

2. **Analyze specific paths**
   ```bash
   # Analyze only application code
   sc assistant dev analyze --path ./src
   ```

3. **Increase system resources**
   ```bash
   # Check available memory
   free -h
   
   # Close other applications
   # Consider running on machine with more RAM
   ```

### **‚ùå Slow Documentation Search**

#### **Problem: Search takes too long**
```
Search taking 5+ seconds per query
```

**Solutions:**

1. **Reduce search scope**
   ```bash
   # Search fewer results
   sc assistant search "query" --limit 3
   
   # Search specific document types
   sc assistant search "query" --type docs
   ```

2. **Check system resources**
   ```bash
   # Monitor CPU/memory during search
   top
   
   # Check disk I/O
   iostat -x 1
   ```

3. **Optimize embeddings**
   ```bash
   # Regenerate embeddings with smaller chunks
   # Edit cmd/embed-docs/main.go to reduce chunk size
   # Then regenerate
   welder run generate-embeddings
   ```

## üîß Build & Installation Issues

### **‚ùå Build Errors**

#### **Problem: Missing embeddings during build**
```
Error: embedded_docs.go: no such file or directory
```

**Solutions:**

1. **Run embedding generation**
   ```bash
   # Generate embeddings first
   welder run generate-embeddings
   
   # Then build
   welder run build
   ```

2. **Check build order in welder.yaml**
   ```yaml
   # Ensure correct task order
   steps:
     - task: generate-schemas
     - task: generate-embeddings  # Must come before build
     - task: build-all
   ```

#### **Problem: Dependency conflicts**
```
Error: package github.com/philippgille/chromem-go: version conflict
```

**Solutions:**

1. **Update dependencies**
   ```bash
   # Clean and update modules
   go clean -modcache
   go mod tidy
   go mod download
   ```

2. **Check Go version**
   ```bash
   # Ensure Go 1.21+ 
   go version
   
   # Update if necessary
   ```

## üÜò Getting Help

### **Diagnostic Information**

When reporting issues, include:

```bash
# System information
sc --version
go version
docker --version
uname -a

# Configuration
cat .sc/cfg.default.yaml
ls -la .sc/stacks/

# Logs
sc assistant mcp --verbose 2>&1 | head -50

# Error details
sc assistant dev analyze --verbose 2>&1
```

### **Support Channels**

1. **Documentation Search**
   ```bash
   # Search for your specific issue
   sc assistant search "your error message"
   ```

2. **Community Support**
   - GitHub Issues: [simple-container-com/api/issues](https://github.com/simple-container-com/api/issues)
   - Stack Overflow: Tag `simple-container`
   - Community Slack: [slack.simple-container.com](https://slack.simple-container.com)

3. **Professional Support**
   - Enterprise Support: support@simple-container.com
   - Consulting Services: consulting@simple-container.com

### **Creating Bug Reports**

Use this template for bug reports:

```markdown
## Bug Report

**Version:** sc --version output
**OS:** Operating system and version
**Mode:** Developer/DevOps

**Expected Behavior:**
What should happen

**Actual Behavior:**
What actually happens

**Reproduction Steps:**
1. Run command X
2. See error Y

**Configuration:**
- Cloud provider: AWS/GCP/K8s
- Environment: staging/production
- Project type: Node.js/Python/Go

**Logs:**
```
Paste relevant log output here
```

**Additional Context:**
Any other relevant information
```

## üîó Related Documentation

- **[Getting Started](getting-started.md)** - Basic setup and usage
- **[Commands Reference](commands.md)** - Complete command documentation  
- **[Developer Mode](developer-mode.md)** - Application team workflows
- **[DevOps Mode](devops-mode.md)** - Infrastructure team workflows
- **[MCP Integration](mcp-integration.md)** - External tool integration


---

## Usage Examples

# AI Assistant Usage Examples

This guide provides comprehensive examples of how to use the Simple Container AI Assistant across all its modes and features.

## üöÄ Quick Start

The AI Assistant provides three main interfaces:

1. **Interactive Chat** - Natural language conversations
2. **Command-line Tools** - Direct CLI commands  
3. **IDE Integration** - Via MCP protocol (Windsurf, Cursor, etc.)

## üí¨ Interactive Chat Mode

### Basic Chat Session

Start an interactive chat session:

```bash
sc assistant chat
```

**Example conversation:**

```
ü§ñ Simple Container AI Assistant
Type 'help' for available commands or ask any question about Simple Container.

You: How do I create a client.yaml file for a Go application?

AI: I'll help you create a client.yaml file for your Go application. Here's a typical structure:

```yaml
schemaVersion: 1.0

stacks:
  production:
    type: cloud-compose
    parent: infrastructure
    config:
      runs: [app]
      scale:
        min: 1
        max: 3
      env:
        GO_ENV: production
        PORT: 8080
      secrets:
        DATABASE_URL: "${secret:db-url}"
```

This configuration:
- Uses cloud-compose deployment type
- References an "infrastructure" parent stack
- Scales between 1-3 instances
- Sets environment variables for Go
- Uses secrets for sensitive data

You: Can you show me Docker Compose setup too?

AI: Here's a docker-compose.yaml for local development:

```yaml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8080:8080"
    environment:
      - GO_ENV=development
      - PORT=8080
    volumes:
      - .:/app
    depends_on:
      - db
      
  db:
    image: postgres:13
    environment:
      POSTGRES_DB: myapp
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

You: /help

Available Commands:
- /search <query>     - Search documentation  
- /analyze           - Analyze current project
- /setup             - Interactive project setup
- /help              - Show this help
- exit               - Leave chat

You: /search postgres configuration

üîç Found 3 relevant documents:

1. **PostgreSQL RDS Setup** (similarity: 0.94)
   Guide for configuring AWS RDS PostgreSQL with Simple Container...

2. **Database Connection Patterns** (similarity: 0.91)  
   Best practices for connecting applications to databases...

3. **Secrets Management** (similarity: 0.88)
   How to securely store database credentials...

You: /analyze

üìÇ Analyzing current project...

Detected:
- Language: Go
- Framework: gorilla/mux  
- Architecture: REST API
- Confidence: 95%

Recommendations:
- Create Go-optimized Dockerfile
- Use cloud-compose deployment
- Set up PostgreSQL database
- Configure environment variables

You: exit

üëã Thanks for using Simple Container AI Assistant!
```

### Chat with OpenAI Integration

For enhanced responses, configure your OpenAI API key:

```bash
# Option 1: Environment variable
export OPENAI_API_KEY=sk-your-key-here
sc assistant chat

# Option 2: Command line flag
sc assistant chat --openai-key sk-your-key-here

# Option 3: Interactive prompt (secure)
sc assistant chat
# AI will prompt for key if not found
```

## üîç Command-Line Search

### Documentation Search

Search the documentation directly from command line:

```bash
# Basic search
sc assistant search "client.yaml example"

# Limited results
sc assistant search "docker compose" --limit 3

# Verbose mode (shows debug info)
sc assistant search "kubernetes deployment" --verbose

# Filter by document type
sc assistant search "postgres" --type guides
```

**Example output:**

```
üîç Searching documentation for: client.yaml example

Found 2 relevant documents:

1. Getting Started with Simple Container
   Path: getting-started/index.md
   Type: documentation
   Similarity: 0.954
   Preview: Welcome to Simple Container! This section will help you get up and running...

2. Quick Start Guide  
   Path: getting-started/quick-start.md
   Type: documentation
   Similarity: 0.953
   Preview: This guide will help you deploy your first application...
```

## üë©‚Äçüíª Developer Mode

### Project Setup and Analysis

```bash
# Analyze current project and generate files
sc assistant dev setup

# Analyze specific directory
sc assistant dev analyze /path/to/project

# Setup with specific parameters
sc assistant dev setup --env production --parent infrastructure --language go

# Skip project analysis (faster)
sc assistant dev setup --skip-analysis --language python --framework flask
```

**Example Developer Mode Session:**

```bash
$ sc assistant dev setup

üöÄ Simple Container Developer Mode - Project Setup
üìÇ Project path: /home/user/my-api

üîç Analyzing project...
   Language:     Go
   Framework:    gorilla/mux
   Version:      1.21
   Architecture: REST API
   Confidence:   95%

üéØ Recommendations:
   üîπ Create Go Dockerfile (high)
   üîπ Set up PostgreSQL database (medium)
   üîπ Configure environment variables (medium)
   üîπ Add health check endpoints (low)

üìù Generating configuration files...
   üìÑ Generating client.yaml... ‚úì
   üìÑ Generating docker-compose.yaml... ‚úì
   üìÑ Generating Dockerfile... ‚úì

üìÅ Generated files:
   ‚Ä¢ .sc/stacks/client.yaml     - Simple Container configuration
   ‚Ä¢ docker-compose.yaml        - Local development environment  
   ‚Ä¢ Dockerfile                 - Container image definition

üöÄ Next steps:
   1. Start local development: docker-compose up -d
   2. Deploy to production:     sc deploy -e production
   3. Set up secrets:          sc secrets add .sc/stacks/secrets.yaml

üí° Recommendations:
   ‚Ä¢ Configure PostgreSQL database in parent stack
   ‚Ä¢ Set up environment-specific configurations
   ‚Ä¢ Add health check endpoint to your Go application
```

### Generated Files Examples

**Generated client.yaml:**

```yaml
schemaVersion: 1.0

stacks:
  production:
    type: cloud-compose
    parent: infrastructure
    parentEnv: production
    config:
      runs: [app]
      scale:
        min: 2
        max: 10
      env:
        GO_ENV: production
        PORT: 8080
        LOG_LEVEL: info
      secrets:
        DATABASE_URL: "${secret:db-url}"
        JWT_SECRET: "${secret:jwt-secret}"
      dependencies:
        - postgres
```

**Generated Dockerfile:**

```dockerfile
# Build stage
FROM golang:1.21-alpine AS builder

WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download

COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -o main .

# Runtime stage  
FROM alpine:latest

RUN apk --no-cache add ca-certificates
WORKDIR /root/

COPY --from=builder /app/main .

EXPOSE 8080
CMD ["./main"]
```

**Generated docker-compose.yaml:**

```yaml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8080:8080"
    environment:
      - GO_ENV=development
      - PORT=8080
      - DATABASE_URL=postgres://user:password@db:5432/myapp
    volumes:
      - .:/app
    depends_on:
      - db
      
  db:
    image: postgres:13-alpine
    environment:
      POSTGRES_DB: myapp
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

volumes:
  postgres_data:
```

## üõ†Ô∏è DevOps Mode

### Infrastructure Management

```bash
# Interactive infrastructure setup
sc assistant devops setup

# Resource management
sc assistant devops resources --list
sc assistant devops resources --add postgres --env production
sc assistant devops resources --remove redis --env staging
sc assistant devops resources --update s3-bucket --env production

# Secrets management
sc assistant devops secrets --list
sc assistant devops secrets --add database-url
sc assistant devops secrets --rotate jwt-secret
```

**Example DevOps Mode Session:**

```bash
$ sc assistant devops setup

üõ†Ô∏è  Simple Container DevOps Mode - Infrastructure Setup
üìÇ Project path: /home/user/my-infrastructure

üèóÔ∏è  Infrastructure Configuration:
   Provider:     AWS
   Region:       us-east-1
   Environment:  production
   
üìã What would you like to set up?
   1. üóÑÔ∏è  Database (PostgreSQL/MySQL)
   2. ü™£ Storage (S3 Bucket)
   3. üîê Secrets (KMS Key)
   4. üåê Load Balancer (ALB)
   5. üìä Monitoring (CloudWatch)
   6. üîß All of the above
   
Select option [1-6]: 6

üìù Generating infrastructure configuration...
   üìÑ Generating server.yaml... ‚úì
   üìÑ Generating secrets.yaml... ‚úì  
   üìÑ Generating cfg.default.yaml... ‚úì

üìÅ Generated files:
   ‚Ä¢ .sc/stacks/infrastructure/server.yaml  - Infrastructure definition
   ‚Ä¢ .sc/stacks/infrastructure/secrets.yaml - Authentication & secrets
   ‚Ä¢ cfg.default.yaml                       - Default configuration

üöÄ Next steps:
   1. Configure AWS credentials: aws configure
   2. Add secrets to Simple Container: sc secrets add .sc/stacks/infrastructure/secrets.yaml
   3. Deploy infrastructure: sc provision -s infrastructure -e production
   4. Verify deployment: sc deploy -e production --dry-run
```

## üîå IDE Integration (MCP Protocol)

### Windsurf IDE Setup

1. **Start MCP Server:**

```bash
sc assistant mcp --port 9999
```

2. **Configure Windsurf** (`~/.config/windsurf/mcp.json`):

```json
{
  "mcpServers": {
    "simple-container": {
      "command": "/usr/local/bin/sc",
      "args": ["assistant", "mcp", "--port", "9999"],
      "env": {},
      "capabilities": {
        "resources": {
          "documentation_search": {
            "description": "Search Simple Container documentation"
          },
          "project_analysis": {
            "description": "Analyze project structure and recommend configurations"
          },
          "resource_discovery": {
            "description": "List available Simple Container resources"
          }
        }
      }
    }
  }
}
```

3. **Use in Windsurf:**

- **@simple-container** "How do I deploy a Go app?"
- **@simple-container** "Search for PostgreSQL examples"
- **@simple-container** "What resources are available for AWS?"
- **@simple-container** "Analyze this project structure"

### MCP Direct API Usage

```bash
# Health check
curl http://localhost:9999/health

# Search documentation
curl -X POST http://localhost:9999/mcp \
  -H "Content-Type: application/json" \
  -d '{
    "jsonrpc": "2.0",
    "id": 1,
    "method": "search_documentation",
    "params": {
      "query": "docker compose setup",
      "limit": 3
    }
  }'

# Analyze project
curl -X POST http://localhost:9999/mcp \
  -H "Content-Type: application/json" \
  -d '{
    "jsonrpc": "2.0",
    "id": 2,
    "method": "analyze_project",
    "params": {
      "path": "/path/to/project"
    }
  }'

# Get supported resources
curl -X POST http://localhost:9999/mcp \
  -H "Content-Type: application/json" \
  -d '{
    "jsonrpc": "2.0",
    "id": 3,
    "method": "get_supported_resources",
    "params": {}
  }'
```

## üéØ Advanced Usage Patterns

### Automation Scripts

**Auto-setup new projects:**

```bash
#!/bin/bash
# setup-new-project.sh

PROJECT_NAME=$1
LANGUAGE=$2

echo "üöÄ Setting up new $LANGUAGE project: $PROJECT_NAME"

mkdir $PROJECT_NAME
cd $PROJECT_NAME

# Initialize git
git init

# Generate Simple Container configuration
sc assistant dev setup \
  --language $LANGUAGE \
  --env production \
  --parent infrastructure \
  --skip-analysis

# Generate README from AI Assistant
sc assistant chat --non-interactive << EOF
Generate a README.md for a $LANGUAGE project named $PROJECT_NAME with Simple Container deployment instructions.
EOF

echo "‚úÖ Project $PROJECT_NAME setup complete!"
echo "üìÇ Files generated:"
ls -la
```

**Documentation search automation:**

```bash
#!/bin/bash
# search-docs.sh

QUERY="$1"
OUTPUT_FILE="search-results.md"

echo "# Documentation Search Results" > $OUTPUT_FILE
echo "Query: \"$QUERY\"" >> $OUTPUT_FILE
echo "Date: $(date)" >> $OUTPUT_FILE
echo "" >> $OUTPUT_FILE

sc assistant search "$QUERY" --limit 5 >> $OUTPUT_FILE

echo "üìÑ Results saved to $OUTPUT_FILE"
```

### CI/CD Integration

**GitHub Actions workflow:**

```yaml
# .github/workflows/simple-container.yml
name: Simple Container Deployment

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  analyze:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Install Simple Container
        run: |
          curl -sSL https://get.simple-container.com | bash
          
      - name: Analyze Project
        run: |
          sc assistant dev analyze --format json > analysis.json
          cat analysis.json
          
      - name: Validate Configuration
        run: |
          sc validate -f .sc/stacks/client.yaml
          
      - name: Deploy (Production only)
        if: github.ref == 'refs/heads/main'
        run: |
          sc deploy -e production
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
```

## üîß Configuration Options

### Verbose Mode

Enable detailed logging for troubleshooting:

```bash
# All commands support --verbose flag
sc assistant search "postgres" --verbose
sc assistant dev setup --verbose
sc assistant chat --verbose
sc assistant mcp --verbose
```

### Custom Configuration

**Environment Variables:**

```bash
# OpenAI API key
export OPENAI_API_KEY=sk-your-key-here

# Custom MCP server settings
export SC_MCP_HOST=0.0.0.0
export SC_MCP_PORT=8888

# Verbose logging by default
export SC_VERBOSE=true
```

**Configuration Files:**

```yaml
# ~/.sc/config.yaml
ai_assistant:
  llm:
    provider: openai
    model: gpt-4
    temperature: 0.7
    max_tokens: 2048
  
  mcp:
    host: localhost
    port: 9999
    
  search:
    default_limit: 10
    similarity_threshold: 0.7
    
  dev_mode:
    auto_analyze: true
    skip_confirmation: false
    default_parent: infrastructure
```

## üö® Troubleshooting

### Common Issues

**"No documents loaded" error:**

```bash
# Check embedded documentation
sc assistant search "test" --verbose

# Should show: "Initialized documentation database with 30 documents"
```

**MCP server connection issues:**

```bash
# Check server status
curl http://localhost:9999/health

# Check if port is in use
lsof -i :9999

# Restart server with different port
sc assistant mcp --port 9998
```

**OpenAI API issues:**

```bash
# Test API key
sc assistant chat --openai-key sk-test-key

# Use fallback templates
sc assistant dev setup --no-llm
```

**File generation problems:**

```bash
# Check permissions
ls -la .sc/

# Create directory structure
mkdir -p .sc/stacks

# Generate with verbose output
sc assistant dev setup --verbose
```

### Debug Information

```bash
# Get AI Assistant version and capabilities
sc assistant --help

# Check embedded documentation status
sc assistant search "" --limit 0 --verbose

# Test MCP server capabilities
curl http://localhost:9999/capabilities | jq .

# Validate generated configurations
sc validate -f .sc/stacks/client.yaml
```

## üéì Best Practices

### Development Workflow

1. **Start with analysis:**
   ```bash
   sc assistant dev analyze
   ```

2. **Generate configuration:**
   ```bash
   sc assistant dev setup
   ```

3. **Test locally:**
   ```bash
   docker-compose up -d
   ```

4. **Deploy to staging:**
   ```bash
   sc deploy -e staging
   ```

5. **Use AI for questions:**
   ```bash
   sc assistant chat
   ```

### DevOps Workflow

1. **Set up infrastructure:**
   ```bash
   sc assistant devops setup
   ```

2. **Configure secrets:**
   ```bash
   sc secrets add .sc/stacks/infrastructure/secrets.yaml
   ```

3. **Provision infrastructure:**
   ```bash
   sc provision -s infrastructure -e production
   ```

4. **Monitor and maintain:**
   ```bash
   sc assistant devops resources --list
   ```

### IDE Integration

1. **Start MCP server:**
   ```bash
   sc assistant mcp --port 9999
   ```

2. **Configure IDE (Windsurf/Cursor)**

3. **Use @simple-container** for contextual help

---

## üìö Additional Resources

- **[Getting Started Guide](../getting-started/index.md)** - Basic Simple Container setup
- **[AI Assistant Commands](commands.md)** - Complete command reference  
- **[Developer Mode Guide](developer-mode.md)** - Detailed developer workflow
- **[DevOps Mode Guide](devops-mode.md)** - Infrastructure management
- **[MCP Integration Guide](mcp-integration.md)** - IDE setup and usage
- **[Troubleshooting Guide](troubleshooting.md)** - Common issues and solutions

---

**Need help?** Ask the AI Assistant: `sc assistant chat` ü§ñ


---

